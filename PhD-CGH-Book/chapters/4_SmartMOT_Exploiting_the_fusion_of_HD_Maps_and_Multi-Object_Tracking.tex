%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% 
% Generic template for TFC/TFM/TFG/Tesis
% 
% By:
% + Javier Macías-Guarasa. 
% Departamento de Electrónica
% Universidad de Alcalá
% + Roberto Barra-Chicote. 
% Departamento de Ingeniería Electrónica
% Universidad Politécnica de Madrid   
% 
% Based on original sources by Roberto Barra, Manuel Ocaña, Jesús Nuevo,
% Pedro Revenga, Fernando Herránz and Noelia Hernández. Thanks a lot to
% all of them, and to the many anonymous contributors found (thanks to
% google) that provided help in setting all this up.
% 
% See also the additionalContributors.txt file to check the name of
% additional contributors to this work.
% 
% If you think you can add pieces of relevant/useful examples,
% improvements, please contact us at (macias@depeca.uah.es)
% 
% You can freely use this template and please contribute with
% comments or suggestions!!!
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\chapter{SmartMOT: Exploiting the fusion of HD maps and Multi-Object Tracking for Real-Time scene understanding}
\label{cha:smartmot_exploiting_the_fusion_of_hdmaps_and_mot}

\begin{FraseCelebre}
	\begin{Frase}
		¡Avanzad, sin temor a la oscuridad! \\
		¡Luchad! ¡Luchad! ¡Luchad jinetes de Théoden! \\
		¡Caerán las lanzas, se quebrarán los escudos,  \\
		aún restará la espada! \\
		¡Rojo será el día hasta el meced del sol! \\
		¡Cabalgad! ¡Cabalgad! \\
		¡Cabalgad a la desolación y al fin del mundo! \\
		¡Muerte! ¡Muerte! ¡Muerte! ¡Adelante Eorlingas!
	\end{Frase}
	\begin{Fuente}
		Discurso de Theoden, Rey de Rohan \\
		Batalla de los Campos de Pelennor \\
		El Señor de los Anillos: El Retorno del Rey
	\end{Fuente}
\end{FraseCelebre}

\section{Introduction}
\label{sec:4_introduction}

In order to achieve a reliable navigation, \acfp{ADS} must perform safe driving behaviours following conventional traffic rules. One of the key concepts when developing safe \acp{ADS} is the perception of the environment. Furthermore, the reliability of the \ac{DM} and local planning modules lies on the performance of the environment detector and its ability to predict future situations. In that sense, a real-time \ac{MOT} system, which goal is to associate detections (usually in the 3D or \ac{BEV} space) in a sequence, is essential for \ac{AD} applications, representing in most cases the preliminary stage before predicting the subsequent future trajectories of these obstacles in the scene, giving the car a valuable reaction time to avoid critical situations or to anticipate its behaviour for the corresponding traffic scenario. The improvements in object detection in the last years have allowed the research community, specially those groups related to \ac{AD}, to focus on \ac{MOT} techniques as a preliminary stage before implementing \ac{MP}, yielding higher accuracy at the cost of computational complexity, making its use prohibitive in real-time systems. 

\ac{MOT} systems aim to estimate the orientation, location and scale of all the objects in the environment over time. While object detection only captures the information of the environment in a single frame, a tracking system must take temporal information into account, filtering outliers (\aka \ false positives) in consecutive detections and being robust to partial or full occlusions. When travelling throughout a route programmed by the path-planner, the vehicle may detect an undetermined number of unforeseen objects over which the \ac{MOT} module should consider only the most relevant from a safety point of view (such as pedestrians, cyclists or cars) to predict and monitor their trajectories. Then, the vehicle can use the evolution of the scene over time to infer driving behaviour and motion patters for improved \ac{MP}.

Most \ac{MOT} approaches \cite{weng20203d, chiu2021probabilistic} model the state of each obstacle with its 3D position, scale, orientation and their corresponding linear and angular velocity. These approaches introduce an unnecessary complexity and computational cost to the system, since most traffic scenes can be described in terms of 2D position, angular and linear velocity, apart from the orientation and scale of the resulting bounding box, that is, a \ac{BEV} perspective, as depicted in Figure \ref{fig:chapter_4_SmartMOT/ITSC_2020_coordinates_conversion}. 

\begin{figure}[h]
	\centering
	\captionsetup{justification=justified}
	\includegraphics[trim=0 0 0 1cm, width=0.8\textwidth]{chapter_4_SmartMOT/ITSC_2020_coordinates_conversion.pdf}
	\caption[\ac{LiDAR} to \ac{BEV} coordinates transformation illustrated in the CARLA simulator]{\ac{LiDAR} to \ac{BEV} coordinates transformation illustrated in the CARLA simulator. \textit{grw} and \textit{grh} stands for the height and width of our real-world grid respectively.}
	\label{fig:chapter_4_SmartMOT/ITSC_2020_coordinates_conversion}
\end{figure}

This Chapter summarizes the SmartMOT pipeline and the experimental results obtained with our proposal. The work in this Chapter was partially published in the following conference paper \cite{gomez2021smartmot}: "SmartMOT: exploiting the fusion of \acp{HDmap} and multi-object tracking for real-time scene understanding in intelligent vehicles applications", 2021 IEEE Intelligent Vehicles Symposium (IV), p. 710-715. We make the following contributions:

\begin{enumerate}
	
	\item We propose SmartMOT, an \href{https://github.com/Cram3r95/SmartMOT}{open-source} \footnote{https://github.com/Cram3r95/SmartMOT} simple-yet-accurate pipeline to perform real-time \ac{MOT} and physics-based uni-modal \ac{MP}.
	
	\item A Monitored Lanes-based Attention Module is proposed to extract monitored lanes around the ego-vehicle (including standard lanes and intersections) and filter non-relevant obstacles. Then, the ego-vehicle will track only those agents that are in the monitored area.
	
	\item We study the \ac{MOT} in the KITTI \ac{MOT} dataset and in data captured by our real-world vehicle, as well as the influence of the map monitor to reduce the inference time of the pipeline in the \ac{CARLA} simulator, with the ultimate goal of achieving real-time uni-modal prediction based on kinematics.
	
\end{enumerate}

\section{SmartMOT pipeline}
\label{sec:4_smartmot}

In order to solve the problem of monitoring the relevant objects around the ego-vehicle in an efficient way, we propose SmartMOT \cite{gomez2021smartmot}, a simple-yet-accurate tracking-by-detection pipeline which consists of a combination of traditional techniques such as the \ac{KF} \cite{kalman1960new} and \ac{HA} \cite{kuhn1955hungarian} for state estimation and data association respectively. Moreover, we incorporate \ac{HDmap} information, in addition to the ego-vehicle status, so as to enhance the efficiency and reliability of the tracking system and subsequent predictions, as observed in Figure \ref{fig:chapter_4_SmartMOT/IV_2021}. 

The SmartMOT pipeline (Figure \ref{fig:chapter_4_SmartMOT/IV_2021}) is made up by: (1) 3D object detection module that returns the bounding boxes, (2) Monitored Area to filter non-relevant objects, \eg \ the \acfp{VRU} that are inside the sidewalk far away the road or the vehicles that are located in a lane in which lane change is not allowed, (3) \acf{BEV} \acf{KF} that predicts the object state from the current frame and updates the object state based on the detected bounding boxes at current frame, (4) \acf{HA}, which associates the current trackers with new detections, (5) Birth and Death memory that deals with the disappeared trajectories (unmatched trajectories exceeding ${age_{max}}$ frames) and the newly appeared trajectories (matched trajectories exceeding ${f_{min}}$ frames) and (6) \ac{CTRV} model which performs short-term physics-based \ac{MP} of the updated trackers information using both the linear and angular velocity information. As observed, except for the pre-trained object detector module, our \ac{MOT} system does not need any training and can be directly used for inference.

\begin{figure}[h]
	\centering
	\includegraphics[trim=0cm 3cm 0cm 0cm, width=\textwidth]{chapter_4_SmartMOT/IV_2021.pdf}
	\captionsetup{justification=justified}
	\caption[SmartMOT pipeline for Multiple-Object Tracking and Short-Term Motion Prediction]{SmartMOT pipeline for Multiple-Object Tracking and Short-Term Motion Prediction. It can be observed that we use as inputs the obstacles 360\degree~detected in the environment, the \ac{HDmap} information and ego-vehicle status. Only those agents that are relevant are tracked, predicted and evaluated in the corresponding use cases.}
	% $\textbf{SmartMOT pipeline}$: $\textbf{(1)}$ The object detection module, mapping layer and localization layer provide the 3D bounding boxes at frame \textit{t}, monitored lanes and ego-vehicle status data respectively; $\textbf{(2)}$ A Monitored Lanes-based Attention Module filters the non-relevant traffic participants and transforms the remaining into the \ac{BEV} image plane; $\textbf{(3)}$ A \ac{BEV} Kalman Filter predicts the state of trajectories in frame \textit{t-1} to current frame \textit{\^{t}} throughout the prediction step; $\textbf{(4)}$ detections at frame \textit{t} and predicted trajectories at \textit{\^{t}} are matched using the Khun-Munkres (\aka \ Hungarian) algorithm; $\textbf{(5)}$ matched trajectories are updated based on their corresponding matched detections and every tracker is evaluated again based on its particular monitorized area, to obtain updated trajectories at frame \textit{{t}}; $\textbf{(6)}$ Unmatched trajectories and detections are used to delete disappeared trajectories or create new ones respectively; $\textbf{(7)}$ Updated trackers at frame \textit{{t}} are predicted using a CTRV model and then evaluated using the monitors module.
	\label{fig:chapter_4_SmartMOT/IV_2021}
\end{figure}

\subsection{3D Object Detection}
\label{subsec:4_smartmot_detection}

The first step our \ac{MOT} algorithm must carry out is to detect the obstacles in the environment around the ego-vehicle. Since this thesis does not focus on the object detection stage of the perception layer, experiments to integrate the object detection and monitored area are conducted in the \ac{CARLA} assuming ground-truth detection including Gaussian noise in the \textit{x,y,z}-axis to simulate real-world detections. Then, at a given frame \textit{t}, the detections provided by the object detection module are given in the following form:

\begin{equation}
	\label{eq:4_smartmot_detection}
	D_{t} =[D_{t}^{1},D_{t}^{2}, ...,D_{t}^{N}]
\end{equation}

Where \textit{N} is the number of detected 3D bounding boxes at a given frame and threshold. At this point, instead of using all the 3D information of the object \cite{chiu2021probabilistic, weng20203d}, we take its projection on the floor plane (\ac{BEV} information), to reduce the complexity and computational cost of the tracking stage, specially in those urban scenarios full of vehicles, based on the assumption that the height (\textit{z}) dimension is not as important as other coordinates (\textit{x}-axis, \textit{y}-axis) in a context of self-driving navigation. Detected 3D bounding boxes are referred to the \ac{LiDAR} coordinate system. A grid is applied to establish a relation between real-world and image dimensions to discretize the possible positions of the detected bounding boxes and decrease the complexity and computational cost of the tracking module. 

Conducting \ac{MOT} in the discrete space \ac{BEV} using pixels instead of 3D real-world units using meters offers several advantages in certain applications and scenarios. Some of these advantages include:

\begin{itemize}
	
	\item \textbf{Computational Efficiency}: Representing the 3D scene in the \ac{BEV} space using pixels allows for faster and more efficient processing, with reduced memory requirements. Working with pixels reduces the complexity of computations compared to using real-world units, as it involves simple 2D operations rather than more complex 3D calculations which often involve larger floating-point values, particularly in terms of the \ac{KF} update/predict transitions or 3D-\ac{IOU} as metric in the affinity matrix of the \acf{HA}.
	
	\item \textbf{Simplified Data Representation}: In the \ac{BEV} space, the environment is represented as a 2D image, which can be easily handled by standard computer vision techniques. This simplification makes it easier to implement and integrate \ac{MOT} algorithms with existing image processing pipelines.

	\item \textbf{Sensor Independence}: The \ac{BEV} representation is sensor agnostic, meaning it can be easily adapted to work with various sensor types, such as \ac{LiDAR} (present case) but also camera or \ac{RADAR}, without the need for sensor-specific calibration or transformations.
	
	\item \textbf{Occlusion Handling}: In the \ac{BEV}, occlusions between objects can be more straightforward to handle. Occluded objects may still be partially visible in the image, and tracking algorithms can take advantage of this partial information to maintain better tracking continuity.
	
	\item \textbf{Top-Down Contextual Information}: By observing the scene from a top-down perspective, the algorithm can gain a holistic view of the environment and utilize higher-level contextual information, such as road layout, lanes, and traffic rules, which can aid in improving tracking accuracy.
	
	\item \textbf{Object Shape Normalization}: When objects are projected onto the \ac{BEV}, they are often represented by simple shapes (\eg \ rectangles) rather than complex 3D shapes. This simplification can facilitate object association and matching across frames (as proposed in the present Chapter).

\end{itemize}

Nevertheless, despite these advantages, it is essential to note that using pixels in the \ac{BEV} space may introduce limitations. For instance, the resolution and accuracy of the tracking might be constrained by the pixel grid size. In that sense, our grid is featured by a rectangle, whose center is located at the \ac{LiDAR} position on the vehicle, where $\textit{grw}$ and $\textit{grh}$ represent the width and height of the grid in \ac{LiDAR} coordinates $\textit{m}$ (meters) respectively, as depicted in Figure \ref{fig:chapter_4_SmartMOT/ITSC_2020_coordinates_conversion}. Then, each detection in Equation \ref{eq:4_smartmot_detection_tuple} is represented as the tuple:

\begin{equation}
	\label{eq:4_smartmot_detection_tuple}
	D_{t}^{i} = [x_{m},y_{m},w_{m},l_{m},\theta,type,score]
\end{equation}

Where $\textit{$x_{m},y_{m}$}$ correspond to the object centroid in \ac{LiDAR} coordinates ($\textit{m}$), $\textit{$w_{m}$}$ and $\textit{$l_{m}$}$ correspond to the width and length of the object respectively ($\textit{m}$), $\theta$ its orientation angle around the \ac{LiDAR} $z$-axis, object type and detection confidence. Figure \ref{fig:chapter_4_SmartMOT/ITSC_2020_coordinates_conversion} illustrates the transformation from the source coordinate system (\ac{LiDAR}), measured in $\textit{m}$ and placed at the ego-vehicle, to the target coordinate system (\ac{BEV}), measured in $\textit{px}$ (pixels) and placed on the top-left corner of the grid, which is the most common way to work with images in computer vision. In other words, we deal with the \ac{MOT} problem from the \ac{BEV} image perspective, in order to adapt \ac{MOT} algorithms originally designed for computer vision purposes. 

Equations \ref{eq:4_smartmot_rw_to_image_transform} and \ref{eq:4_smartmot_rw_to_image_apply_transform} show the transformation matrix between both coordinate systems, including both the rotation and the translation (\textit{$\frac{grw}{2}$} and \textit{$\frac{grh}{2}$}), where a $LiDAR_{point}=[x_{m},y_{m},z_{m},1]^{T}$ is given as the column vector in homogeneous coordinates.

\begin{equation}
	\label{eq:4_smartmot_rw_to_image_transform}
	T = \left[ \begin{array}{cccc}
		0  &  -1 &  0  &  \frac{grw}{2} \\
		-1 &  0  &  0  &  \frac{grh}{2} \\
		0  &  0  &  -1 &  0            \\
		0  &  0  &  0  &  1 \end{array} \right] 
\end{equation}

\begin{equation}
	\label{eq:4_smartmot_rw_to_image_apply_transform}
	BEVV_{point} = T \cdot LiDAR_{point}
\end{equation}

At this point, each detection is represented by the tuple shown in Equation \ref{eq:4_smartmot_detection_tuple}, but now \textit{$x_{m},y_{m}$} represent the obstacle centroid in \ac{BEV} image perspective. Furthermore, the resolution of the \ac{BEV} image can be modified, in such a way the image width in pixels is given to the algorithm and the image height is calculated according to the aspect ratio of the real world with respect to the width of the image in pixels. Finally, to convert a point from real-word units ($\textit{m}$) to image units (pixels, $\textit{px}$), we apply the corresponding scale factor to each coordinate:

\begin{equation}
	\label{eq:4_conversion_meter2px}
	\left[ \begin{array}{c}
		x_{px}  \\
		y_{pc} \end{array} \right] 
	=
	\left[ \begin{array}{cc}
		\frac{gpw}{grw} & 0  \\
		0 & \frac{gph}{grh} \end{array} \right]
	\left[ \begin{array}{c}
		x_{m}  \\
		y_{m} \end{array} \right] 
\end{equation}

However, it is very common to have different scales for $\textit{x}$ and $\textit{y}$-axis since it is more interesting to have a further view in the $\textit{x}$ \ac{LiDAR} axis rather than a large side sweep in terms of $\textit{y}$ \ac{LiDAR} axis. Considering this hypothesis, the right way to obtain the width and length of the \ac{BEV} \ac{LiDAR} bounding box in $\textit{pixels}$ is to obtain the corners of the rotated bounding box in pixels and then compute the $\mathcal{L}_2$ distance among the corresponding corners to obtain the width and length in $\textit{pixels}$. Nevertheless, the object detector provides the rotation angle of the obstacle (featured as $\theta$) according to its own coordinate system and not around the ego-vehicle coordinate system. Regarding this constraint, to calculate the dimensions of the bounding box in pixels, three steps must be followed:

\begin{itemize}
	\item First, we assume a horizontal bounding box ($\theta$ = 0) at the \ac{BEV} image coordinate system origin, where \textit{$c_{1,m}$} corresponds to the top-left corner (\textit{$c_{2,m}$}, \textit{$c_{3,m}$} and \textit{$c_{4,m}$} are placed clockwise).
	
	\begin{equation}
		\begin{split}
			c_{1,m} = (x_{m}-\frac{l_{m}}{2},y_{m}-\frac{w_{m}}{2}) \\
			c_{2,m} = (x_{m}-\frac{l_{m}}{2},y_{m}+\frac{w_{m}}{2}) \\
			c_{3,m} = (x_{m}+\frac{l_{m}}{2},y_{m}-\frac{w_{m}}{2}) \\
			c_{4,m} = (x_{m}+\frac{l_{m}}{2},y_{m}+\frac{w_{m}}{2}) 
		\end{split}
	\end{equation}
	
	\item Second, each corner point is converted from real-world units ($m$) to image units ($px$) using Equation \ref{eq:4_conversion_meter2px}.
	
	\item Then, the $\mathcal{L}_2$ distance is applied between \textit{$c_{1,m}$} and \textit{$c_{2,m}$} to obtain the width in pixels, in the same way that the $\mathcal{L}_2$ distance is applied between \textit{$c_{1,m}$} and \textit{$c_{4,m}$} to obtain the length in pixels.
	
	\begin{equation}
		\label{widthpixels}
		w_{px} = \sqrt{(c1_{px,x}-c2_{px,x})^2 + (c1_{px,y}-c2_{px,y})^2}
	\end{equation}
	
	\begin{equation}
		\label{lengthpixels}
		l_{px} = \sqrt{(c4_{px,x}-c2_{px,x})^2 + (c4_{px,y}-c2_{px,y})^2}
	\end{equation}
	
	\item Finally, the detection tuple that will feed the tracking algorithm remains as follows, where all variables are expressed in $px$:
	
	\begin{equation}
		\label{detpx}
		D_{t,i} = [x_{px},y_{px},w_{px},l_{px},\theta,type,score]
	\end{equation}
	
\end{itemize}

\subsection{Monitored Lanes-based Attention Module}
\label{subsec:4_smartmot_mlam}

\acp{ADS} need to locate itself in the environment to know what is happening around in order to make decisions and execute a correct navigation like a human driver would. When we talk about localization, the first thing we need is a map where to be located and, particularly for \ac{AD}, a \ac{HDmap} that contains not only a general geometric description of the scene, but also the topological information of the lanes (lanes type, boundaries constraints, etc.) as well as the semantic information of the road. 

A \ac{HDmap} is usually a text file describing the real-world features related to the road map and its location within a 2D/3D space, and can do things that other sensors cannot \cite{wong2020mapping}: First, they have an \textit{infinite range} and, therefore, can \textit{see} even into occluded areas. Second, \acp{HDmap} will never fail due to environmental conditions. Lastly, \acp{HDmap} contain highly refined data. This information can be used by different modules of an \ac{ADS} (including localization, vehicle control, path planning, perception and system management) drastically reducing the computational load and complexity in comparison to other more complex methods, providing robustness and reliability to the system. Regarding this, in terms of mapping information, we may distinguish three main categories:

\begin{itemize}
	
	\item \textbf{Topological information} provides the connectivity between geometry features. Particularly in the field of \ac{AD}, this is usually the network of roads. This kind of information can allow vehicles to traverse the most energy-efficient route, based on traffic speed, road grade or distance, as well as ensure that \acp{ADS} obey traffic regulation orders, such as one-way streets or the corresponding regulatory elements (pedestrian crossing, traffic light, stop signal, etc.).
	
	\item \textbf{Geometric information} provides the geometry or shape of other environmental features that can be static (permanent obstruction, such as buildings, bridges or tunnels), temporary (exist for only a limited amount of time, like traffic cones, parked vehicles or temporary road works) and dynamic features (moving people, objects or vehicles). Most of these features are incorporated by means of perception systems, specially in terms of dynamic features, in order to include that information in the \ac{HDmap} for successful motion planning and prediction.
	
	\item \textbf{Semantic information} returns the \textit{meaning} of aforementioned features, such as road speed limit, road classification, lane information or even the relational information among the different lanes, \ie \ how lanes work together, different types of lanes, where vehicles must stop and where vehicles can and cannot turn.
\end{itemize}

As illustrated, providing rich physical contextual information allows \acp{ADS} to make informed decisions in different driving scenarios. In this thesis, we particularly make use of the OpenDrive \cite{dupuis2010opendrive} \ac{HDmap} format, which has been mainly used for two different purposes, as shown in Figure \ref{fig:chapter_4_SmartMOT/path_planner_map_monitor}:

\begin{itemize}
	
	\item \textbf{Global Path Planning}, which uses a specific path planner where inputs are the \ac{HDmap} information and the ego-vehicle current location to retrieve an optimal (usually optimized based on the traveled distance) global route towards an specific goal.
	
	\item \textbf{Map monitoring}, responsible for monitoring the most relevant static and dynamic map elements around the ego-vehicle at each time-step, such as standard lanes (current, back), intersection lanes (merge, split and cross) and regulatory elements (e.g. give way, stop, pedestrian crossing, traffic light).
	
\end{itemize}

\begin{figure}[] 
	\centering
	\includegraphics[width=0.8\textwidth]{chapter_4_SmartMOT/path_planner_map_monitor.pdf}
	\caption{Main uses of \ac{HDmap}: Path Planning and Map Monitoring}
	\label{fig:chapter_4_SmartMOT/path_planner_map_monitor}
\end{figure}

In particular, given a pre-defined global route, in this thesis we focus our interest on designing a map monitor module, responsible for retrieving the most relevant lanes around the ego-vehicle to enhance real-time perception and scene understanding requirements.

\subsubsection{Map Monitor}
\label{subsubsec:4_smartmot_mapmonitor}

In a similar way to humans that pay more attention to close obstacles, people walking towards them or upcoming turns rather than considering the presence of building or people far away, the perception layer of a self-driving car must be modelled to focus more on the salient regions of the scene \cite{sadeghian2019sophie} and the more relevant agents to predict the future behaviour of each traffic participant. In that sense, \acp{HDmap} have been widely adopted to provide offline (also known as context) information to complement the online information provided by the sensor suite of the vehicle and its corresponding algorithms. 

Recent learning-based approaches \cite{hong2019rules} \cite{chai2019multipath} \cite{gao2020vectornet} \cite{casas2018intentnet}, which present the benefit of having probabilistic interpretations of different behaviour hypotheses, require to build a representation to encode the trajectory and map information. \cite{hong2019rules} assumes that detections around the vehicle are provided and focuses its work on behaviour prediction by encoding entity interactions with ConvNets. Intentnet \cite{casas2018intentnet} proposes to jointly detect traffic participants (mostly focused on vehicles) and predict their trajectories using raw LiDAR pointcloud and rendered \ac{HDmap} information. PRECOG \cite{rhinehart2019precog} aims to capture the future stochasiticity by flow-based generative models. Furthermore, MultiPath \cite{chai2019multipath} uses ConvNets as encoder and adopts pre-defined trajectory anchors to regress multiple possible future trajectories. 

As observed, recent \ac{DL}-based techniques use relatively complicated filters to predict, in an accurate way, the spatial features of the obstacles in the scene, increasing the complexity and computational cost of the system. On the other hand, traditional methods for behaviour prediction are rule-based, where multiple behaviour hypothesis are generated based on constraints from the road maps. As stated above, road maps present some clear advantages over other perception sensors. Then, \acp{HDmap} can be an additional sensor that cannot fail unless the road infrastructure changes, providing meaningful, accurate and useful information in real-time operation.  

Regarding this, we design a Map Monitor in charge of monitoring the surrounding area of the vehicle. The inputs of the Map Monitor are the information provided by the Map Parser module (in charge of getting the information of the map from the \ac{HDmap} file and transform it into custom classes that can be used by other modules like Planning or Perception) and the waypoint route previously obtained by the path planner. The main goal of the Map Monitor is to only monitor the most relevant map elements around the ego-vehicle given the route provided by the global planner (or a new route if the local planer decides to recalculate the route). This Map Monitor was published (where I am a co-author) in the following conference paper \cite{diaz2022hd}: "HD maps: Exploiting OpenDRIVE potential for Path Planning and Map Monitoring", 2022 IEEE Intelligent Vehicles Symposium (IV), p. 1211-1217. 

First, the path planner returns the route that is divided in segments separated by a given distance and calculates in which segment of the route the ego-vehicle is found, activating a flag in such a way the Map Monitor can start operating. Otherwise, in case the ego-vehicle cannot be located inside the route, the Map Monitor is deactivated. Secondly, a monitor callback is called periodically every time the ego-vehicle status (position, velocity, orientation, etc.) is received, as observed in Figure \ref{fig:chapter_4_SmartMOT/IV_2021}. This callback evaluates, if the Map Monitor module is active, calculates the monitored elements frontwards and backwards for a given distance which is proportional to the ego-vehicle velocity given a braking distance linear model that establishes a linear regression between two arrays of velocity and braking distance data. Nevertheless, we make use of a threshold distance to still monitor the environment if the ego-vehicle is stopped. %We will detail the corresponding hyperparameters in Chapter \ref{sec:4_mot_and_euroncap}.

The monitored elements are:

\begin{itemize}
	
	\item \textbf{Standard Lanes}: Current, back and the corresponding left and right lanes. Current lane is monitored from current position to a dynamic distance depending on the velocity of the ego-vehicle. Back lane is monitored from current position to back a proportional distance of the dynamic current lane obtained distance. Left and right lanes are monitored the same distance that current and back only if the lane marking from the \ac{HDmap} data allows the lane change.
	
	\item \textbf{Intersection Lanes}: Other lanes that intersect the current monitored lane are checked. Intersection lanes can have different roles: split (1 lane splits into 2 or more), merge (2 or more lanes merge into 1) and cross (a lane crosses a part of the current lane). To calculate the intersection lanes, each lane of every junction (junctions are areas where more than 2 roads meet) in the current lane is evaluated. The polygon of each lane is calculated and evaluated if is inside the polygon of the current lane. It is important to consider that roundabouts are considered as a set of multiple junctions. 
	
	\item \textbf{Regulatory Elements}: The monitored elements are stops, giveaways, traffic lights, speed limits and crosswalks. The regulatory elements are only monitored for the next intersection affecting the route. 
	
\end{itemize}

An example of our map monitor module in the CARLA simulator \cite{dosovitskiy2017carla} using the \ac{RVIZ} \cite{quigley2009ros} may be observed in Figure \ref{fig:chapter_4_SmartMOT/monitored_area_CARLA_ROS}.

\begin{figure}[h] 
	\centering
	\includegraphics[width=0.6\textwidth]{chapter_4_SmartMOT/lane_types.pdf}
	\captionsetup{justification=justified}
	\caption[Monitored area in the CARLA simulator using the \ac{RVIZ} tool]{Monitored area in the CARLA simulator using the \ac{RVIZ} tool. It can be appreciated the visualization of the global route, standard lanes, intersection lanes and different traffic lights (regulatory elements). Note that the relevant traffic light is coloured while remaining ones are masked.}
	\label{fig:chapter_4_SmartMOT/monitored_area_CARLA_ROS}
\end{figure}

As illustrated in Figure \ref{fig:chapter_4_SmartMOT/IV_2021}, once the object detections have been provided and the monitored area has been computed, the Monitored Lanes-based Attention Module helps us to increase the efficiency and robustness of the system to avoid tracking and predicting all obstacles in the environment, which would escalate the computational cost especially in arbitrarily complex urban scenario. 

\begin{figure}[h] 
	\centering
	\includegraphics[trim=0 1cm 3cm 1cm, width=0.8\textwidth]{chapter_4_SmartMOT/monitored_lanes_based_attention_module.pdf}
	\captionsetup{justification=justified}
	\caption{Overview of our proposed Monitored Lanes-based Attention module to filter non-relevant object detections. Note that in order to study if a detection is inside a polygon, we make use of the Jordan's Curve Theorem. Moreover, if the object detection is a \ac{VRU}, the closest polygon is widened towards the sidewalk. Both map, ego-vehicle status and object detection must be in global (map) coordinates.}
	\label{fig:chapter_4_SmartMOT/monitored_lanes_based_attention_module}
\end{figure}

This attention module is not focused on \ac{DL} because the main purpose is to filter non-relevant obstacles in an efficient and interpretable way, such as agents driving way in opposite direction lanes, parked vehicles or pedestrians who are chatting on the sidewalk. The filtering process carried out by our Monitored Lanes-based Attention Module is illustrated in Figure \ref{fig:chapter_4_SmartMOT/monitored_lanes_based_attention_module} and summarized as follows:

\begin{algorithm}[h]
	\SetAlgoLined
	\KwData{point, polygon}
	\KwResult{isInside}
	crossings $\leftarrow$ 0\;
	\For{i from 0 to (polygon.length - 1)}{
		vertex1 $\leftarrow$ polygon[i]\;
		vertex2 $\leftarrow$ polygon[(i + 1) mod polygon.length]\;
		\If{(point.y > min(vertex1.y, vertex2.y)) \textbf{and} (point.y $\leq$ max(vertex1.y, vertex2.y))}{
			\If{point.x $\leq$ max(vertex1.x, vertex2.x)}{
				\If{vertex1.y $\neq$ vertex2.y}{
					xIntersection $\leftarrow$ (point.y - vertex1.y) * (vertex2.x - vertex1.x) / (vertex2.y - vertex1.y) + vertex1.x\;
					\If{vertex1.x == vertex2.x \textbf{or} point.x $\leq$ xIntersection}{
						crossings $\leftarrow$ crossings + 1\;
					}
				}
			}
		}
	}
	isInside $\leftarrow$ (crossings \% 2 == 1)\;
	\KwRet{isInside}\;
	\caption{Jordan's Curve theorem to determine if a point is inside a polygon}
	\label{alg:4_jordan_curve_theorem}
\end{algorithm}

\begin{enumerate}
	
	\item We determine the lanes of interest around the vehicle provided by the map monitor, until a given threshold, which will depend on the ego-vehicle velocity. The minimum information will be the current front and back lane information (mandatory for the \ac{ACC} and Unexpected \ac{VRU} use cases), as well as left and right lane information, if lane change is available considering the presence of a discontinuous line. Moreover, if an intersection is near the ego-vehicle, other lanes of interest such as merging, splits and intersections are considered (see Figure \ref{fig:chapter_4_SmartMOT/monitored_area_CARLA_ROS}), which are specially useful in urban scenarios.% when the vehicle faces a roundabout or other vehicles incorporation in an intersection.
	
	\item In order to consider an agent as relevant, we study the presence of this agent in our monitored lanes. The main idea is to find the closest polygon segment (two nodes on the left, same on the right) to the agent. To do that, we iteratively compute the $\mathcal{L}_2$ distance between the agent position (transformed into global (\aka \ map) coordinates) and the left way nodes (starting from the beginning) of a certain lane. Note that it is irrelevant to take either the left or right way in terms of observing if the detection is inside a polygon made up by four nodes of the lane, such as there exists the same number of nodes for both ways. For example, in the case of an agent located in front of the vehicle, the distance will decrease since the subsequent left way nodes are closer to the agent. 
	
	\item Once the new calculated distance is greater than the previous value, that means the closest segment with nodes \textit{$N_0$} and \textit{$N_1$} are found. Taking the same lane indexes in the right way, we obtain a four-side polygon in which the detection is evaluated using the Jordan's curve theorem \cite{tverberg1980proof}, as depicted in Algorithm \ref{alg:4_jordan_curve_theorem}. In this theorem, the input parameters are a point and a polygon, where, by means of a simple-yet-accurate ray casting algorithm, a loop is used to iterate over the polygon vertices and performs the necessary checks to determine the number of crossings. The Jordan's Curve Theorem states that a point is inside a polygon if the number of crossings from an arbitrary direction is odd. Consequently, in our particular case, if the object detection lies outside the closest polygon segment, the traffic participant is considered as non-relevant.
	
	\item Nevertheless, despite this proposal is coherent for non-holonomic obstacles with more constrained behaviours like cars, vans or trucks, the behaviour of \acfp{VRU}, is usually difficult to predict. Hence, we widen the closest segment area a certain threshold \textit{L} to the sidewalk so as to track the closest \acp{VRU} to the road. Note that according to the \ac{ITS} society, \acp{VRU} are road users not in a car, bus or truck, generally considered to include pedestrians, motorcycle riders, cyclists, children 7-years and under, the elderly and users of mobility devices.
	
\end{enumerate}

\subsection{BEV Kalman Filter: State Prediction}
\label{subsec:4_smartmot_state_prediction}

Once we have obtained the most relevant \ac{BEV} detections of the environment, a \ac{BEV} \ac{KF} is used to track the objects. To predict the state of object trajectories from the previous frames to the current frame, we approximate objects inter-frame displacement using a constant velocity model, which is independent of other objects in the scene and of the \ac{LiDAR} motion. Regarding this, the estimation of the measured variables in the following frame are:

\begin{equation}
	\begin{split}
		x_{px}(\hat{t})=x_{px}(t)+v_{x} \\
		y_{px}(\hat{t})=y_{px}(t)+v_{y} \\
		s(\hat{t})=s(t)+v_{s} \\
		\theta(\hat{t})=\theta(t)+v_{\theta}
	\end{split}
\end{equation}

Since we formulate the tracking problem over the \ac{BEV} plane, we remove all variables related to the $\textit{z}$-coordinate of the object. On the other hand, since our tracking-by-detection algorithm is inspired by the well-established \ac{SORT} \cite{bewley2016simple} tracking algorithm, originally proposed to track pedestrians using videos as input, some additional variables are included in the object state, such as the aspect ratio and the scale of the bounding box, to help in the tracking stage. The aspect ratio can be defined as the relation between the width and the length of the obstacle. Likewise, the scale represents the area of the target bounding box. Then, the state of each object tracker (usually referred as trajectory tracker in the literature) can be expressed as:

\begin{equation}
	\label{state}
	T_{t}^{j} = [x_{px},y_{px},s,r,\theta,x_{px}^{'},y_{px}^{'},s^{'},\theta^{'}]
\end{equation}

Note that the angular velocity $\theta^{'}$ is used in the state space to improve the prediction of the obstacle in later frames. Furthermore, as shown in \cite{bewley2016simple}, the aspect ratio of the bounding box is considered to be constant. As observed in Figure \ref{fig:chapter_4_SmartMOT/IV_2021}, at every frame \textit{t}, a tuple $T_{t}=[T_{t}^{1},T_{t}^{2}, ...,T_{t}^{M}]$ is returned by the data association module, where each element correspond to an association between a detection and a tracker. Note that $M$ represents the current number of trackers. Then, based on these associations between trackers of the previous frame and current detections, and assuming a 1st-order \ac{KF} (constant velocity model), the tuple $T_{\hat{t}}$ is calculated, where each element corresponds to the predicted trajectory ($T_{\hat{t}}^{j}$) in the current frame $\textit{t}$ expressed as:

\begin{equation}
	\label{est}
	T_{\hat{t}}^{j} = [x_{px}(\hat{t}),y_{px}(\hat{t}),s(\hat{t}),r,\theta(\hat{t}),x_{px}^{'},y_{px}^{'},s^{'},\theta^{'}]
\end{equation}

This tuple of predicted trajectories based on the previous frame associations, in addition to the current frame detections, represents the inputs to the data association algorithm at frame $t$.

\subsection{Data association}
\label{subsec:4_smartmot_data_association}

In order to associate the detections $D_{t}$ and the trackers information after the \ac{KF} state prediction $T_{\hat{t}}$, the \acf{HA} is applied. The resulting affinity matrix presents $N$ rows (number of filtered detections after the monitored lanes-based attention module at frame $t$) and $M$ columns, which correspond to the number of predicted trajectories (\ie \ the trackers) based on the information of frame $t-1$. Each element of the matrix corresponds to the \ac{IOU} in the \ac{BEV} plane between every pair of predicted trajectory and detection. 

Then, following the principles stated in the \ac{HA} stated in Chapter \ref{subsec:3_HA_formulation}, we solve the bipartite graph matching problem, rejecting the matching if the \ac{BEV}-\ac{IOU} metric is lower than a given hyperparameter $IoU_{th}$, giving rise to a set of matched detections ($D_{matched}$) and predicted trackers ($T_{matched}$) with the same length $H$ (the number of matches), as well as a set of unmatched detections ($D_{unmatched}$), where $P = N - H$ is the number of unmatched detections, and a set of unmatched trajectories ($T_{unmatched}$), where $Q = M - H$ is the number of unmatched trackers.

\subsection{BEV Kalman Filter - Object State Update}
\label{subsec:4_smartmot_data_state_update}

As observed in Figure \ref{fig:chapter_4_SmartMOT/IV_2021}, once we have the corresponding sets of matched detections and trajectories, based on the \ac{KF} prediction-update cycle, we update the state space of each trajectory based on its corresponding matched detection. To do that, we use the weighted average between the matched detection values and the state space of the trajectory tracker, according to \cite{kalman1960new}. 

On the other hand, in the same way that \cite{weng20203d}, we appreciate that this state update does not work properly for obstacle orientation. The reason is simple: Unless the object detector is based on sensor fusion and vision information is included, the object detector cannot distinguish if the obstacle is rotated 0 or $\pi$, $\frac{\pi}{2}$ and $\frac{3\pi}{2}$, and so on, around its \textit{z}-axis. That is, the orientation may differ by $\pi$ in two consecutive frames. Then, if no orientation correction is applied, the Kalman Filter associated to the tracker can get easily confused, since it tries to adapts itself to the new orientation value rotating the object by $\pi$ in following frames, giving rise to a low BEV-IoU between new detections and predicted trajectories. 

In that sense, regarding the assumption that obstacles must move smoothly and its orientation cannot be modified by $\pi$ in one frame (0.1 s assuming a frequency of 10 Hz), when this happens the orientation of the corresponding matched detection or matched tracker can be considered wrong. To solve this problem, the detection module only considers angle from 0 to $\pi$ (that is, if an angle exceeds $\pi$, it is substracted to the provided angle). Moreover, if the difference of orientation between a given matched detection and its corresponding matched trajectory is greater than $\frac{\pi}{2}$, as stated before, either the orientation of the detection or the orientation of the tracker is wrong. Finally, we add $\pi$ to the orientation of the tracker with the aim to be consistent with the matched detection. 

\subsection{Deletion and Creation of Track Identities}
\label{subsec:4_smartmot_data_deletion_creation}

When obstacles leave and enter the aforementioned monitored lanes, unique identities must be destroyed or created accordingly. In most tracking algorithms it is known as the Birth and Death Memory, which is based on the set of unmatched trackers and detections provided by the data association algorithm, where the unmatched trackers represent potential objects leaving the monitored area, in the same way that unmatched detections represent potential objects entering in the area of interest. 

In order to avoid tracking of false positives or non-relevant obstacles, a new tracker is not created until the unmatched detection has been continuously detected in the next $f_{min}$ frames. Then, the tracker is initialized with the features of the detected bounding box, and the associated velocities set to zero. Note that, as stated in \cite{bewley2016simple}, since the velocity associated to the measured variables is unobserved at this moment (\ie \, tracker initialization), the covariance initializes the value of the velocities (in the present work, velocity of the $x_{px},y_{px}$ centroid, scale $s$ and rotation angle $\theta$) with large values, reflecting their uncertainty. 

To avoid removing true positives trajectories from the scene, they are not terminated unless they are not detected during consecutive $a_{max}$ frames. This assumption prevents an unbounded growth in the number of localization errors and trackers due to predictions over long duration where the object detector does not provide any correction. Note that since this work does not consider object re-identification for simplicity, an object should leaves the scene and then reappears, according to the \ac{SORT} algorithm, if it is initialized with a new tracker under a new identity. As shown in Figure \ref{fig:chapter_4_SmartMOT/IV_2021}, the inputs to the Matched Trackers module are the updated matched trajectories from the \ac{BEV} \ac{KF} and a set of created and deleted trackers, which jointly represent the input trajectories for the prediction step in the following frame.

\subsection{\ac{CTRV} uni-modal prediction}
\label{subsec:4_smartmot_ctrv_prediction}

The last stage of our SmartMOT pipeline is a physics-based \ac{MP} model to predict the future behaviour of the agents in the short-term. In particular, we make use of the previously studied \ac{CTRV} model. Once the tracker information (position, velocity and orientation) has been retrieved in real-world coordinates (instead of \ac{BEV} image coordinates), we are able to differentiate between static and dynamic agents. Then, given the ego-vehicle status and dynamic agents short-term prediction, we analyze the risk of collision or carry out the state of the current behaviour (\ac{ACC}, Pedestrian Crossing, Give way, and so forth and so on) in such a way SmartMOT can send the corresponding signal to the \ac{DM} layer. %Figure \ref{fig:chapter_4_SmartMOT/filtering_process_example} illustrates an example of the filtering process, tracking-by-detection paradigm and \ac{CTRV} prediction.
\begin{comment}
\begin{figure}[] 
	\centering
	\includegraphics[width=0.8\textwidth]{chapter_4_SmartMOT/filtering_process_example.jpg}
	\captionsetup{justification=justified}
	\caption[Simulation use case of SmartMOT in the \ac{CARLA} simulator]{Simulation use case of SmartMOT in the \ac{CARLA} simulator: Non-relevant objects are filtered by means of the monitored lanes-based attention module. Nevertheless, \acfp{VRU} are considered on the sidewalk if they are close enough to the closest segment of the corresponding monitored lane.}
	\label{fig:chapter_4_SmartMOT/filtering_process_example}
\end{figure} 
\end{comment}

\section{Experimental results}
\label{sec:4_mot}

We validate our proposed tracking algorithm (SmartMOT) using the KITTI \ac{MOT} benchmark \cite{geiger2012we} in a quantitative and qualitative way, as well as using some \ac{LiDAR} recorded data from our campus to observe some qualitative results and a comparison of inference frequency using an edge-computing device and standard Personal Computer (PC) desktop. Finally, we study some interesting scenarios in the \ac{CARLA} simulator where map information is included in order to appreciate how the inference time of the \ac{MOT} may be drastically reduced when the system pays attention to the most relevant agents around the \ac{ADS} according to the current traffic scenario. 

% Then, we conduct an interesting study of how integrating the monitored area can reduce the risk of collision and/or the impact velocity on the \ac{VRU}.

\subsection{Dataset}
\label{subsec:4_mot_dataset}

In order to evaluate our proposed \ac{MOT} system pipeline, we carry out the evaluation in the KITTI \ac{MOT} benchmark \cite{geiger2012we} based on the method proposed by \cite{weng20203d}. This is composed of 29 testing and 21 training/validation video sequences, where each sequence is provided with RGB images (left and right camera of the stereo pair), \ac{LiDAR} point-cloud and the corresponding calibration file. Since KITTI does not provide any annotation (\ie, the ground-truth) for the testing split, we decided to evaluate our system in the training/validation split. Moreover, although KITTI distinguish among eight different classes for the object type, our work focus on the car subset, since it is the class that contains the most number of instances over the whole benchmark.

\subsection{Multi-Object Tracking metrics}
\label{subsec:4_mot_metrics}

Mainstream metrics applied to MOT systems are extracted from CLEAR \ac{MOT} metrics \cite{bernardin2008evaluating}, such as \acf{MOTA} and \acf{MOTP}. These metrics provide a comprehensive assessment of tracking performance by considering aspects such as accuracy, precision, and overall performance. Now, the main metrics are described:

\subsubsection{Multi-Object Tracking Accuracy (MOTA)}
\label{subsubsec:4_MOTA}

The \acf{MOTA} metric is commonly used to evaluate the performance of multi-object tracking algorithms. It measures the overall tracking accuracy by considering the \acp{FP}, \acp{FN}, and \ac{IDS} in the tracking results. The formula for calculating \ac{MOTA} is given as:

\begin{equation}
	MOTA = 1 - \frac{{\text{{FN}} + \text{{FP}} + \text{{IDS}}}}{{\text{{GT}}}}
\end{equation}

where:

\begin{itemize}
	
	\item \textbf{FN (False Negatives)} represents the number of ground-truth objects that were not correctly detected by the tracking algorithm.
	
	\item \textbf{FP (False Positives)} represents the number of false detections made by the tracking algorithm.
	
	\item \textbf{IDS (Identity Switches)} represents the number of times the algorithm incorrectly switches the identity of a tracked object.
	
	\item \textbf{GT (Ground-Truth)} represents the total number of ground-truth objects in the video sequence.
	
\end{itemize}

A higher \ac{MOTA} value indicates better tracking accuracy, with a perfect tracking result yielding MOTA = 1.

\subsubsection{Multi-Object Tracking Precision (MOTP)}
\label{subsubsec:4_MOTP}

The \acf{MOTP} metric is used to assess the localization accuracy of a \ac{MOT} algorithm. It measures the average precision of the tracked object positions by considering the distance between the predicted locations and their corresponding ground truth locations. The formula for calculating \ac{MOTP} is given as:

\begin{equation}
	MOTP = \frac{{\sum_{{i=1}}^{{N}} d_i}}{{N}}
\end{equation}

where:

\begin{itemize}
	
	\item \textbf{\(N\)} represents the total number of matched object pairs between the predicted and ground truth locations.
	
	\item \textbf{\(d_i\)} represents the $\mathcal{L}_2$ between the predicted location and the ground-truth location for the \(i\)-th matched object pair.
	
\end{itemize}

The \ac{MOTP} metric ranges between 0 and 1, with a higher value indicating better localization accuracy. A perfect tracking result with exact object positions would yield \ac{MOTP} = 1.

\subsubsection{Integral metrics: AMOTA and AMOTP}
\label{subsubsec:4_integral_metrics}

Previous \ac{MOT} metrics analyze the \ac{MOT} system performance at a given threshold, not taking into account the confidence provided by the object detector and possibly misunderstanding the capability of the method. That means they do not take into account the full spectrum of precision and accuracy over different thresholds. Moreover, as discussed by AB3DMOT \cite{weng20203d}, KITTI 2DMOT evaluates the performance \ac{MOT} by means of aforementioned traditional CLEAR \ac{MOT} metrics, where the detected 3D bounding box onto the image plane. As expected, this does not demonstrate the full strength of 3D \ac{DAMOT}. 

In that sense, AB3DMOT \cite{weng2019baseline} recently presented a 3D extension of the KITTI 2DMOT evaluation, known as KITTI-3DMOT, which introduces two new integral \ac{MOT} metrics to solve the problem of evaluating the \ac{MOTA} and \ac{MOTP} of the system across all detection thresholds, known as \ac{AMOTA} and \ac{AMOTP}, as shown in Equation \ref{eq:4_amota}:

\begin{equation}
	\label{eq:4_amota}
	AMOTA = \frac{1}{L}\sum_{\{\frac{1}{L},\frac{2}{L},...,1\}}(1-\frac{FP+FN+IDS}{num_{gt}})
\end{equation}

Where $L$ is the number of different recall values. Note that \ac{IDS}, \acp{FP} and \acp{FN} are modified according to the results of each threshold value. Likewise, \ac{AMOTP} can be estimated by integrating the \ac{MOTP} metric across all recall values. We will use these integral metrics, in addition to the aforementioned CLEAR \ac{MOT} metrics, to demonstrate the effectiveness of our tracking pipeline. Note that, since both \ac{AMOTA} and \ac{AMOTP} metrics are the integral of the corresponding metrics over different detection thresholds, as expected, the follow the same principle: the higher, the better. Note that in our proposed method (\ac{BEV} based), in order to compare our results with other \ac{SOTA} approaches in the 3D space using the aforementioned KITTI-3DMOT tool, we simply retrieve $z$-information (centroid, bounding box height) from the object detection stage.

\subsection{Comparison with the State-of-the-Art}
\label{subsec:4_mot_leaderboard}

We compare our proposed SmartMOT pipeline (PointPillars as 3D object detector \cite{lang2019pointpillars}, \ac{BEV} \ac{KF} as data estimator and \ac{HA} as data association algorithm excluding the map monitoring and filtering process in this case), against modern open-sourced 3D MOT systems such as mmMOT \cite{zhang2019robust}, FANTrack \cite{baser2019fantrack} and Monocular3D \cite{weng2019monocular} using the proposed KITTI-3DMOT. Results are observed in Table \ref{table:4_MOT_pipelines_validation}, where we achieve results that are up-to-pair with other \ac{SOTA} tracking methods. Note that these results were obtained with default values of the hyperparameters in the tracking stage ($age_{max}$ = 1, $min_{hits}$ = 1, $IoU_{thr}$ = 0.1). 

\begin{table}[h]
	\captionsetup{justification=justified}
	\caption[Comparative of Multi-Object Tracking pipelines using the KITTI-3DMOT evaluation tool in the validation set (car class)]{Comparative of Multi-Object Tracking pipelines using the KITTI-3DMOT evaluation tool in the validation set (car class). We bold the best results in \textbf{black} and the second best in \boldblue{blue} for each metric.}
	\label{table:4_MOT_pipelines_validation}
	\centering
	\resizebox{\linewidth}{!}{
		\begin{tabular}{c | ccccc}
			\toprule
			Method & \ac{AMOTA} [\%] $\uparrow$ & \ac{AMOTP} [\%] $\uparrow$ & \ac{MOTA} [\%] $\uparrow$ & \ac{MOTP} [\%] $\uparrow$ & \ac{IDS} $\downarrow$  \\
			\midrule
			mmMOT \cite{zhang2019robust} & 33.08 & 72.45 & 74.07 & \boldblue{78.16} & \boldblue{10}  \\
			FANTrack \cite{baser2019fantrack} & \bf{40.03} & \boldblue{75.01} & \boldblue{74.30} & 75.24 & 35 \\
			Monocular 3D \cite{weng2019monocular} & 31.37 & 64.29 & 62.38 & 68.26 & \bf{1} \\
			\midrule
			Ours (SmartMOT \cite{gomez2021smartmot} (tracking only)) & \boldblue{39.90} & \bf{79.31} & \bf{94.20} & \bf{82.06} & 150 \\ 
			\bottomrule
		\end{tabular}
	}
\end{table}

For a deeper information of these hyperparameters, we refer the reader to the next section. Note that in terms of the 3D object detection stage (Section \ref{subsec:4_smartmot_detection}), we set the size of the real-world grid \textit{grw} (width)=50m (25m to the left/right, respectively) and \textit{grh} (height)=140m (70m to the bottom/top, respectively), considering the ego-vehicle is in the center of the grid, as depicted in Figure \ref{fig:chapter_4_SmartMOT/ITSC_2020_coordinates_conversion}. On the other hand, since the \ac{MOT} is performed in the \ac{BEV} image space in $pixels$, in order to compute the corresponding equivalences, we set $gph$ (grid width in $px$) = 1000, in such a way $gpw$ $\approx$ 358 $px$. Note that the size of the grid both in m and $px$ is empiric and can be manually adjusted.  

% We evaluate our system using the KITTI-3DMOT evaluation tool proposed by \cite{weng20203d}, obtaining the results summarized in Table \ref{table:4_MOT_pipelines_validation}. In this table, we compare our numbers with the obtained by the representative state-of-the-art MOT system, AB3DMOT \cite{weng20203d}, with the following parameters: $IoU_{th}$ = 0.1 in the data association module, $f_{min}$ = 3 and $a_{max}$ = 2, and for two different object detectors: Pointrcnn \cite{shi2019pointrcnn} and Monocular 3D \cite{weng2019monocular}. Additionally, we include our previous proposal, which uses PointPillars \cite{lang2019pointpillars} as object detector, with an $IoU_{th}$ = 0.1 in the data association module, and $f_{min}$ = 1, $a_{max}$ = 3. Best results are coloured in black and the second best in blue. It can be appreciated the individual effect of using different 3D object detectors as well as using different hyperparameters in terms of tracking configuration. We get the best performance in three evaluated parameters, as well as the second best results in terms of MOTA, significantly improving our previous results for all parameters. Even though we do not overcome the results obtained by AB3DMOT using \cite{pointrcnn} as object detector, these are promising results since PointPillars configurations run at least twice faster with respect to remaining configurations, dealing with the real-time requirement in terms of autonomous driving.

\subsubsection{Ablation study}
\label{subsubsec:4_mot_ablation}

Once we decide to implement a specific tracking-by-detection configuration, we carry out an ablation study to analyze the influence of maximum age, minimum number of hits and minimum threshold in the data association cost matrix to achieve the best tracking results. The main hyperparameters are:

\begin{itemize}
	
	\item \textbf{$age_{max}$}: Maximum number of frames for a tracker to be associated again to a certain detection.
	
	\item \textbf{$min_{hits}$}: Minimum number of consecutive frames in which a tentative tracker must be associated to a detection to be considered as an actual tracker.
	
	\item \textbf{$IoU_{thr}$}: Threshold to match a predicted trajectory and a detection in the data association module.
	
\end{itemize}

Table \ref{table:4_MOT_ablation} shows an ablation study by modifying these hyperparameters ($age_{max}$, $min_{hits}$ and $IoU_{thr}$ respectively), where we follow the principles stated by AB3DMOT \cite{weng2019baseline}. In terms of maximum age, the baseline is 1 (\ie \ if a tracker disappears more than one frame, it is deleted from the set of trajectories), and it is changed to 2 and 3. Moreover, the number of minimum hits is 1 by default, and studied in the case of 3 and 5 minimum hits to consider a detection as a tracker. Finally, the authors in AB3DMOT \cite{weng20203d} \cite{weng2019baseline} change the minimum \ac{IOU} to consider a possible association among detections and trackers in the affinity matrix from 0.01 to 0.1 and 0.25. Note that these ones represent minimum values, and then the \ac{HA} will optimize the combinations to have the highest \ac{IOU} in every case. Even though typical numbers for \ac{IOU} are 0.25, 0.5 and 0.7 in 2D object detection or tracking, since our evaluation is conducted in the 3D space, the risk of occlusions is noticeable decreased in such a way there will not be many associations associated to a particular tracker and vice-versa.

In our particular case, with an $IoU_{thr}$ of 0.01 we get quite similar results in terms of \ac{MOTA} and \ac{MOTP} as in the 0.1 case, decreasing from 150 to 54 the number of identity switches. This is coherent since if the \ac{HA} is able to observe more possible combinations, once it computes the optimal one, it will have a better chance of being the real identifier, although the inference time increases. On the other hand, increasing $min_{hits}$ allows us to reduce the identity switching noticeably, overcoming one of the main drawbacks associated to the motion metric proposed by \ac{SORT}. Moreover, modifying the maximum age to consider a tracker has left the scene barely modifies the studied metrics. As illustrated in Table \ref{table:4_MOT_ablation}, our final configuration achieves an impressive number of 2 \ac{IDS} and quite acceptable CLEAR and integral metrics, which are key as a preliminary stage to predict the short-term for each trajectory in the \ac{MP} stage.

\begin{table}[h]
	\captionsetup{justification=justified}
	\caption[Ablation study of the final tracking stage configuration of SmartMOT using the KITTI-3DMOT evaluation tool in the validation set (car class)]{Ablation study of the final tracking stage configuration of SmartMOT using the KITTI-3DMOT evaluation tool in the validation set (car class). We bold the best results in \textbf{black} and the second best in \boldblue{blue} for each metric.}
	\label{table:4_MOT_ablation}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{c c c | c c c c c}
			\toprule
			$age_{max}$ & $min_{hits}$ & $IoU_{thr}$ & \ac{AMOTA} [\%] $\uparrow$ & \ac{AMOTP} [\%] $\uparrow$ & \ac{MOTA} [\%] $\uparrow$ & \ac{MOTP} [\%] $\uparrow$ & \ac{IDS} $\downarrow$  \\
			\midrule
			1 & 1 & 0.1 & \bf{39.90} & 79.31 & 94.20 & 82.06 & 150 \\
			1 & 1 & 0.01 & 39.84 & 70.96 & 95.13 & 81.84 & 54 \\
			1 & 1 & 0.25 & 39.37 & \bf{79.35} & 89.10 & 82.42 & 176 \\
			\boldblue{1} & \boldblue{3} & \boldblue{0.1} & \boldblue{39.54} & \boldblue{71.24} & \boldblue{91.38} & \boldblue{83.23} & \boldblue{2} \\
			1 & 5 & 0.1 & 39.26 & 71.36 & 88.84 & \bf{83.68} & 3 \\
			2 & 1 & 0.1 & 39.49 & 79.24 & 94.91 & 81.48 & 154 \\
			3 & 1 & 0.1 & 39.50 & 79.15 & \bf{95.16} & 81.15 & 152 \\
			\bottomrule
		\end{tabular}
	}
\end{table}

Finally, our final system configuration is as following: We use PointPillars \cite{lang2019pointpillars} as object detector, trained over 1,187,840 training steps using the KITTI benchmark database, the \ac{BEV} \ac{KF} formulated in the previous section, an $IoU_{th}$ = 0.1 as the threshold to associate a detection with a tracker in the data association module, and $min_{hits}$ = 3, $age_{max}$ = 1 values for the birth and death module respectively. 

\subsection{Qualitative results}
\label{subsubsec:4_mot_quali_carla_campus}

In this subsection we may appreciate some qualitative results using SmartMOT in the KITTI \ac{MOT} benchmark in data captured by our real-world prototype and in the \ac{CARLA} simulator. It is important to note that the map monitor and filtering process is only included in simulation. 

\begin{figure}[!h]
	\centering
	\begin{subfigure}{0.45\textwidth}
		\captionsetup{justification=centering}
		\includegraphics[width=\textwidth]{chapter_4_SmartMOT/qualitative/MOT_kitti_lidar_1.PNG}
		\caption{\ac{BEV} \ac{LiDAR} perspective, $t$}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}
		\captionsetup{justification=centering}
		\includegraphics[width=\textwidth]{chapter_4_SmartMOT/qualitative/MOT_kitti_lidar_2.PNG}
		\caption{\ac{BEV} \ac{LiDAR} perspective, $t+20$}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}
		\captionsetup{justification=centering}
		\includegraphics[width=\textwidth]{chapter_4_SmartMOT/qualitative/MOT_kitti_left_camera_1.PNG}
		\caption{Left RGB camera perspective, $t$}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}
		\captionsetup{justification=centering}
		\includegraphics[width=\textwidth]{chapter_4_SmartMOT/qualitative/MOT_kitti_left_camera_2.PNG}
		\caption{Left RGB camera perspective, $t+20$}
	\end{subfigure}
	\captionsetup{justification=justified}
	\caption[SmartMOT in the KITTI \ac{MOT} validation dataset without map monitoring]{SmartMOT in the KITTI \ac{MOT} validation dataset without map monitoring. Velodyne HDL-64 is used as raw data. It can be appreciated two perspectives ((a) (b) \ac{BEV} \ac{LiDAR} and (c) (d) Left RGB camera) in frame $t$ (left column) and $t+20$ (right column).}
	\label{fig:chapter_4_SmartMOT/MOT_KITTI}
\end{figure}

Figure \ref{fig:chapter_4_SmartMOT/MOT_KITTI} illustrates the tracked agents in a standard urban scenario of the KITTI \ac{MOT} dataset in frames $t$ and $t+20$ respectively, where the Velodyne HDL-64 is used as raw data for the object detector. It may be observed how the corresponding agents are correctly estimated, compensating their position with the ego-motion. Moreover, as aforementioned, SmartMOT operates in the \ac{BEV} plane, in such a way we assume the dimensions, in particular the $z$-coordinate of the agents, are the same for the detection and the tracker.

% we reproduce a very common situation (as observed in the KITTI dataset) which is the ego-vehicle driving in narrow streets full of parked obstacles aside, evaluating its performance in night conditions. Despite this is probably the major disadvantage when using camera information (very poor performance in night conditions), we get impressive results in this situation, as illustrated in Figure \ref{fig:chapter_4_SmartMOT/MOT_CARLA}. This is pretty much coherent since LiDAR sensors are not passive sensors like cameras but they supply their own illumination source, which hits objects the reflected energy is detected and measured by the sensor in order to compute the distance to the object.

On the other hand, in terms of our real-world prototype, we focus on implementing a 360\degree real-time and power-efficient SmartMOT pipeline in an efficient way. Perception systems in \ac{AD} must process a huge amount of information coming from at least one sensor in order to understand the environment. However, the physical space occupied by the processing units in the vehicle or their power consumption are metrics to be deeply analyzed, even more if these processing units will be integrated in an electric vehicle, where the state of the batteries is crucial. 

Regarding this, the \ac{SOTA} approach is to use powerful but power-efficient \ac{AI} embedded systems as computation devices for autonomous machines, since they present a remarkable ratio between performance and power consumption in a reduced-size hardware. In terms of the advantage of using neural networks in \ac{GPU}, these embedded systems present a powerful \ac{GPU} unit as well as fast storages based on Solid State Disks (SSDs) and a large Random Access Memory (RAM) memory size. At the time of writing this paper (2023), the best ratio of performance vs power consumption and size is represented by the NVIDIA Jetson embedded computing boards. NVIDIA Jetson is the world's leading \ac{AI} computing platform for \ac{GPU}-accelerated parallel processing in mobile embedded systems. These kits allow to implement \ac{SOTA} frameworks and libraries to conduct accelerated computing, such as \ac{CUDA}, cuDNN or TensorRT (Tensor RealTime). 

\begin{table}[h]
	\captionsetup{justification=justified}
	\caption[Comparative of inference frequency of SmartMOT between the NVIDIA Jetson AGX Xavier and our PC desktop]{Comparative of inference frequency of SmartMOT between the NVIDIA Jetson AGX Xavier and our PC desktop (Intel Core i7-9700, 16GB RAM) with CUDA-based NVIDIA GeForce RTX 1080 Ti 11GB VRAM.}
	\label{table:4_smartmot_xavier_vs_computer_hz}
	\begin{center}
		\begin{tabular}{ c | c c c}
			\toprule
			Stage & Frequency AGX $\uparrow$ &  Frequency PC $\uparrow$ & Ratio \\
			& Xavier (Hz) & desktop (Hz) & \\
			\midrule
			Detection & 7.3 & \bf{41.7} & 5.7x \\
			\hline
			Tracking & 15 & \bf{101.9} & 6.7x \\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

In this particular work we make use of the NVIDIA Jetson AGX Xavier, one of the most powerful \ac{AI} embedded system specially designed for \acp{ADS}. Table \ref{table:4_smartmot_xavier_vs_computer_hz} shows a comparative between the embedded system and our PC frequency in the inference stage, where the detection stage (PointPillars) is reduced by almost 6 times and the tracking by almost 7 times. Nervertheless, although the detection and tracking frequencies are on the border to be considered real-time according to the requirements of the perception systems for \acp{ADS} (at least 10 Hz), the embedded system consumes 30 W whilst only the 1080 Ti GPU consumes 250 W at full power respectively. 

Considering that the embedded system computation power is reduced by 6.2 times (average between the detection and tracking frequency ratios) but only the \ac{GPU} (not considering the whole PC desktop) presents a power consumption 8.3 higher, it makes the NVIDIA Jetson AGX Xavier a better suitable option for large scale-deployment in the \ac{AD} field rather than using heavy desktop graphic cards. Distributing several sensor processing across multiple embedded systems for parallelization will result in lower power consumption than using conventional \acp{GPU} in future \ac{AD} prototypes. 

\begin{figure}[h]
	\centering
	\begin{subfigure}{0.43\textwidth}
		\captionsetup{justification=centering}
		\includegraphics[width=\textwidth]{chapter_4_SmartMOT/qualitative/MOT_campus_lidar_1.PNG}
		\caption{}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.43\textwidth}
		\captionsetup{justification=centering}
		\includegraphics[width=\textwidth]{chapter_4_SmartMOT/qualitative/MOT_campus_lidar_2.PNG}
		\caption{}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.43\textwidth}
		\captionsetup{justification=centering}
		\includegraphics[width=\textwidth]{chapter_4_SmartMOT/qualitative/MOT_campus_left_camera_1.PNG}
		\caption{}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.43\textwidth}
		\captionsetup{justification=centering}
		\includegraphics[width=\textwidth]{chapter_4_SmartMOT/qualitative/MOT_campus_left_camera_2.PNG}
		\caption{}
	\end{subfigure}
	\captionsetup{justification=justified}
	\caption[SmartMOT in our campus with our real-world vehicle without map monitoring]{SmartMOT in our campus with our real-world vehicle without map monitoring. Velodyne VLP-16 is used as raw sensor data. It can be appreciated two perspectives ((a) (b) \ac{BEV} \ac{LiDAR} and (c) (d) Left RGB camera) in frame $t$ (left column) and $t+20$ (right column).}
	\label{fig:chapter_4_SmartMOT/MOT_campus}
\end{figure}

Qualitative results of running our SmartMOT pipeline in our own vehicle, equipped with a VLP-16 \ac{LiDAR} instead of the HDL-64 shown in KITTI, are illustrated in Figure \ref{fig:chapter_4_SmartMOT/MOT_campus}. It can be appreciated that although the obtained results are slightly worse than with the KITTI dataset (equipped with a HDL-64 sensor), we obtain quite promising results, validating the pipeline studied in this work both in terms of accuracy and real-time operation.

\begin{figure}[]
	\begin{subfigure}{\textwidth}
		\includegraphics[width=\textwidth]{chapter_4_SmartMOT/qualitative/smartmot_CARLA_t}
		\label{subfig:chapter_4_SmartMOT/qualitative/smartmot_CARLA_t}
		\caption{Traffic scenario at frame $t$}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\includegraphics[width=\textwidth]{chapter_4_SmartMOT/qualitative/smartmot_CARLA_t+40.png}
		\label{subfig:chapter_8_Applications/qualitative/smartmot_CARLA_t+40}
		\caption{Traffic scenario at frame $t+40$}
	\end{subfigure}
	\captionsetup{justification=justified}
	\caption[Simulation use case processed by SmartMOT (tracking + map monitor filtering + \ac{CTRV} prediction in the \ac{CARLA} simulator)]{Simulation use case processed by SmartMOT (tracking + map monitor filtering + \ac{CTRV} prediction in the \ac{CARLA} simulator). As observed, only the most relevant objects are tracked (\ie \ those which are in the monitored area and may be relevant in the short-term for the ego-vehicle).}
	\label{fig:chapter_4_SmartMOT/SmartMOT_CARLA}
\end{figure}

Finally, we illustrate some qualitative results in \ac{CARLA} with and without the Monitored Lanes-based Attention Module to study the average inference time in several scenarios. Note that this attention module was not applied neither in the KITTI dataset (since \ac{HDmap} information is not provided here) nor in our campus with the real-world vehicle of our research group since map information was not available at the moment of conducting the different experiments. 

One of the best advantages of \ac{CARLA} is the possibility to create ad-hoc urban layouts by means of an OpenSCENARIO \cite{jullien2009openscenario} script definition where town, vehicles, climate conditions and also driving behaviours are defined, helpful to validate \ac{AD} algorithms (specially those focused on the perception layer) under different traffic and weather conditions. 

In this particular case, we design different complex 4-ways intersection in the Town03, where different lanes of interest are monitored (\ie \ current, left, merge, split and intersection). Note that we chose intersections since the number of agents and relevant lanes are noticeable higher than other traffic situations. For simplicity, in order to appreciate the full strength of SmartMOT when applying our proposed Monitored Lanes-based Attention Module, we take the detection \ac{GT} of the simulator until a given threshold (in this case, $150m$), in order to avoid the error propagated from the detection stage in real-world scenarios. Additional qualitative results may be found in \href{https://github.com/Cram3r95/SmartMOT}{SmartMOT} \footnote{https://github.com/Cram3r95/SmartMOT}.

As an example, Figure \ref{fig:chapter_4_SmartMOT/SmartMOT_CARLA} illustrates how only the agents which current positions are within the monitored area are considered as relevant detections (\ie \ agents which are on the left or right lane where a lane change maneuver is possible, agents which are within an intersection lane where there is an explicit risk of collision, etc.), noticeable reducing the average inference time throughout the route since there are way less agents to be tracked. In particular, after running the model several times in the different intersections with an average number of 30 traffic agents (located 360\degree around the ego-vehicle) we report an average inference time of the tracking stage of 27 ms when the map monitor module is not used. This inference time makes sense since it represents a frequency of 37Hz, whilst in Table \ref{table:4_smartmot_xavier_vs_computer_hz} we illustrated a frequency of 101.9Hz with fewer detected obstacles (Figure \ref{fig:chapter_4_SmartMOT/MOT_campus}). On the other hand, considering the same experimental setup (\ie \ same traffic scenario, number of agents and platform, in our case a PC desktop (Intel Core i7-9700, 16GB RAM) with CUDA-based NVIDIA GeForce RTX 1080 Ti 11GB VRAM), when our Monitored Lanes-based Attention module is used in the SmartMOT pipeline, we are able to reduce the inference time of the overall pipeline from 27ms to an average time of 8ms (around 6 agents are considered in the monitored area), that is, about a third of the original inference time when all objects in the environment (including relevant and non-relevant) were tracked. 

In conclusions, with this experimental setup, it takes approximately 1ms to perform state estimation, data association and physics-based. The higher the number of trackers, the longer the inference time of SmartMOT since calculations are not parallelized, so our filtering process helps the overall model to avoid unnecessary calculations which will not be relevant for the ego-vehicle at least in the short-term. 

\begin{comment}
\subsection{EuroNCAP-based validation}
\label{subsec:4_euroncap}

% Check this link: https://cdn.euroncap.com/media/58230/euro-ncap-assessment-protocol-vru-v1003.pdf

A considerable amount of research works and studies, related to pedestrian detection and collision avoidance behavior are present in the literature, where the main objective is to validate the perception and control modules. Nevertheless, as stated before, we aim to demonstrate how incorporating HD map information helps the whole AD stack to anticipate faster the behaviour of the traffic participants in the corresponding traffic scenarios. Then, common metrics for all frameworks must be used to evaluate the whole architecture, where all modules are integrated. Regarding this, New Car Assessment Programs (NCAPs) protocols are introduced, evaluating the safety of vehicles for different traffic situations and Advanced Driver Assistance Systems, such as Child Occupant Protection (COP), Speed Assist Systems (SAS) or Autonomous Emergency Braking (AEB). Euro-NCAP \cite{article_EuroNCAP_2} is introduced in 1997, representing the widely most adopted performance assessment within the scope of the collaboration of European Union countries. China New Car Assessment Program (C-NCAP) \cite{article_CNCAP} is presented (2006) as a research and development benchmark for vehicle manufacturers in Asia, being most of its protocols based on Euro-NCAP. National Highway Transportation Safety Administration (NHTSA), funded in 1970 as an agency of the Department of Transportation of United States, published \cite{article_NHTSA} its guidance documents and regulations on vehicles equipped with ADAS. As observed, these programmes do not present specific protocols in order to evaluate AD stacks, presenting noticeable differences, such as different scenarios, parameters and evaluation metrics. 

Regarding this, in order to evaluate SmartMOT, we adopt the validation method proposed by \cite{gutierrez2021validation}, which proposes to generalize the \ac{VRU} v.10.0.3 protocol \cite{web_VRU_assessment_protocol}, representing a baseline to compare the performance of different pipelines for the particular situation (both in simulation and real-world) of an Unexpected Vulnerable Road User (VRU) jumping into the road during the navigation, where an Autonomous Emergency Braking (AEB) behaviour must be executed. 

\subsubsection{Implementation details of the EuroNCAP scenario}
\label{subsubsec:4_euroncap_implementation_details}

Figure \ref{fig:chapter_4_SmartMOT/CPNA_scenario} illustrates this traffic situation, in which the VRU (a pedestrian in this particular case) starts in the closest sidewalk to the vehicle in a perpendicular position to the vehicle orientation. Once the vehicle starts the navigation and the L2 distance between the ego-vehicle centroid and the VRU centroid is lower than a certain threshold \textit{d}, the VRU starts its path to unexpectedly cross the road in such a way the ego-vehicle must detect, track and forecast its future trajectory in order to avoid the collision or at least reduce the impact velocity as much as possible. Then, the protocol consists on reproducing the CPNA crash avoidance scenario, with a fixed VRU velocity (\(v_p\)) of 5 km/h and a variable ego-vehicle velocity that ranges from 10 km/h to 60 km/h. It is important to note that the threshold \textit{d} is not fixed, but it is ego-vehicle velocity dependent, that is, the pedestrian must start walking in such a way the impact point (\(P_I\)) (Figure \ref{fig:4_cpna_scenario}) is in the center of the lane for each particular velocity. 

\begin{figure}[h]
	\centering\includegraphics[width=0.4\textwidth]{chapter_4_SmartMOT/CPNA_scenario.jpg}
	\caption{Car to Pedestrian Nearside Adult (CPNA) scenario}	
	\label{fig:chapter_4_SmartMOT/CPNA_scenario}
\end{figure}

Regarding the evaluation metrics, a score for each test is calculated based on the velocity reduction of the vehicle, as following:

\begin{itemize}
	\item For a vehicle velocity \(v_v\) less than or equal to 40km/h:
	\begin{itemize}
		\item If the vehicle stops without collision, the highest score is achieved:
		\begin{equation}
			score_{test} = score_{max}
			\label{eq1}
		\end{equation}
		
		\item Otherwise, if the vehicle collides, its score is defined as follows:
		\begin{equation}
			score_{test} = \frac{v_{test}-v_{impact}}{v_{test}} \cdot score_{max}
			\label{eq2}
		\end{equation}
	\end{itemize}
	\item For \(v_v\) higher than 40km/h:
	\begin{itemize}
		\item If the vehicle is able to reduce its speed in at least 20 km/h, the highest score is achieve:
		\begin{equation}
			v_{impact} \leq v_{test} - 20 \to score_{test} = score_{max}
			\label{eq3}
		\end{equation}
		\item Otherwise, if the vehicle collides at a velocity greater than the velocity under test less a threshold of 20 km/h, no score is achieve:
		\begin{equation}
			v_{impact} > v_{test} - 20 \to score_{test} = 0
			\label{eq4}
		\end{equation}
	\end{itemize}
\end{itemize}

Finally, the final score of a particular pipeline is given by the arithmetic mean of the results obtained in each CPNA crash avoidance test for different weather conditions. For further details about the validation protocol, we refer the reader to \cite{gutierrez2021validation}.

\subsubsection{Experimental results in the EuroNCAP-based scenario}
\label{subsubsec:4_euroncap_experimental_results}

In this section we obtain some interesting both qualitative and quantitative results, evaluating our AD stack \cite{gomez2021train} in the CPNA crash avoidance scenario using two different perception layer strategies. On the one hand, we implement the perception module stated by \cite{gomez2020real} which tracks all objects in the environment regardless their topological information and considers a naive velocity dependent rectangular monitored area in front of the vehicle to determine the distance to the nearest object in the route as well as to predict the collision. On the other hand, we use SmartMOT to track and predict the future trajectories of only the most relevant obstacles around the vehicle, that is, those in which the human in manual driver should pay attention throughout the route, such as VRUs close to the road, vehicles in intersections and lanes where the lane change maneuver is allowed, etc. Qualitative results may be found in the following play list \href{https://cutt.ly/uk9ziaq}{SmartMOT} \footnote{SmartMOT: https://cutt.ly/uk9ziaq}, where the SmartMOT performance is illustrated. 

Regarding urban environment complexity, in order to validate a whole AD architecture the system must be tested in countless environments and scenarios, which would escalate the cost and development time exponentially with a physical approach. Considering this, the use of photo-realistic simulation (virtual development and validation testing) and an appropriate design of the driving scenarios are the current keys to build safe and robust AV. 

In our work we propose the use of CARLA (CAR Learning to Act) \cite{dosovitskiy2017carla} as the best open-source simulator to reach our goals, taking even more importance when analyzing the behaviours the vehicle can face in these complex traffic scenarios. One of the best advantages of CARLA is the possibility to create ad-hoc urban layouts, useful to validate the navigation architecture in challenging driving scenarios. This code can be downloaded from the ScenarioRunner repository, associated to the CARLA GitHub. The ScenarioRunner is a module that allows the user to define and execute traffic scenarios for the CARLA simulator. In the present case, we define several scenarios according to the CPNA crash avoidance traffic situation, modifying the velocity of the ego-vehicle and the presence of other traffic participants. All test were carried out in a PC desktop (Intel Core i7-9700k, 32GB RAM with CUDA-based NVIDIA GeForce RTX 2080 Ti 11GB VRAM), using the version 0.9.10.1 version of CARLA as well as the corresponding ROS Bridge, responsible of communicating the CARLA environment with our ROS-based architecture, and ScenarioRunner modules. In particular, we make use of the OpenScenario standard, supported by ScenarioRunner, where both the VRU and ego-vehicle features can be modified to accomplish the Euro-NCAP requirements. % Due to size constraint of this paper, we do not validate the performance of our architecture for different weather conditions but only in daytime conditions. 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{chapter_4_SmartMOT/quantitative/unexpected_vru_temporal_graph.jpg}
	\captionsetup{justification=justified}
	\caption[Unexpected Vulnerable Road User (VRU) temporal diagram]{Unexpected Vulnerable Road User (VRU) temporal diagram. At the top, the events produced by our monitors and map manager modules. In the middle, the selector, and start (background) PNs of our decision-making layer. At the bottom, the velocity of the car throughout the navigation}
	\label{fig:chapter_4_SmartMOT/quantitative/unexpected_vru_temporal_graph}
\end{figure}

In order to appreciate the behaviour of the vehicle during navigation, we incorporate an illustrative temporal diagram (Figure \ref{fig:chapter_4_SmartMOT/quantitative/unexpected_vru_temporal_graph}), representing a powerful manner to qualitatively validate how the architecture behaves in an end-to-end manner, since we can observe how the car behaves considering the different actions and events \cite{gomez2021train} provided by the executive layer, which is actually the output of the whole architecture before sending commands to the motor. As observed, the ego-vehicle starts far away from the adversary and starts its navigation. At second 22 a pedestrian that is in the sidewalk is detected, so tracking-by-detection and subsequent motion prediction must be carried as fast as possible to avoid collision, since the scenario is designed in such a way that the pedestrian must start walking in such a way the impact point (\(P_I\)) (Figure \ref{fig:chapter_4_SmartMOT/CPNA_scenario}) is in the center of the lane for each particular velocity. After that, our prediction module intersects the ego-vehicle forecasted trajectory and the pedestrian forecasted trajectory. If the Intersection over Union (IoU) is greater than a threshold (in this case, 0.01), a \textit{predictedcollision} flag is activated and the low-level (reactive) control, which always runs in the background of the decision-making layer, performs an emergency break until the car is stopped in front of the obstacle. Navigation is resumed once the obstacle leaves the driving lane. Table \ref{table:4_CPNA_results} compares the performance of the architecture by implementing \cite{gomez2020real} and a rectangular monitorized lane to retrieve the nearest object in route and predict collision against our proposal, where it can be appreciated that for velocities greater than 40 km/h, using HD map semantic and geometric information gives the car a valuable reaction time to anticipate the VRU behaviour and avoid the collision, achieving the highest score. 

\begin{table}[h]
	\centering
	\captionsetup{justification=justified}
	\caption{Comparison of our two different perception strategies in the Car to Pedestrian Nearside Adult (CPNA) scenario. We bold the best score in \textbf{black}.}
	\label{table:4_CPNA_results}
	\begin{tabular}{c | c | c  c | c  c  } 
		\toprule 
		\multicolumn{6}{c}{\textbf{CPNA}}\\
		\multirow{2}{*}{\textbf{\(v_{test}\)}} & \multirow{2}{*}{\textbf{\(score_{max}\)}} & \multicolumn{2}{c}{\textbf{Rectangular area + \cite{gomez2020real}}} & \multicolumn{2}{c}{\textbf{SmartMOT}} \\ 
		& & \(v_{impact}\) & \(score\) & \(v_{impact}\) & \(score\) \\
		\midrule
		10 km/h & 1.00 & 0.0 km/h & 1.00 & 0.0 km/h & 1.00 \\
		20 km/h & 1.00 & 0.0 km/h & 1.00 & 0.0 km/h & 1.00 \\
		30 km/h & 2.00 & 0.0 km/h & 2.00 & 0.0 km/h & 2.00 \\
		40 km/h & 3.00 & 0.0 km/h & 3.00 & 0.0 km/h & 3.00 \\
		50 km/h & 2.00 & 23.82 km/h & 2.00 & 0.0 km/h & 2.00 \\
		60 km/h & 1.00 & 44.23 km/h & 0.00 & 0.0 km/h & 1.00  \\
		\midrule
		\textbf{Total} & 10.00 &  & 9.00 & & \textbf{10.0} \\
		\bottomrule
	\end{tabular}
\end{table}

Figure \ref{fig:4_cpna_results} shows different analysis of the CPNA crash avoidance scenario with variable ego-vehicle and the incorporation of other traffic participants in the scenario (\ref{subfig:chapter_4_SmartMOT/euroncap_graphics_c} \ref{subig:chapter_4_SmartMOT/euroncap_graphics_d}). \(T_0\) corresponds with the moment the vehicle either stops or collides, and crosses \textbf{x} represent the moment in which the system sends a predicted collision signal to the executive layer, so it is coherent that crosses in tests where the ego-vehicle collides with the VRU are shifted to the right (prediction collision signal was given in time). Left column tracks all objects around the vehicle and adopts a geometric monitorized area to estimate the nearest distance and predicted collision, whilst right column uses HD map information to help in the Multi-Object Tracking and motion prediction tasks, monitoring only the most relevant traffic participants around the vehicle that is, the main purpose of SmartMOT.

As observed, using HD map information is able to avoid collision until a ego-vehicle velocity of 80 km/h, where SmartMOT is not able to send a signal of predicted collision (output of the system, as shown in Figure \ref{fig:chapter_4_SmartMOT/IV_2021}) in time, colliding at a velocity of 39.78 km/h. Nevertheless, this velocity at the moment of collision is even lower that the impact velocity (44.23 km/h) when testing the system under 60 km/h condition not using HD map in the MOT stage, illustrating how incorporating additional semantic and geometric map information helps the vehicle to react faster or at least mitigate the effect of collision. Moreover, we simulate both perception strategies using the most common velocities in urban scenarios, which range from 30 to 50 km/h, including \ref{subfig:chapter_4_SmartMOT/euroncap_graphics_c} \ref{fig:chapter_4_SmartMOT/euroncap_graphics_d} static adversaries (in particular, vehicles and pedestrians) which do not actually in the traffic scenario to fulfill the particular requirements stated by \cite{gutierrez2021validation} protocol. As expected, tracking all objects around the ego-vehicle and using the rectangular monitorized area suffers when the number of traffic participants is increased around the ego-vehicle, whilst SmartMOT holds this exponential increase by analyzing the objects and their corresponding role as relevant obstacles considering the information provided by the HD map, avoiding the collision in all situations.

\begin{figure}[h]
	\centering
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{chapter_4_SmartMOT/quantitative/euroncap_without_adversaries_geometric.jpg}
		\caption{Rectangular monitored area without adversaries}
		\label{subfig:chapter_4_SmartMOT/euroncap_graphics_a}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{chapter_4_SmartMOT/quantitative/euroncap_without_adversaries_hdmap.jpg}
		\caption{SmartMOT area without adversaries}
		\label{subfig:chapter_4_SmartMOT/euroncap_graphics_b}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{chapter_4_SmartMOT/quantitative/euroncap_with_adversaries_geometric.jpg}
		\caption{Rectangular monitored area with adversaries}
		\label{subfig:chapter_4_SmartMOT/euroncap_graphics_c}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{chapter_4_SmartMOT/quantitative/euroncap_with_adversaries_hdmap.jpg}
		\caption{SmartMOT area with adversaries}
		\label{subfig:chapter_4_SmartMOT/euroncap_graphics_d}
	\end{subfigure}
	\captionsetup{justification=justified}
	\caption[Analysis of the Car to Pedestrian Nearside Adult (CPNA) crash avoidance scenario with variable ego-vehicle velocity]{Analysis of the Car to Pedestrian Nearside Adult (CPNA) crash avoidance scenario with variable ego-vehicle velocity. Left column (Figures \ref{subfig:chapter_4_SmartMOT/euroncap_graphics_a} and \ref{subfig:chapter_4_SmartMOT/euroncap_graphics_c}) adopts a rectangular monitorized area to estimate the nearest distance and predicted collision, Right column (Figures \ref{subfig:chapter_4_SmartMOT/euroncap_graphics_b} and \ref{subfig:chapter_4_SmartMOT/euroncap_graphics_d}) uses HD map information for this purpose. On the other hand, first row shows the scenario without additional traffic participants, second row analyzes the crash avoidance scenario including additional traffic participants to the road, monitorized sidewalk area and non-relevant sidewalk area. Crosses in the lines represent the moment in which the system sends a predicted collision signal to the executive layer}
	\label{fig:4_cpna_results}
\end{figure}

\end{comment}

\section{Summary}
\label{sec:4_summary}

In this Chapter we propose SmartMOT, an open-source simple-yet-powerful pipeline that fuses the concepts of tracking-by-detection and \ac{HDmap} information to design a real-time and power-efficient \ac{MOT} and uni-modal \ac{MP} pipeline used to track and predict the future trajectories of only the most relevant obstacles around the ego-vehicle, incorporating a Monitored Lanes-based Attention Module to the pipeline, improving the way in which the vehicles are considered as relevant.

Then, experimental results focus on validating the tracking stage (without including map monitoring) in the KITTI dataset where the Velodyne HDL-64 is used, performing an ablation study to choose the final hyperparameters of the proposal. This proposal is also validated using data captured from our \ac{LiDAR} using a Velodyne VLP-16, obtaining successful qualitative results since we do not have \ac{GT} in this case. On top of that, we run the pipeline in a PC desktop and the Jetson NVIDIA AGX Xavier mounted in our real-world vehicle, concluding that even though the PC desktop presents a higher inference frequency, the higher ratio compared to the power consumption saved when using the edge computing device is lower. In that sense, in spite the fact that its integration is more difficult due to the distribution of several sensor processing across multiple embedded systems for parallelization, multiple edge-computing devices will be a preferred option rather than using heavy PC desktops and \ac{GPU} in future \ac{AD} prototypes. 

Finally, the Chapter illustrates some results in \ac{CARLA}, including map monitor information, where it can be clearly appreciated how tracking the most relevant agents of the scene (\ie \ those which are included in the monitored area) can drastically reduce the inference time, making it suitable for real-time classic \ac{MOT} and uni-modal \ac{MP} operation, where no learning-based methods are required to reason what is happening in the traffic scenario, both in terms of physical and social information.

%Once the best hyperparameters are obtain by means of an ablation study, an end-to-end validation of our pipeline in the Unexpected \ac{VRU} scenario is carried out, following an Euro-NCAP-based validation protocol, illustrating how integrating map information in the pipeline can minimize or at least reduce the impact velocity with the \ac{VRU} by reducing the computational complexity of the problem. Moreover, a temporal graph is depicted, representing a very intuitive and powerful manner to appreciate how the integration of this extended version of SmartMOT gives the vehicle a valuable time to anticipate the corresponding behaviours. We hope that our distributed pipeline can serve as a solid baseline on which others can build on to advance the state-of-the-art in fusing perception data and map information to perform real-time motion prediction and decision-making evaluation in arbitrarily complex urban scenarios. 