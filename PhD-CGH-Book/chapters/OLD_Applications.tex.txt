\section{Introduction}
\label{sec:5_introduction}

In this chapter we will detail the experiments carried out to assess the performance of our prediction algorithms, both in terms of accuracy and computational resources (time, \acp{FLOP}, parameters) for real-time applications in the field of \ac{AD}. Regarding the proposed methods, we first make use of some well-established tracking metrics to evaluate the behaviour of our physics-based tracking method; then, we follow the validation method proposed by \cite{gutierrez2021validation} to check how integrating HD map information in the prediction pipeline can be determinant to decrease the risk of collision or at least to minimize the impact velocity in an EURO-NCAP based scenario. After that, we study the prediction metrics obtained in the Argoverse 1 \cite{chang2019argoverse} and Argoverse 2 \cite{wilson2023argoverse} Motion Forecasting datasets. 

Once we have evaluated our different proposals, the best model (including its social and map version) is selected to carry out some challenging applications. First, regarding the decision-making layer using the SMARTS \cite{SMARTS} framework based on the SUMO (Simulation of Urban MObility) \cite{Sumo} simulator, we study the influence of the prediction pipeline when computing the most optimal action for the ego-vehicle in contrast to reactive proposals where only the past observations or current adversaries positions are required. Second, the prediction pipeline is integrated in the \ac{ADS} of our research group using the CARLA \cite{dosovitskiy2017carla} (CAR Learning to Act) to study how the ego-vehicle can leverage the scene understanding and the future behaviour prediction of the agents to improve the overall score by means of a holistic validation using the CARLA Leaderboard, inspired in the well-established NHTSA typology.

 

\section{SmartMOT results}
\label{sec:5_mot_and_euroncap}

As stated in Section \ref{sec:4_smartmot}, SmartMOT is a tracking pipeline that leverages HD map information to subsequently conduct a physics-base unimodal prediction. To validate this algorithm, we first validate the proposed tracking algorithm using the KITTI \ac{MOT} benchmark. Then, we conduct an interesting study of how integrating the monitored area can reduce the risk of collision and/or the impact velocity on the \ac{VRU}.

\subsection{Multi-Object Tracking performance}
\label{subsec:5_mot_results}

In order to evaluate our proposed \ac{MOT} system pipeline, we carry out the evaluation in the KITTI \ac{MOT} benchmark based on the method proposed by \cite{weng20203d}. The KITTI MOT benchmark is composed of 29 testing and 21 training/validation video sequences, where each sequence is provided with the corresponding RGB images (left and right camera of the stereo pair), LiDAR point cloud and the corresponding calibration file. Since KITTI does not provide any annotation (\ie, the groundtruth) for the testing split, we decided to evaluate our system in the training/validation split. Moreover, although KITTI distinguish among eight different classes for the object type, our work focus on the car subset, since it is the class that contains the most number of instances over the whole benchmark.

\subsubsection{Multi-Object Tracking metrics}
\label{subsubsec:5_mot_metrics}

Mainstream metrics applied to MOT systems are extracted from CLEAR MOT metrics \cite{bernardin2008evaluating}, such as MOTA (Multi-Object Tracking Accuracy), MOTP (Multi-Object Tracking Precision), ML/MT (Number of Mostly Lost/Tracked trajectories), IDS (Number of identity swutches), FRAG (Number of fragmentations generated by false negatives) and FN/FP (Number of false negatives/positives). These metrics provide a comprehensive assessment of tracking performance by considering aspects such as accuracy, precision, and overall performance:

\paragraph{MOTA (Multi-Object Tracking Accuracy)}
\label{par:5_MOTA}

The MOTA metric is commonly used to evaluate the performance of multi-object tracking algorithms. It measures the overall tracking accuracy by considering the false positives (FP), false negatives (FN), and identity switches (IDS) in the tracking results. The formula for calculating MOTA is given as:

\begin{equation}
MOTA = 1 - \frac{{\text{{FN}} + \text{{FP}} + \text{{IDS}}}}{{\text{{GT}}}}
\end{equation}

where:

\begin{itemize}
	\item FN (False Negatives) represents the number of ground truth objects that were not correctly detected by the tracking algorithm.
	\item FP (False Positives) represents the number of false detections made by the tracking algorithm.
	\item IDS (Identity Switches) represents the number of times the algorithm incorrectly switches the identity of a tracked object.
	\item GT (Ground Truth) represents the total number of ground truth objects in the video sequence.
\end{itemize}

A higher MOTA value indicates better tracking accuracy, with a perfect tracking result yielding MOTA = 1.

\paragraph{MOTP (Multi-Object Tracking Precision)}
\label{par:5_MOTP}

The MOTP metric is used to assess the localization accuracy of a multi-object tracking algorithm. It measures the average precision of the tracked object positions by considering the distance between the predicted locations and their corresponding ground truth locations. The formula for calculating MOTP is given as:

\begin{equation}
MOTP = \frac{{\sum_{{i=1}}^{{N}} d_i}}{{N}}
\end{equation}

where:
\begin{itemize}
	\item \(N\) represents the total number of matched object pairs between the predicted and ground truth locations.
	\item \(d_i\) represents the Euclidean distance between the predicted location and the ground truth location for the \(i\)-th matched object pair.
\end{itemize}

The MOTP metric ranges between 0 and 1, with a higher value indicating better localization accuracy. A perfect tracking result with exact object positions would yield MOTP = 1.

\paragraph{Integral metrics: AMOTA and AMOTP}
\label{par:5_integral_metrics}

Nevertheless, these metrics analyze the DAMOT system performance at a given threshold, not taking into account the confidence provided by the object detector and possibly misunderstanding the capability of the method. That means they do not take into account the full spectrum of precision and accuracy over different thresholds. Moreover, these traditional metrics evaluate the performance of the MOT system on the image plane (by projecting the detected 3D bounding box onto the image plane), which does not demonstrate the full strength of 3D DATMO. In that sense, AB3DMOT \cite{weng20203d} recently presented a 3D extension of the KITTI 2D MOT evaluation, known as KITTI-3DMOT, which focuses on the dimensions, orientation and centroid position of the 3D bounding box instead of the projection onto the image plane to evaluate the performance of the MOT system. Moreover, two new integral MOT metrics are introduced in order to solve the problem of evaluating the MOTA and MOTP of the system across all thresholds, known as AMOTA and AMOTP (Average MOTA and MOTP), as shown in Equation \ref{eq:5_amota}:

\begin{equation}
	\label{eq:5_amota}
	AMOTA = \frac{1}{L}\sum_{\{\frac{1}{L},\frac{2}{L},...,1\}}(1-\frac{FP+FN+IDS}{num_{gt}})
\end{equation}

Where $L$ is the number of different recall values. Note that IDS, FP and FN are modified according to the results of each threshold value. Likewise, AMOTP can be estimated by integrating MOTP across all recall values.

\subsubsection{MOT leaderboard}
\label{subsubsec:5_mot_leaderboard}

\begin{table}[h]
	\caption{Comparative of Multi-Object Tracking pipelines using the KITTI-3DMOT evaluation tool in the validation set (car class). We bold in \bf{black} the best results for each category}
	\label{table:5_MOT_pipelines_validation}
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|c|}
			\hline
			Method & AMOTA &  AMOTP &  MOTA &  MOTP & IDs\\
			& (\%) & (\%) & (\%) & (\%) & \\
			\hline
			SmartMOT \cite{gomez2021smartmot} (tracking only) & 39.90 & \bf{79.31} & \bf{94.20} & 82.06 & 150 \\ 
			using PointPillars \cite{lang2019pointpillars} (Ours) & & & & & \\ \hline
			mmMOT \cite{zhang2019robust} & 33.08 & 72.45 & 74.07 & 78.16 & 10  \\ \hline
			FANTrack \cite{baser2019fantrack} & \bf{40.03} & 75.01 & 74.30 & 75.24 & 35 \\ \hline
			Monocular 3D \cite{weng2019monocular} & 31.37 & 64.29 & 62.38 & 68.26 & \bf{1} \\ \hline
		\end{tabular}
	\end{center}
\end{table}

We compare our proposed \ac{MOT} pipeline (PointPillars as 3D object detector \cite{lang2019pointpillars}, \ac{BEV} Kalman Filter, with the state space specified in Section \ref{sec:4_smartmot} as data estimator and Hungarian algorithm as data association algorithm) against modern open-sourced 3D MOT systems such as mmMOT \cite{zhang2019robust}, FANTrack \cite{baser2019fantrack} and Monocular3D \cite{weng2019monocular} using the proposed KITTI-3DMOT. Results are observed in Table \ref{table:5_MOT_pipelines_validation}, where we achieve results that are on-pair with other \ac{SOTA} tracking methods. Note that these results were obtained with default values of the hyperparameters in the tracking stage ($age_{max}$ = 1, $min_{hits}$ = 1, $IoU_{thr}$ = 0.1). For a deeper information of these hyperparameters, we refer the reader to the next subsection, where we conduct an ablation study to analyze the influence of maximum age or minimum threshold in the data association cost matrix to achieve the best tracking results.

% We evaluate our system using the KITTI-3DMOT evaluation tool proposed by \cite{weng20203d}, obtaining the results summarized in Table \ref{table:5_MOT_pipelines_validation}. In this table, we compare our numbers with the obtained by the representative state-of-the-art MOT system, AB3DMOT \cite{weng20203d}, with the following parameters: $IoU_{th}$ = 0.1 in the data association module, $f_{min}$ = 3 and $a_{max}$ = 2, and for two different object detectors: Pointrcnn \cite{shi2019pointrcnn} and Monocular 3D \cite{weng2019monocular}. Additionally, we include our previous proposal, which uses PointPillars \cite{lang2019pointpillars} as object detector, with an $IoU_{th}$ = 0.1 in the data association module, and $f_{min}$ = 1, $a_{max}$ = 3. Best results are coloured in black and the second best in blue. It can be appreciated the individual effect of using different 3D object detectors as well as using different hyperparameters in terms of tracking configuration. We get the best performance in three evaluated parameters, as well as the second best results in terms of MOTA, significantly improving our previous results for all parameters. Even though we do not overcome the results obtained by AB3DMOT using \cite{pointrcnn} as object detector, these are promising results since PointPillars configurations run at least twice faster with respect to remaining configurations, dealing with the real-time requirement in terms of autonomous driving.

\subsubsection{MOT ablation}
\label{subsubsec:5_mot_ablation}

Once we decide to implement a specific tracking-by-detection configuration, we carry out an ablation study that allows us to observe the performance in function of the tracking hyperparameters. These are:

\begin{itemize}
	\item \textbf{$age_{max}$: } Maximum number of frames for a tracker (Kalman Filter) to be associated again to a certain detection
	\item \textbf{$min_{hits}$: } Minimum number of consecutive frames in which a tentative tracker must be associated to a detection to be considered as an actual tracker
	\item \textbf{$IoU_{thr}$: } Threshold to match a predicted trajectory and a detection in the data association module
\end{itemize}

Table \ref{table:5_MOT_ablation} shows an ablation study by modifying these parameters. With a threshold $IoU_{thr}$ of 0.01 we get quite similar results in terms of MOTA and MOTP, decreasing by 36 $\%$ the number of identity switches (150 to 54). On the other hand, increasing the minimum number of hits allows us to reduce the identity switching noticeably, overcoming one of the main drawbacks associated to the motion metric proposed by SORT. Moreover, modifying the maximum age to consider a tracker has left the scene barely modifies the studied metrics. Finally, we bold in black the best values for each metric and in blue our final configuration ($age_{max}$ = 1, $min_{hits}$ = 3, $IoU_{thr}$ = 0.1) that achieves an impressive number of 2 identity switches and quite acceptable CLEAR and integral metrics, which are key as a preliminary stage to predict the short-term for each trajectory in the motion prediction stage.

\begin{table}[h]
	\caption{Ablation study of the final tracking stage configuration of SmartMOT using the KITTI-3DMOT evaluation tool in the validation set (car class). We bold the best results in \textbf{black} and the second best in \boldblue{blue} for each metric}
	\label{table:5_MOT_ablation}
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|c|c|c|}
			\hline
			$age_{max}$ & $min_{hits}$ & $IoU_{thr}$ & AMOTA & AMOTP & MOTA & MOTP & IDs \\
			& & & (\%) & (\%) & (\%) & (\%) & \\
			\hline
			1 & 1 & 0.1 & \bf{39.90} & 79.31 & 94.20 & 82.06 & 150 \\
			\hline
			1 & 1 & 0.01 & 39.84 & 70.96 & 95.13 & 81.84 & 54 \\
			\hline
			1 & 1 & 0.25 & 39.37 & \bf{79.35} & 89.10 & 82.42 & 682 \\
			\hline
			\boldblue{1} & \boldblue{3} & \boldblue{0.1} & \boldblue{39.54} & \boldblue{71.24} & \boldblue{91.38} & \boldblue{83.23} & \boldblue{2} \\
			\hline
			1 & 5 & 0.1 & 39.26 & 71.36 & 88.84 & \bf{83.68} & 3 \\
			\hline
			2 & 1 & 0.1 & 39.49 & 79.24 & 94.91 & 81.48 & 154 \\
			\hline
			3 & 1 & 0.1 & 39.50 & 79.15 & \bf{95.16} & 81.15 & 152 \\
			\hline
		\end{tabular}
	\end{center}
\end{table}

Finally, our final system configuration is as following: We use PointPillars trained over 1,187,840 training steps using the KITTI MOT benchmark database, the BEV Kalman Filter formulated in the previous section, an $IoU_{th}$ = 0.1 as the threshold to associate a detection with a tracker the data association module, and $min_{hits}$ = 3, $age_{max}$ = 1 values for the birth and death module respectively. 

\subsubsection{Qualitative results in CARLA and our campus}
\label{subsubsubsec:5_mot_quali_carla_campus}

In this subsection we may appreciate some qualitative results both in simulation and in our real-world prototype using SmartMOT. One of the best advantages of CARLA is the possibility to create ad-hoc urban layouts by means of an OpenSCENARIO \cite{jullien2009openscenario} script definition where town, vehicles, climate conditions and also driving behaviours are defined, helpful to validate \ac{AD} algorithms (specially those focused on the perception layer) under different traffic and weather conditions. 

In terms of simulation, we reproduce a very common situation (as observed in the KITTI dataset) which is the $ego\_vehicle$ driving in narrow streets full of parked obstacles aside, evaluating its performance in night conditions. Despite this is probably the major disadvantage when using camera information (very poor performance in night conditions), we get impressive results in this situation, as illustrated in Figure \ref{fig:5_MOT_quali_carla}. This is pretty much coherent since LiDAR sensors are not passive sensors like cameras but they supply their own illumination source, which hits objects the reflected energy is detected and measured by the sensor in order to compute the distance to the object.

\begin{figure}[h]
	\centering
	\begin{subfigure}{0.24\textwidth}
		\captionsetup{justification=centering}
		\includegraphics[width=4cm, height=6cm]{5_qualitative_mot_carla/night_1.PNG}
		\caption{}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.24\textwidth}
		\captionsetup{justification=centering}
		\includegraphics[width=4cm, height=6cm]{5_qualitative_mot_carla/night_2.PNG}
		\caption{}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.24\textwidth}
		\captionsetup{justification=centering}
		\includegraphics[width=4cm, height=6cm]{5_qualitative_mot_carla/night_3.PNG}
		\caption{}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.24\textwidth}
		\captionsetup{justification=centering}
		\includegraphics[width=4cm, height=6cm]{5_qualitative_mot_carla/night_4.PNG}
		\caption{}
	\end{subfigure}
	\caption[Detection and Tracking of Multiple Objects in the CARLA simulator]{Detection and Tracking of Multiple Objects in the parked aside vehicles at night traffic scenario considering a curved trajectory (a,b) and straight trajectory (c,d)}
	\label{fig:5_MOT_quali_carla}
\end{figure}

On the other hand, in terms of our real-world prototype, we focus on implementing a 360º real-time and power-efficient \ac{MOT} pipeline in an efficient way. Perception systems in autonomous driving must process a huge amount of information coming from at least one sensor in order to understand the environment. However, the physical space occupied by the processing units in the vehicle or their power consumption are metrics to be deeply analyzed, even more if these processing units will be integrated in an electric vehicle, where the state of the batteries is crucial. In that sense, the current approach is to use powerful but power-efficient \ac{AI} embedded systems as computation devices for autonomous machines, since they present a remarkable ratio between performance and power consumption in a reduced-size hardware. Regarding the advantage of using neural networks in GPU, these embedded systems present a powerful GPU unit as well as fast storages based on solid state disks and a large RAM memory size. At the time of writing this paper, the best ratio of performance vs power consumption and size is represented by the NVIDIA Jetson embedded computing boards. NVIDIA Jetson is the world's leading AI computing platform for GPU-accelerated parallel processing in mobile embedded systems. These kits allow to implement state-of-the-art frameworks and libraries to conduct accelerated computing, such as CUDA, cuDNN or TensorRT (Tensor RealTime). 

\begin{table}[h]
	\caption{Comparative of inference frequency between the NVIDIA Jetson AGX Xavier and our PC desktop (Intel Core i7-9700, 16GB RAM) with CUDA-based NVIDIA GeForce RTX 1080 Ti 11GB VRAM}
	\label{table:xavier_vs_computer_hz}
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|c|}
			\hline
			Stage & Frequency AGX &  Frequency PC & Ratio \\
			& Xavier (Hz) & desktop (Hz) & \\
			\hline
			Detection & 7.3 & \bf{41.7} & 5.7x \\
			\hline
			Tracking & 15 & \bf{101.9} & 6.7x \\
			\hline
		\end{tabular}
	\end{center}
\end{table}

In this particular work we make use of the NVIDIA Jetson AGX Xavier, which is as far as we know one of the most powerful AI embedded system specially designed for autonomous machines. Table \ref{table:xavier_vs_computer_hz} shows a comparative between the embedded system and our PC frequency in the inference stage, where the detection (PointPillars) is reduced by almost 6 times and the tracking by almost 7 times. Nervertheless, although the detection and tracking frequencies are on the border to be considered real-time according to the requirements of the perception systems for autonomous machines, the embedded system consumes 30 W whilst only the 1080 Ti GPU consumes 250 W at full power respectively. Considering that the embedded system computation power is reduced by 6.2 times (average between the detection and tracking frequency ratios) but only the GPU (not considering the whole PC desktop) presents a power consumption 8.3 higher, makes the current NVIDIA Jetson AGX Xavier a better suitable option for large scale-deployment in the autonomous driving field rather than using desktop graphic cards. Distributing several sensor processing across multiple embedded systems for parallelization will result in lower power consumption than using conventional GPUs in future autonomous driving prototypes. Qualitative results of running our DAMOT pipeline in our own vehicle, equipped with a VLP-16 LiDAR instead of the HDL-64 shown in CARLA and KITTI, are illustrated in Figure \ref{fig:5_MOT_quali_campus}. It can be appreciated that although the obtained results are slightly worse than with the KITTI dataset (equipped with a HDL-64 sensor), we obtain quite promising results, validating the pipeline studied in this work both in terms of accuracy and real-time operation.

\begin{figure}[h]
	\centering
	\begin{subfigure}{0.43\textwidth}
		\captionsetup{justification=centering}
		\includegraphics[width=\textwidth]{5_qualitative_mot_campus/campus_1.PNG}
		\caption{}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.43\textwidth}
		\captionsetup{justification=centering}
		\includegraphics[width=\textwidth]{5_qualitative_mot_campus/campus_2.PNG}
		\caption{}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.43\textwidth}
		\captionsetup{justification=centering}
		\includegraphics[width=\textwidth]{5_qualitative_mot_campus/campus_3.PNG}
		\caption{}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.43\textwidth}
		\captionsetup{justification=centering}
		\includegraphics[width=\textwidth]{5_qualitative_mot_campus/campus_4.PNG}
		\caption{}
	\end{subfigure}
	\caption{Detection and Tracking of Multiple Objects in our campus with our real-world vehicle}
	\label{fig:5_MOT_quali_campus}
\end{figure}

\subsection{EuroNCAP-based validation}
\label{subsec:5_euroncap}

A considerable amount of research works and studies, related to pedestrian detection and collision avoidance behavior are present in the literature, where the main objective is to validate the perception and control modules. Nevertheless, as stated before, we aim to demonstrate how incorporating HD map information helps the whole AD stack to anticipate faster the behaviour of the traffic participants in the corresponding traffic scenarios. Then, common metrics for all frameworks must be used to evaluate the whole architecture, where all modules are integrated. Regarding this, New Car Assessment Programs (NCAPs) protocols are introduced, evaluating the safety of vehicles for different traffic situations and Advanced Driver Assistance Systems, such as Child Occupant Protection (COP), Speed Assist Systems (SAS) or Autonomous Emergency Braking (AEB). Euro-NCAP \cite{article_EuroNCAP_2} is introduced in 1997, representing the widely most adopted performance assessment within the scope of the collaboration of European Union countries. China New Car Assessment Program (C-NCAP) \cite{article_CNCAP} is presented (2006) as a research and development benchmark for vehicle manufacturers in Asia, being most of its protocols based on Euro-NCAP. National Highway Transportation Safety Administration (NHTSA), funded in 1970 as an agency of the Department of Transportation of United States, published \cite{article_NHTSA} its guidance documents and regulations on vehicles equipped with ADAS. As observed, these programmes do not present specific protocols in order to evaluate AD stacks, presenting noticeable differences, such as different scenarios, parameters and evaluation metrics. 

Regarding this, in order to evaluate SmartMOT, we adopt the validation method proposed by \cite{gutierrez2021validation}, which proposes to generalize the \ac{VRU} v.10.0.3 protocol \cite{web_VRU_assessment_protocol}, representing a baseline to compare the performance of different pipelines for the particular situation (both in simulation and real-world) of an Unexpected Vulnerable Road User (VRU) jumping into the road during the navigation, where an Autonomous Emergency Braking (AEB) behaviour must be executed. 

\subsubsection{Implementation details of the EuroNCAP scenario}
\label{subsubsec:5_euroncap_implementation_details}

Figure \ref{fig:5_cpna_scenario} illustrates this traffic situation, in which the VRU (a pedestrian in this particular case) starts in the closest sidewalk to the vehicle in a perpendicular position to the vehicle orientation. Once the vehicle starts the navigation and the L2 distance between the ego-vehicle centroid and the VRU centroid is lower than a certain threshold \textit{d}, the VRU starts its path to unexpectedly cross the road in such a way the ego-vehicle must detect, track and forecast its future trajectory in order to avoid the collision or at least reduce the impact velocity as much as possible. Then, the protocol consists on reproducing the CPNA crash avoidance scenario, with a fixed VRU velocity (\(v_p\)) of 5 km/h and a variable ego-vehicle velocity that ranges from 10 km/h to 60 km/h. It is important to note that the threshold \textit{d} is not fixed, but it is ego-vehicle velocity dependent, that is, the pedestrian must start walking in such a way the impact point (\(P_I\)) (Figure \ref{fig:5_cpna_scenario}) is in the center of the lane for each particular velocity. 

\begin{figure}[h]
	\centering\includegraphics[width=0.4\textwidth]{5_CPNA_scenario.jpg}
	\caption{Car to Pedestrian Nearside Adult (CPNA) scenario}	
	\label{fig:5_cpna_scenario}
\end{figure}

Regarding the evaluation metrics, a score for each test is calculated based on the velocity reduction of the vehicle, as following:

\begin{itemize}
	\item For a vehicle velocity \(v_v\) less than or equal to 40km/h:
	\begin{itemize}
		\item If the vehicle stops without collision, the highest score is achieved:
		\begin{equation}
			score_{test} = score_{max}
			\label{eq1}
		\end{equation}
		
		\item Otherwise, if the vehicle collides, its score is defined as follows:
		\begin{equation}
			score_{test} = \frac{v_{test}-v_{impact}}{v_{test}} \cdot score_{max}
			\label{eq2}
		\end{equation}
	\end{itemize}
	\item For \(v_v\) higher than 40km/h:
	\begin{itemize}
		\item If the vehicle is able to reduce its speed in at least 20 km/h, the highest score is achieve:
		\begin{equation}
			v_{impact} \leq v_{test} - 20 \to score_{test} = score_{max}
			\label{eq3}
		\end{equation}
		\item Otherwise, if the vehicle collides at a velocity greater than the velocity under test less a threshold of 20 km/h, no score is achieve:
		\begin{equation}
			v_{impact} > v_{test} - 20 \to score_{test} = 0
			\label{eq4}
		\end{equation}
	\end{itemize}
\end{itemize}

Finally, the final score of a particular pipeline is given by the arithmetic mean of the results obtained in each CPNA crash avoidance test for different weather conditions. For further details about the validation protocol, we refer the reader to \cite{gutierrez2021validation}.

\subsubsection{Experimental results in the EuroNCAP-based scenario}
\label{subsubsec:5_euroncap_experimental_results}

In this section we obtain some interesting both qualitative and quantitative results, evaluating our AD stack \cite{gomez2021train} in the CPNA crash avoidance scenario using two different perception layer strategies. On the one hand, we implement the perception module stated by \cite{gomez2020real} which tracks all objects in the environment regardless their topological information and considers a naive velocity dependent rectangular monitored area in front of the vehicle to determine the distance to the nearest object in the route as well as to predict the collision. On the other hand, we use SmartMOT to track and predict the future trajectories of only the most relevant obstacles around the vehicle, that is, those in which the human in manual driver should pay attention throughout the route, such as VRUs close to the road, vehicles in intersections and lanes where the lane change maneuver is allowed, etc. Qualitative results may be found in the following play list \href{https://cutt.ly/uk9ziaq}{SmartMOT} \footnote{SmartMOT: https://cutt.ly/uk9ziaq}, where the SmartMOT performance is illustrated. 

Regarding urban environment complexity, in order to validate a whole AD architecture the system must be tested in countless environments and scenarios, which would escalate the cost and development time exponentially with a physical approach. Considering this, the use of photo-realistic simulation (virtual development and validation testing) and an appropriate design of the driving scenarios are the current keys to build safe and robust AV. 

In our work we propose the use of CARLA (CAR Learning to Act) \cite{dosovitskiy2017carla} as the best open-source simulator to reach our goals, taking even more importance when analyzing the behaviours the vehicle can face in these complex traffic scenarios. One of the best advantages of CARLA is the possibility to create ad-hoc urban layouts, useful to validate the navigation architecture in challenging driving scenarios. This code can be downloaded from the ScenarioRunner repository, associated to the CARLA GitHub. The ScenarioRunner is a module that allows the user to define and execute traffic scenarios for the CARLA simulator. In the present case, we define several scenarios according to the CPNA crash avoidance traffic situation, modifying the velocity of the ego-vehicle and the presence of other traffic participants. All test were carried out in a PC desktop (Intel Core i7-9700k, 32GB RAM with CUDA-based NVIDIA GeForce RTX 2080 Ti 11GB VRAM), using the version 0.9.10.1 version of CARLA as well as the corresponding ROS Bridge, responsible of communicating the CARLA environment with our ROS-based architecture, and ScenarioRunner modules. In particular, we make use of the OpenScenario standard, supported by ScenarioRunner, where both the VRU and ego-vehicle features can be modified to accomplish the Euro-NCAP requirements. % Due to size constraint of this paper, we do not validate the performance of our architecture for different weather conditions but only in daytime conditions. 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{5_Unexpected_Pedestrian_Temporal_Graph.jpg}
	\caption[Unexpected Vulnerable Road User (VRU) temporal diagram]{Unexpected Vulnerable Road User (VRU) temporal diagram. At the top, the events produced by our monitors and map manager modules. In the middle, the selector, and start (background) PNs of our decision-making layer. At the bottom, the velocity of the car throughout the navigation}
	\label{fig:5_unexpected_vru_temporal_graph}
\end{figure}

In order to appreciate the behaviour of the vehicle during navigation, we incorporate a very illustrative temporal diagram (Figure \ref{fig:5_unexpected_vru_temporal_graph}), representing a powerful manner to qualitatively validate how the architecture behaves in an end-to-end manner, since we can observe how the car behaves considering the different actions and events \cite{gomez2021train} provided by the executive layer, which is actually the output of the whole architecture before sending commands to the motor. As observed, the ego-vehicle starts far away from the adversary and starts its navigation. At second 22 a pedestrian that is in the sidewalk is detected, so tracking-by-detection and subsequent motion prediction must be carried as fast as possible to avoid collision, since the scenario is designed in such a way that the pedestrian must start walking in such a way the impact point (\(P_I\)) (Figure \ref{fig:5_cpna_scenario}) is in the center of the lane for each particular velocity. After that, our prediction module intersects the ego-vehicle forecasted trajectory and the pedestrian forecasted trajectory. If the Intersection over Union (IoU) is greater than a threshold (in this case, 0.01), a \textit{predictedcollision} flag is activated and the low-level (reactive) control, which always runs in the background of the decision-making layer, performs an emergency break until the car is stopped in front of the obstacle. Navigation is resumed once the obstacle leaves the driving lane. Table \ref{table:CPNA_results} compares the performance of the architecture by implementing \cite{gomez2020real} and a rectangular monitorized lane to retrieve the nearest object in route and predict collision against our proposal, where it can be appreciated that for velocities greater than 40 km/h, using HD map semantic and geometric information gives the car a valuable reaction time to anticipate the VRU behaviour and avoid the collision, achieving the highest score. 

\begin{table}[h]
	\centering
	\caption{Comparison of our two different perception strategies in the Car to Pedestrian Nearside Adult (CPNA) scenario. We bold the best score in \textbf{black}.}
	\label{table:CPNA_results}
	\begin{tabular}{c | c | c  c | c  c  } 
		\hline 
		\multicolumn{6}{c}{\textbf{CPNA}}\\
		\hline \hline
		\multirow{2}{*}{\textbf{\(v_{test}\)}} & \multirow{2}{*}{\textbf{\(score_{max}\)}} & \multicolumn{2}{c}{\textbf{Rectangular area + \cite{gomez2020real}}} & \multicolumn{2}{c}{\textbf{SmartMOT}} \\ 
		& & \(v_{impact}\) & \(score\) & \(v_{impact}\) & \(score\) \\
		\hline
		10 km/h & 1.00 & 0.0 km/h & 1.00 & 0.0 km/h & 1.00 \\
		20 km/h & 1.00 & 0.0 km/h & 1.00 & 0.0 km/h & 1.00 \\
		30 km/h & 2.00 & 0.0 km/h & 2.00 & 0.0 km/h & 2.00 \\
		40 km/h & 3.00 & 0.0 km/h & 3.00 & 0.0 km/h & 3.00 \\
		50 km/h & 2.00 & 23.82 km/h & 2.00 & 0.0 km/h & 2.00 \\
		60 km/h & 1.00 & 44.23 km/h & 0.00 & 0.0 km/h & 1.00  \\
		\hline \hline
		\textbf{Total} & 10.00 &  & 9.00 & & \textbf{10.0} \\
		\hline
	\end{tabular}
\end{table}

Figure \ref{fig:5_cpna_results} shows different analysis of the CPNA crash avoidance scenario with variable ego-vehicle and the incorporation of other traffic participants in the scenario (\ref{fig:5_euroncap_graphics_c} \ref{fig:5_euroncap_graphics_d}). \(T_0\) corresponds with the moment the vehicle either stops or collides, and crosses \textbf{x} represent the moment in which the system sends a predicted collision signal to the executive layer, so it is coherent that crosses in tests where the ego-vehicle collides with the VRU are shifted to the right (prediction collision signal was given in time). Left column tracks all objects around the vehicle and adopts a geometric monitorized area to estimate the nearest distance and predicted collision, whilst right column uses HD map information to help in the Multi-Object Tracking and motion prediction tasks, monitoring only the most relevant traffic participants around the vehicle that is, the main purpose of SmartMOT.

As observed, using HD map information is able to avoid collision until a ego-vehicle velocity of 80 km/h, where SmartMOT is not able to send a signal of predicted collision (output of the system, as shown in Figure \ref{fig:4_IV_2021}) in time, colliding at a velocity of 39.78 km/h. Nevertheless, this velocity at the moment of collision is even lower that the impact velocity (44.23 km/h) when testing the system under 60 km/h condition not using HD map in the MOT stage, illustrating how incorporating additional semantic and geometric map information helps the vehicle to react faster or at least mitigate the effect of collision. Moreover, we simulate both perception strategies using the most common velocities in urban scenarios, which range from 30 to 50 km/h, including \ref{fig:5_euroncap_graphics_c} \ref{fig:5_euroncap_graphics_d} static adversaries (in particular, vehicles and pedestrians) which do not actually in the traffic scenario to fulfill the particular requirements stated by \cite{gutierrez2021validation} protocol. As expected, tracking all objects around the ego-vehicle and using the rectangular monitorized area suffers when the number of traffic participants is increased around the ego-vehicle, whilst SmartMOT holds this exponential increase by analyzing the objects and their corresponding role as relevant obstacles considering the information provided by the HD map, avoiding the collision in all situations.

\begin{figure}[h]
	\centering
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{5_euroncap_without_adversaries_geometric.jpg}
		\caption{Rectangular monitored area without adversaries}
		\label{fig:5_euroncap_graphics_a}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{5_euroncap_without_adversaries_hdmap.jpg}
		\caption{SmartMOT area without adversaries}
		\label{fig:5_euroncap_graphics_b}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{5_euroncap_with_adversaries_geometric.jpg}
		\caption{Rectangular monitored area with adversaries}
		\label{fig:5_euroncap_graphics_c}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{5_euroncap_with_adversaries_hdmap.jpg}
		\caption{SmartMOT area with adversaries}
		\label{fig:5_euroncap_graphics_d}
	\end{subfigure}

	\caption[Analysis of the Car to Pedestrian Nearside Adult (CPNA) crash avoidance scenario with variable ego-vehicle velocity]{Analysis of the Car to Pedestrian Nearside Adult (CPNA) crash avoidance scenario with variable ego-vehicle velocity. Left column (Figures \ref{fig:5_euroncap_graphics_a} and \ref{fig:5_euroncap_graphics_c}) adopts a rectangular monitorized area to estimate the nearest distance and predicted collision, Right column (Figures \ref{fig:5_euroncap_graphics_b} and \ref{fig:5_euroncap_graphics_d}) uses HD map information for this purpose. On the other hand, first row shows the scenario without additional traffic participants, second row analyzes the crash avoidance scenario including additional traffic participants to the road, monitorized sidewalk area and non-relevant sidewalk area. Crosses in the lines represent the moment in which the system sends a predicted collision signal to the executive layer}
	\label{fig:5_cpna_results}
\end{figure}

 

Moreover, as a preliminary analysis (Table \ref{table:5_argoverse_1_error_goals}), we compute the $\mathcal{L}_2$ error between different preprocessing methods to compute the future travelled distance mentioned in previous sections and filter the end point of the raw centerlines. We compare four different centerlines preprocessing methods, where the orientation is implicit given the lane boundaries constraint: \ac{CTRV} without past trajectory filtering (the the kinematic state of the agent is calculated using the raw past trajectory), \ac{CTRV} with Least-Squares ($2^{nd}$ order) and the same substituting \ac{CTRV} for \ac{CTRA}. As expected, the best prior information, considering the mean and median $\mathcal{L}_2$ error over the whole validation split between the target agent ground-truth end-point and $M$ filtered centerlines end-points, is obtained when considering the velocity and acceleration in the kinematic state and filtering the input with the Least-Squares ($2^{nd}$ order). 

\begin{table}[!tpbh]
	\centering
	\caption{\textbf{Error comparison} (in terms of $\mathcal{L}_2$~distance) between the target agent ground-truth end-point and proposed centerlines end-points with different preprocessing methods in the validation split (39,472 samples). CTRV and CTRA stand for \textit{Constant Turn Rate Velocity} and \textit{Constant Turn Rate Acceleration} respectively. We represent both the median and median values for each case.
	}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{l c c}
			\toprule
			Preprocessing method & $\mathcal{L}_2$ Mean~(m) $\downarrow$ & $\mathcal{L}_2$ Median~(m) $\downarrow$ \\
			\midrule
			CTRV without past trajectory filtering & 6.22 & 4.33 \\
			CTRA without past trajectory filtering & 8.78 & 6.31 \\
			CTRV + Least-Squares (2nd order) & 6.14 & 4.21 \\
			CTRA + Least-Squares (2nd order) & \textbf{5.37} & \textbf{3.89} \\
			\bottomrule
	\end{tabular}}
	\label{table:5_argoverse_1_error_goals}
\end{table}

It is important to note that, in the case of the mean (average), while it has traditionally been a popular measure of a mid-point in a sample, it has the disadvantage of being affected by any single value being too high or too low compared to the rest of the sample. This is why a median is sometimes taken as a better measure. Regarding a value of 3.89 m, our centerlines are representative enough as preliminary information assuming the non-holonomic and anisotropic constraints mentioned in Section \ref{subsubsec:4_efficient_baselines_preprocessing_map}. 

\subsubsection{Implementation Details}
\label{subsubsec:5_argoverse_1_implementation_details}

Regarding the methods proposed in the Argoverse 1 dataset (\ac{GAN}-based method, Efficient baselines (social and map) and Augmented Map baseline with Transformer encoder), the implementation details for each method are detailed in this section. All local tests were conducted in a PC desktop (AMD Ryzen 9 5900X, 32GB RAM with CUDA-based NVIDIA GeForce RTX 3090 24GB VRAM, Ubuntu 18.04), training the different models for 150 epochs using Adam optimizer with learning rate $0.001$ and default parameters, linear LR Scheduler with factor $0.5$ decay on plateaus (5k iterations) and batch size $64$.

\paragraph{GAN-based model details}
\label{par:5_gan_based_implementation}

The loss function is weighted by setting $\lambda_{gan}$=1.4, $\lambda_{ade}$=1 and $\lambda_{fde}$=1.5, giving more importance to the adversarial loss and the final displacement error. Similar to \cite{sadeghian2019sophie}, the \ac{LSTM} encoder (attention block) encodes trajectories using a single layer \ac{MLP} with an embedding dimension of 16. We set all \ac{LSTM} units to have $32$ hidden dimensions. The number of target points is set also to 32 in order to compute the physical context. Moreover, in order to calculate these target points we consider the same prediction horizon $t_{pred}=3s$ to estimate the distance travelled assuming a constant velocity model. 

On the other hand, in this model we designed our dataloader to sample in each batch a 30/70 proportion of straight and curved trajectories (regarding the target agent's whole trajectory). We classify a trajectory as straight or curve estimating a first degree trajectory by means the RANSAC algorithm with the highest number of inliers (tolerance $t$ set to 2m, max trials=30, min samples=60\% total observations). Then, if the actual trajectory presents 20\% or more consecutive points further than $t$ with respect to the closest point of the fitted trajectory, the whole sequence is labelled as curve. We do this to focus in the training process in non-linear prediction, which represents one the key challenges in vehicle motion prediction. 

\paragraph{Efficient baselines details}
\label{par:5_efficient_baselines_implementation}

We rotate the whole scene regarding the orientation in the last observation frame of the target agent to align this agent with the positive y-axis. The hidden dimension for the Motion History encoder is 64, where both the hidden state $\mathbf{h_{in}}$ and cell state $\mathbf{c_{in}}$ are initialized with zeros (dim = $128$), whilst the MLP encoder for both the specific centerline and plausible area is 128. Regarding the Social Interaction module, the latent vector of the Crystal-GCN layers is 128 and the number of heads in the MHSA module is $L_h = 4$. In terms of the Autoregressive predictor, the spatial embedding and \textit{dist2centerline} modules encode the past data and distance to the specific centerline using a \textit{window size} of 20. We set the number of plausible centerlines (\textit{M}) as 3, which cover most cases (if less than 3 plausible centerlines are available, we add padded centerlines as vector of zeros). The time tensor is a single number that represents the current timestep, in such a way the LSTM input is \textit{(2 $\times$ window size) + 1 = 41}. The regression head is represented by \textit{k=6} FC layers that map the output latent vector returned by the LSTM to the final output relative displacements (dim = 2, xy). Multimodal predictions are processed by an MLP residual of sizes 60, 60 and 6 with interspersed ReLU activations in order to obtain the corresponding confidences.

In these baselines, we use the standard NLL, Hinge and WTA losses, where $\alpha=1.0$ , $\beta=0.1$ and $\gamma=0.65$ initially, and can be manually adjusted during training (especially $\gamma$) (See Section \ref{subsec:4_efficient_baselines_losses}.

\paragraph{Augmented Efficient baseline with Transformer encoder details}
\label{par:5_augmented_efficient_baseline_implementation}

The implementation details of this model are quite similar to the efficient map baseline in terms of Social Interaction Module (the latent vector of the Crystal-GCN layers is 128 and the number of heads in the MHSA module is $L_h = 4$) and decoding module (the autoregressive \ac{LSTM} prediction is 256 and the regression head is represented by \textit{k=6} FC layers that map the output latent vector returned by the LSTM to the final output relative displacements). The social transformer encoder first smooths the trajectories of the agents and then presents 2 layers of self-attention and normalization mechanisms, whilst the physical transformer encoder employs an MLP encoder in every layer (again 2) in addition to self-attention and normalization, performing feature aggregation along the hidden dimension. We use the Hinge (a.k.a. max-margin) and Winner-Takes-All (WTA) losses between the predicted trajectories and the ground-truth, optimizing for confidences and regressions, where $\beta=0.5$ and $\gamma=1$ initially, and can be manually adjusted during training (especially $\gamma$).

\paragraph{Common data augmentations} 

\begin{itemize}
	\item Dropout and swapping random points from the past trajectory in order to make the trained model general enough so as to perform well on the unseen traffic scenarios in the split test which presents different scene geometries such as left/right turning or emergency braking.
	\item Point location perturbations under a $\mathcal{N}(0, 0.2)$ [m] noise distribution \cite{ye2021tpcn}
	\item We also apply the well-known hard-mining technique to improve the model generalization under difficult scenarios. To perform this technique, once the corresponding model is trained, we perform inference on the training set to find the most difficult scenes in terms of minADE. Then, we mine those scenes such that the baselines models perform poorly, and increase their proportion in the batch during training. This hard-mining technique replaced the 30/70 proportion of straight and curved trajectories proposed in the GAN-based model since in urban scenarios there are many challenging predictions which are straight trajectories but are non-linear (sudden breaks or accelerations), and we realized that enhancing the model with hard-mining instead of curve vs straight trajectory class balance performed better.
\end{itemize}

\subsubsection{Quantiative results}
\label{subsubsec:5_argoverse_1_quantitative}

\paragraph{Ablation studies in validation}

Our main goal is to achieve competitive results while being efficient in terms of model complexity, in particular in terms of \textbf{FLOPs} (Floating-Point Operations per second) and \textbf{parameters} in order to enable these models for real-time operation. For this reason, we have proposed light-weight models, whose main input is the history of past trajectories of the agents, complemented by interpretable map-based features. 

In this section we analyze our results and ablation studies, and prove the benefits of our different methods (GAN-based, Social and Map Efficient baselines, and Augmented Efficient Map baseline) for vehicle \ac{MP} in Argoverse 1. Table \ref{table:results_val_social} and \ref{table:results_val_map} illustrate our ablation study regarding our social and map baselines respectively. 

Even though the GAN-based model was built before our efficient baselines, since it includes map information, we first compare our social efficient baseline (Section \ref{subsec:4_efficient_baselines_social} with other SOTA models \cite{khandelwal2020if, schmidt2022crat, liang2020learning} without map information or with the corresponding module disabled. Our social baseline, trained with NLL loss, presents a number of 351K parameters and 0.87 / 1.63 for minADE and minFDE (k=6) respectively.

We perform the following \textbf{ablation studies} in Table \ref{table:results_val_social}: Reduce social hidden dim (including LSTM, GNN and MHSA modules) from 128 to 64, replace the standard head with residual head, replace only last data (standard autoregressive decoder input) with last $N$ data (temporal decoder). We obtain better results with hidden dim = 64, decreasing the number of parameters. Linear residual, standard in most MP models, presents worse results with a much higher number of parameters, since most works use it in a non-autoregressive way, decoding directly from the latent space. On the other hand, using temporal decoder instead of only the last position as LSTM input achieves better results with a slightly higher number of parameters. Then, we conclude our Best Social Model (BSM), as a preliminary stage before implementing the map features, presents the following modifications: social hidden dim = 64 and temporal decoder. Hard-mining and additional losses (Hinge and WTA) applied to the best social model achieve the best social results (Social Baseline).

\begin{table}[!tpbh]
	\caption[Ablation Study for map-free MP on the Argoverse 1 validation set]{\textbf{Ablation Study for map-free MP on the Argoverse 1 \cite{chang2019argoverse} validation set}. Our methods are indicated with $\dag$, our highlighted method indicates our map-free baseline (Best Social Model = BSM). Prediction metrics (minADE, minFDE) are reported in meters.}
	\label{table:results_val_social}
	\setlength{\tabcolsep}{5pt}
	\centering
	\resizebox{\linewidth}{!}{
		\begin{tabular}{l c c c c c}
			\toprule
			\multirow{2}{*}{Method} & \multirow{2}{*}{Number of Parameters} &
			\multicolumn{2}{c}{$k=1$} & \multicolumn{2}{c}{$k=6$} \\ 
			& & minADE & minFDE & minADE & minFDE  \\ 
			\midrule
			TPCN \cite{ye2021tpcn} & - & 1.42 & 3.08 & 0.82 & 1.32 \\
			LaneGCN  \cite{liang2020learning} (w/o map) & $\approx 1 M $ & 1.58 & 3.61 & 0.79 & \textbf{1.29} \\
			WIMP \cite{khandelwal2020if} (w/o map) & $> 20 M$ & $1.61$ & 5.05 & 0.86 & 1.39 \\
			CRAT-Pred (LSTM + GNN + Lin. Residual) \cite{schmidt2022crat} & 449K & 1.44 & 3.17 & 0.86 & 1.47 \\
			CRAT-Pred (LSTM + GNN + Multi-Head Self-Attention + Lin. Residual) \cite{schmidt2022crat} & 515K & \textbf{1.41} & \textbf{3.10} & 0.85 & 1.44 \\ 
			\midrule
			$\dag$ LSTM-128 + GNN + MHSA (Baseline social) &  351K  & 1.82 & 3.72 & 0.87 & 1.63 \\
			$\dag$ LSTM-64 + GNN + MHSA &  \textbf{97K}  & 1.77 & 3.68 & 0.86 & 1.61 \\
			$\dag$ LSTM-128 + GNN + MHSA + Lin. Residual &  552K  & 2.02 & 4.16 & 1.02 & 1.95 \\
			$\dag$ LSTM-128 (TDec) + GNN + MHSA  &  365K  & 1.81 & 4.04 & 0.83 & 1.57 \\
			$\dag$ LSTM-64 (TDec) + GNN + MHSA \quad (Best Social Model)  &  105K  & 1.79 & 4.01 & 0.81 & 1.56 \\
			\midrule
			$\dag$ Best Social Model + HardM (10 \%) &  105K  & 1.76 & 3.97 & 0.80 & 1.53 \\
			\rowcolor{gray}$\dag$ Best Social Model + HardM (10 \%) \emph{w/} Loss Hinge + WTA &  105K  & 1.62 & 3.57 & \textbf{0.76} & 1.43 \\
			\bottomrule
	\end{tabular}}
\end{table}

In terms of map information, we conduct different ablation studies (Table \ref{table:results_val_map}) considering the three proposed algorithms. 

Regarding the GAN-based model, we study the influence of incorporating target points and class balance to our baseline. As expected, by explicitly defining the locations an agent is likely to be at a fixed prediction horizon for a given input trajectory and scene geometry, we are able to improve our baseline. Additionally, since nonlinear trajectories are more challenging than standard straight trajectories, we also observe how enforcing the class balance (straight, curve) during training is able to improve performance. Note that this model focuses on unimodal predictions instead of covering different future plausible paths.

Then, we continue integrate the map features starting from the BSM without hard-mining and with the initial loss (NLL), in order to check how implementing these additional regularization terms help the model to generalize better in both experiments (only social and social+map). We perform the following ablations: compute the most plausible centerline (\textit{M}=1) returned by the \textbf{Argoverse API}, consider \textit{M}=3 centerlines, replace MLP encoder with 1D-CNN encoder in a similar way \cite{mercat2020multiattentmotion}, explicitly iterate over all centerlines as \textbf{specific} deep physical context instead of decoding from a common latent space, add low-level features (feasible area) as a common \textbf{static} deep physical context for each iteration and finally adding an additional component to the LSTM input determined by the vector distance between the considered input window (last $N$ data) and the corresponding centerline. It can be observed that introducing map features increases the number of parameters in exchange of a noticeable metrics decrement, specially in terms of minFDE ($k$ = 6). Considering \textit{M}=3 centerlines instead of only the most plausible centerline allows the model to compute a more diverse set of predictions, while replacing a standard MLP encoder with 1D-CNN encoder increases the number of parameters achieving worse metrics, according to this experimental setup. 

\begin{table}[!tpbh]
	\caption[Ablation Study for map-based motion forecasting on the Argoverse validation set]{\textbf{Ablation Study for map-based motion forecasting on the Argoverse \cite{chang2019argoverse} validation set}. Our methods are indicated with $\dag$. We highlight our map-based baseline method, as a reference for future comparisons. Hyphens "-" indicate that attributes are either not applicable, or not available. \textit{M} indicates the number of centerlines, \textit{FA} stands for Feasible Area (Low-level features) and \textit{D2C} stands for Distance2Centerline module (see Figure \ref{fig:4_TITS_2023})}
	\label{table:results_val_map}
	\setlength{\tabcolsep}{5pt}
	\centering
	\resizebox{\linewidth}{!}{
		\begin{tabular}{l c c c c c}
			\toprule
			\multirow{2}{*}{Method} & \multirow{2}{*}{Number of Parameters} &
			\multicolumn{2}{c}{$k=1$} & \multicolumn{2}{c}{$k=6$} \\ 
			&               & minADE & minFDE & minADE & minFDE  \\ 
			\midrule
			GAN-based model & 278K & 1.91 & 4.43 & - & - \\
			GAN-based model + Class Balance & 342K & 1.73 & 4.08 & - & - \\
			GAN-based model + Target Points & 278K & 1.78 & 4.02 & - & - \\
			GAN-based model + Class Balance + Target Points & 342K & 1.64 & 3.76 & - & - \\
			\midrule
			\textbf{$\dag$ Our Map-free Efficient Baseline (BSM, No Hard-mining, Loss = NLL)} &  105K  & 1.79 & 4.01 & 0.81 & 1.56 \\
			LaneGCN  \cite{liang2020learning} & 3.7M & \textbf{1.35} & \textbf{2.97} & \textbf{0.71} & \textbf{1.08} \\
			WIMP \cite{khandelwal2020if} (w/o map, NLL loss) & $> 25 M$ & 1.41 & 6.38 & 1.07 & 1.61 \\
			WIMP \cite{khandelwal2020if} (w/o map, EWTA loss) & $> 25 M$ & 1.45 & 3.19 & 0.75 & 1.14 \\
			\midrule
			$\dag$ BSM + Oracle &  \textbf{277K}  & 1.62 & 3.56 & 0.77 & 1.42 \\
			$\dag$ BSM + \textit{M}=3 &  307K  & 1.60 & 3.53 & 0.76 & 1.39 \\
			$\dag$ BSM + \textit{M}=3 (1D-CNN) &  432K  & 1.63 & 3.59 & 0.78 & 1.43 \\
			$\dag$ BSM + \textit{M}=3 loop &  326K  & 1.62 & 3.41 & 0.76 & 1.40 \\
			$\dag$ BSM + \textit{M}=3 loop + \textit{FA} &  458K  & 1.62 & 3.40 & 0.76 & 1.40 \\
			$\dag$ BSM + \textit{M}=3 loop + \textit{FA} + \textit{D2C} (Best Global Model) &  459K  & 1.61 & 3.40 & 0.75 & 1.39 \\
			\midrule
			$\dag$ Best Global model + HardM (10 \%)  & 459K  & 1.55 & 3.31 & 0.75 & 1.36 \\
			$\dag$ Best Global model + HardM (10 \%) \emph{w/} Loss Hinge + WTA &  459K  & 1.46 & 3.22 & 0.72 & 1.28 \\
			\rowcolor{gray}$\dag$ Best Global model + HardM (10 \%) \emph{w/} Loss Hinge + WTA + Transformer Encoders &  1.235M  & 1.38 & 3.1 & 0.68 & 1.16 \\
			\bottomrule
	\end{tabular}}
\end{table}

We include our low-level (static) features as a static deep physical context which is common to all iterations over the different centerlines and an additional vector distance to the corresponding centerline, achieving our best results without additional regularization terms (hard-mining and Hinge / WTA losses). Finally, as stated in Section \ref{subsec:4_augmented_baseline} we include transformer encoders to enhance the accuracy of the final model. 

As conclusion, our methods (GAN-based, efficient social and map baseline, and augmented map baseline) obtain regression metrics (minADE and minFDE with both $k$ = 1 and 6) up-to-pair with other \ac{SOTA} models with a noticeable lower number of parameters, specially in the ablation study for map-based \ac{MP} models, demonstrating how focusing on the most important map-features drastically decreases the network complexity obtaining similar results in terms of accuracy. As expected, incorporating high-level and well-structured physical information (centerlines), instead of only discrete points (proposed in the GAN-based method), not only represents potential goal points \cite{dendorfer2020goalgan} (\ie potential destinations or end-of-trajectory points for the agents), but also gathers information about the feasible area around the agent. This information is \textit{"cheap"} and \textit{interpretable}, therefore, we do not need further exhaustive annotations from the HD Map in comparison with other methods like HOME, which gets as input a 45-channel encoded map \cite{gilles2021home}. Furthermore, incorporating a multimodal head with the corresponding training losses noticeable reduces the error metrics, specially in the $k=6$ scenario.

\paragraph{Comparison with the State-of-the-Art}

The Argoverse Benchmark \cite{chang2019argoverse} has over 290 submitted methods, however, the top approaches achieve, in our opinion, essentially the same performance. In order to do a fair comparison, we analyze the \textit{state-of-the-art} performance in this benchmark, we show the results in Table~\ref{table:5_argoverse_1_r}. Given the standard deviations (in meters) of the most important regression metrics (minADE and minFDE, both in the unimodal and multimodal case), we conclude that there are no significant performance differences for the top-25 models. In fact, Argoverse 2 \cite{wilson2023argoverse}. explicitly mentions that there is a \textit{"goldilocks zone"} of task difficulty in the Argoverse 1 test set, since it has begun to plateau. 

We conclude there is no significant performance difference for the top-25 models. We achieve competitive results using efficient models and minimal HD map information (both static context and specific plausible centerlines) leads to a better performance. 

We define $\mu$ADE$_k$ and $\mu$FDE$_k$ as the average ADE and FDE of the top-$k$ ranked methods, in the same way we define $\sigma$ADE$_k$ and $\sigma$FDE$_k$ for the standard deviation of the results. 
Table~\ref{tab:5_argoverse_1_results_test} shows these statistics. Given the small $\sigma$ADE$_k$ and $\sigma$FDE$_k$ values (in meters), we can conclude that there is no significant performance difference for the top-25 models, however, we cannot get enough information from the benchmark to test this hypothesis properly.

\begin{comment}
\begin{table}[h]
	\centering
	\caption{\textbf{Results on the Argoverse 1.0 Benchmark~\cite{chang2019argoverse}}. We borrow some numbers from~\cite{chang2019argoverse, gilles2021home, gilles2022gohome}. We specify the map info for each model: Raster, GNN or polyline, as stated in Table \ref{table:2_dl_related_work_mp}. We indicate the error \textcolor{blue}{difference} of our method \emph{w.r.t.} top-25 SOTA methods, in centimeters. Our predictions differ \emph{w.r.t.} top-25 SOTA only \textcolor{blue}{10cm} and \textcolor{blue}{15cm} for the unimodal and multimodal minADE metric respectively, yet our model is much more efficient.}
	%\begin{tabular}{lc>{\columncolor[gray]{0.9}}c>{\columncolor[gray]{0.9}}c c c}
	%\begin{tabular}{\textwidth}{lccccc}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{l c c c c c}
			\toprule
			%\rowcolor[gray]{0.9} Model & Map info & \multicolumn{2}{c}{K=1} & \multicolumn{2}{c}{K=6}\\
			Model & Map info & \multicolumn{2}{c}{K=1} & \multicolumn{2}{c}{K=6}\\
			& & minADE $\downarrow$ & minFDE $\downarrow$ & minADE $\downarrow$ & minFDE $\downarrow$ \\
			\midrule
			Constant Velocity~\cite{chang2019argoverse} & - & 3.53 & 7.89 &  &  \\ 
			Argoverse Baseline (NN)~\cite{chang2019argoverse} & - & 3.45 & 7.88 & 1.71 & 3.29 \\
			Argoverse Baseline (LSTM)~\cite{chang2019argoverse} & Polyline & 2.96 & 6.81 & 2.34 & 5.44  \\
			Argoverse Baseline (NN)~\cite{chang2019argoverse} & Polyline & 3.45 & 7.88 & 1.71 & 3.29  \\
			\midrule
			SGAN~\cite{gupta2018social} & Map + Traj. & 3.61 & 5.39 &  &  \\
			%TPNet~\cite{fang2020tpnet} & Map + Traj. & 2.33 & 5.29 &  &   \\
			%TPNet-map~\cite{fang2020tpnet} & Map + Traj. & 2.23 & 4.71 &  &   \\
			%TPNet-map-safe~\cite{fang2020tpnet} & Map + Traj. & 2.23 & 4.70 &  &  \\
			TPNet-map-mm~\cite{fang2020tpnet} & Raster & 2.23 & 4.70 & 1.61 & 3.70 \\
			% Alibaba-ADLab & Map + Traj. & 1.97 & 4.35 & 0.92 & 1.48 \\ 
			% HIKVISION-ADLab-HZ  & Map + Traj. & 1.94 & 3.90 & 1.21 & 1.83 \\
			Challenge Winner: uulm-mrm (2nd)~\cite{chang2019argoverse} & Polyline & 1.90 & 4.19 & 0.94 & 1.55 \\
			Challenge Winner: Jean (1st)~\cite{mercat2020multi, chang2019argoverse} & Polyline & 1.74 & 4.24 & 0.98 & 1.42 \\
			TNT~\cite{zhao2021tnt} & GNN & 1.77 & 3.91 & 0.94 & 1.54 \\
			mmTransformer~\cite{liu2021multimodal} & Polyline & 1.77 & 4.00 & 0.84 &  1.33 \\
			HOME~\cite{gilles2021home} & Raster & 1.72 & 3.73 & 0.92 & 1.36 \\
			LaneConv~\cite{deo2018convolutionalmotion} & Raster & 1.71 & 3.78 & 0.87 & 1.36 \\
			UberATG~\cite{liang2020learning} & GNN & 1.70 & 3.77 & 0.87 & 1.36 \\
			LaneRCNN~\cite{zeng2021lanercnn} & GNN & 1.70 & 3.70 & 0.90 & 1.45 \\
			GOHOME~\cite{gilles2022gohome} & GNN & 1.69 & 3.65 & 0.94 & 1.45 \\
			% TPCN~\cite{ye2021tpcn} & Map + Traj. & 1.58 & 3.49 & 0.82 & 1.24 \\
			\textbf{State-of-the-art (top-10)}~\cite{gilles2022gohome, liu2021multimodal, varadarajan2022multipath++, ye2021tpcn} &  & \textbf{1.57}$\pm$0.06 &  \textbf{3.44}$\pm$0.15 & \textbf{0.79}$\pm$0.02 & \textbf{1.17}$\pm$0.04  \\
			\textbf{State-of-the-art (top-25)}~\cite{gilles2022gohome, liu2021multimodal, varadarajan2022multipath++, ye2021tpcn} &  & \textbf{1.63}$\pm$0.08 & \textbf{3.59}$\pm$0.20 & \textbf{0.81}$\pm$0.03 & \textbf{1.22}$\pm$0.06  \\
			% mean and variance
			\midrule
			Ours (Social baseline, including HardM and losses) & - & 2.57 & 4.36 & 1.26 & 2.67 \\
			\rowcolor{gray} Ours (Map baseline, including HardM and losses) & Polyline & 
			1.73~{\scriptsize{\textcolor{blue}{(10cm)}}} & 3.89~{\scriptsize{\textcolor{blue}{(30cm)}}} & 0.96~{\scriptsize{\textcolor{blue}{(15cm)}}} & 1.63~{\scriptsize{\textcolor{blue}{(41cm)}}} \\
			\bottomrule
	\end{tabular}}
	\label{table:results_test}
\end{table}
\end{comment}

\begin{table}
	\centering
	\caption{Results on the Argoverse Benchmark~\cite{chang2019argoverse}. We borrow some numbers from~\cite{chang2019argoverse, gilles2021home, gilles2021gohome}. We specify the map info for each model: Raster, GNN (Graph) or polyline. Prediction metrics (minADE, minFDE) are reported in meters. We \textbf{bold} the best metrics of each column. Our methods are indicated with $\dag$.}
	%\begin{tabular}{lc>{\columncolor[gray]{0.9}}c>{\columncolor[gray]{0.9}}c c c}
	\resizebox{\linewidth}{!}{
	\begin{tabular}{l c c c c c}
		\toprule
		%\rowcolor[gray]{0.9} Model & Map info & \multicolumn{2}{c}{K=1} & \multicolumn{2}{c}{K=6}\\
		Model & Map info & \multicolumn{2}{c}{K=1} & \multicolumn{2}{c}{K=6}\\
		& & minADE $\downarrow$ & minFDE $\downarrow$ & minADE $\downarrow$ & minFDE $\downarrow$ \\
		\midrule
		Constant Velocity~\cite{chang2019argoverse} & - & 3.53 & 7.89 &  &  \\ 
		Argoverse Baseline (NN)~\cite{chang2019argoverse} & - & 3.45 & 7.88 & 1.71 & 3.29 \\
		Argoverse Baseline (LSTM)~\cite{chang2019argoverse} & Polyline & 2.96 & 6.81 & 2.34 & 5.44  \\
		Argoverse Baseline (NN)~\cite{chang2019argoverse} & Polyline & 3.45 & 7.88 & 1.71 & 3.29  \\
		\midrule
		% SGAN~\cite{gupta2018sgan} & Map + Traj. & 3.61 & 5.39 &  &  \\
		%TPNet~\cite{fang2020tpnet} & Map + Traj. & 2.33 & 5.29 &  &   \\
		%TPNet-map~\cite{fang2020tpnet} & Map + Traj. & 2.23 & 4.71 &  &   \\
		%TPNet-map-safe~\cite{fang2020tpnet} & Map + Traj. & 2.23 & 4.70 &  &  \\
		TPNet-map-mm~\cite{fang2020tpnet} & Raster & 2.23 & 4.70 & 1.61 & 3.70 \\
		% Alibaba-ADLab & Map + Traj. & 1.97 & 4.35 & 0.92 & 1.48 \\ 
		% HIKVISION-ADLab-HZ  & Map + Traj. & 1.94 & 3.90 & 1.21 & 1.83 \\
		Challenge Winner: uulm-mrm (2nd)~\cite{chang2019argoverse} & Polyline & 1.90 & 4.19 & 0.94 & 1.55 \\
		Challenge Winner: Jean (1st)~\cite{mercat2020multiattentmotion, chang2019argoverse} & Polyline & 1.74 & 4.24 & 0.98 & 1.42 \\
		TNT~\cite{zhao2020tnt} & GNN & 1.77 & 3.91 & 0.94 & 1.54 \\
		mmTransformer~\cite{liu2021multimodal} & Polyline & 1.77 & 4.00 & 0.84 &  1.33 \\
		HOME~\cite{gilles2021home} & Raster & 1.72 & 3.73 & 0.92 & \textbf{1.36} \\
		LaneGCN~\cite{liang2020learninggraph} & GNN & 1.70 & 3.77 & \textbf{0.87} & \textbf{1.36} \\
		LaneRCNN~\cite{zeng2021lanercnn} & GNN & 1.70 & 3.70 & 0.90 & 1.45 \\
		GOHOME~\cite{gilles2021gohome} & GNN & \textbf{1.69} & \textbf{3.65} & 0.94 & 1.45 \\
		% TPCN~\cite{ye2021tpcn} & Map + Traj. & 1.58 & 3.49 & 0.82 & 1.24 \\
		\midrule
		$\dag$ GAN-based model \cite{gomez2022exploring} & Polyline & 1.67 & 3.82 & - & - \\
		$\dag$ MAPFE4MP (Social baseline) \cite{gomez2022exploringmapbased} & - & 2.57 & 4.36 & 1.26 & 2.67 \\
		$\dag$ MAPFE4MP (Map baseline) \cite{gomez2022exploringmapbased} & Polyline & 1.73 & 3.89 & 0.96 & 1.63 \\
		$\dag$ Augmented MAPFE4MP (Map baseline) & Polyline & 1.72 & 3.75 & 0.91 & 1.52 \\
		\bottomrule
	\end{tabular}}
	\label{table:5_argoverse_1_results_test}
\end{table}

\paragraph{Efficiency discussion}

In terms of \textbf{efficiency discussion}, to the best of our knowledge, very few methods reports efficiency-related information \cite{gilles2021gohome, gilles2021home, liu2021multimodal, gao2020vectornet}. Furthermore, comparing runtimes is difficult, as only a few competitive methods provide code and models. The Argoverse Benchmark~\cite{chang2019argoverse} provides insightful metrics about the model's performance, mainly related with the predictions error. However, there are no metrics about efficiency (i.e. model complexity in terms of parameters or FLOPs). In the AD context, we consider these metrics as important as the error evaluation because, in order to design a reliable AD stack, we must produce reliable predictions on time, meaning the inference time (related to model's complexity and inputs) is crucial. SOTA methods already provide predictions with an error lesser than 1 meter in the multi-modal case. In our opinion, an accident will rarely happen because some obstacle predictions are offset by one or half a meter, this uncertainty in prediction can be acceptable in the design of AV, but rather because lack of coverage or delayed response time. Despite its high accuracy and fast inference time, LaneGCN \cite{liang2020learning} makes use of multiple GNN layers that can lead to issues with over-smoothing for map-encoders \cite{li2018deeper}. Moreover, as mentioned in \cite{gao2020vectornet}, CNN-based models for processing the HD map information are able to capture social and map interactions, but most of them are computationally too expensive. LaneRCNN \cite{zeng2021lanercnn} adds huge number of hyperparameters to the model, making it quite complex since it proposes to capture agent and map interactions with a local interaction graph per agent, not just a single vector. 

Similar to image classification, where model efficiency depends on its accuracy and parameters/FLOPs, we use the same criteria to compare models. We show the \textbf{efficiency comparison} with other relevant methods in Table \ref{table:effcomp}. We calculate FLOPs and parameters using a third-party library \footnote{https://github.com/facebookresearch/fvcore}. Some minor operations were not supported, yet, their contributions to the number of FLOPs were residual and ignored. 
The results for the other methods are consulted from \cite{gilles2021home} \cite{gilles2021gohome} \cite{gao2020vectornet} \cite{he2022multi}. We calculate FLOPs using the relation: $\text{GMACs} \approx 0.5 * \text{GFLOPs}$ using \url{https://github.com/facebookresearch/fvcore}.

In order to calculate the FLOPs, we follow the common practice \cite{gao2020vectornet} \cite{gu2021densetntwaymo} \cite{gilles2021gohome} of fixing the number of lanes \emph{i.e.,} the number of centerlines is limited to 3. %, since we focus on the plausible area around the target agent.
% 40 and 10 the number of lanes (if the method is based on polyline or GNN, not raster) and agents per traffic scenario, respectively.
%
\cite{gao2020vectornet} compares its GNN backbone with CNNs of different kernel sizes and map resolution to compute deep map features (decoder operations and parameters are excluded, min), demonstrating how CNN based methods noticeably increase the amount of parameters and operations per second. We do not require CNNs to extract features from the HD map since we use our map-based feature extractor to obtain the feasible area (see Section \ref{subsec:map_baseline}), assuming anisotropic driving (the most important features are ahead) and non-holonomic constraints, in such a way these features are interpretable in comparison with CNNs high-dimensional outputs. Note that, in both variants (social and map baselines), the self-attention module is used with a dynamic number of input agents, this typically implies a quadratic growth in complexity with the number of agents in the scene~\cite{vaswani2017attention}, yet, this only applies to the MHSA layers.

Even though our method do not obtain the best regression metrics, we achieve comparable results (Table~\ref{table:effcomp}) against other SOTA approaches whilst our number of FLOPs is several orders of magnitude smaller than other approaches \cite{gu2021densetntwaymo} \cite{liang2020learning}, obtaining a good trade-off between model complexity and error (minADE, k=6).

Moreover, as it is well known in machine learning, the number of parameters is not always proportional to the inference speed. In that sense, our transformer approach also has certain benefits in comparison to LSTM/RNN temporal encoding, since these are non-parallelizable, therefore, despite having more parameters, transformers are faster~\cite{vaswani2017attention}.

\begin{table}[h]
	\centering
	\caption{Efficiency comparison among SOTA methods. We show the number of parameters for each model, FLOPs, minADE (k=6) in the Argoverse test set, and runtime. Works from \cite{gao2020vectornet} focus on unimodal predictions (k=1). \textit{N/A} stands for \textit{Not Available}. Time measured on a RTX 2080 Ti (using batch 32). Some numbers are borrowed from \cite{zhou2022hivt, bhattacharyya2022ssllanes}.
	}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{lcccc}
			\toprule
			Model & \# Par.~(M) & FLOPs (G)~$\downarrow$ & minADE~(m) $\downarrow$ & Run~(ms) $\downarrow$ \\
			\midrule
			CtsConv~\cite{walters2020trajectory} & 1.08 & 0.34 & 1.85 & 684  \\
			%$\rho_1$-ECCO~\cite{walters2020trajectory} & 0.051 & 1.7 & 3.89 \\
			%mmTransformer~\cite{liu2021mmtransf} & 4 & 1.77 & & 0.66 \\
			R18-k3-c1-r100~\cite{gao2020vectornet} & 0.25 & 0.66 & 2.21 & \textit{N/A} \\
			R18-k3-c1-r400~\cite{gao2020vectornet} & 0.25 & 10.56 & 2.16 & \textit{N/A} \\
			VectorNet~\cite{gao2020vectornet} & \textbf{0.072} & 0.41 & 1.66 & 1103 \\
			DenseTNT (w/ 100ms opt.) \cite{gu2021densetntwaymo} & 1.1 & 0.763 & 0.88 & 2644 \\
			DenseTNT (w/ goal set pred.) \cite{gu2021densetntwaymo} & 1.1 & 0.763 & 0.85 & 531 \\
			LaneGCN \cite{liang2020learning} & 3.7 & 1.071 & 0.87 & 173 \\
			mmTransformer \cite{liu2021multimodal} & 2.607 & 0.177 & 0.84 & \textit{N/A} \\
			MF-Transformer \cite{he2022multi} & 2.469 & 0.408 & \textbf{0.82} & \textit{N/A} \\
			HOME+GOHOME~\cite{gilles2021gohome} & 0.40 & 0.09 & 0.94 & 32 \\
			\midrule
			GAN-based 
			MAPFE4MP (Social baseline) \cite{gomez2022exploringmapbased} & 0.105 & \textbf{0.007} & 1.26 & 7 \\
			MAPFE4MP (Map baseline) \cite{gomez2022exploringmapbased} & 0.621 & 0.047 & 0.96 & 31 \\
			
			Ours & 1.235 & \textbf{0.038} & 0.91 & \textbf{16} \\
			\bottomrule
	\end{tabular}}
	\label{table:5_argoverse_1_efficiency_comparison}
\end{table}








We illustrate the qualitative results of our best model.

\begin{figure}[h]
	\centering
	\setlength{\tabcolsep}{2.0pt}
	%\renewcommand{\arraystretch}{1.2}%
	\begin{tabular}{cccc}
		\fbox{\includegraphics[width=0.23\linewidth]{5_qualitative_results_argoverse_1/val-120-general-view.png}} & 
		\fbox{\includegraphics[width=0.23\linewidth]{5_qualitative_results_argoverse_1/val-120-plausible_hdmap.png}} &
		\fbox{\includegraphics[width=0.23\linewidth]{5_qualitative_results_argoverse_1/val-120-unimodal.png}} & 
		\fbox{\includegraphics[width=0.23\linewidth]{5_qualitative_results_argoverse_1/val-120-multimodal_k_6.png}}
		
		\tabularnewline
		
		\fbox{\includegraphics[width=0.23\linewidth]{5_qualitative_results_argoverse_1/val-215-general-view.png}} & 
		\fbox{\includegraphics[width=0.23\linewidth]{5_qualitative_results_argoverse_1/val-215-plausible_hdmap.png}} &
		\fbox{\includegraphics[width=0.23\linewidth]{5_qualitative_results_argoverse_1/val-215-unimodal.png}} & 
		\fbox{\includegraphics[width=0.23\linewidth]{5_qualitative_results_argoverse_1/val-215-multimodal_k_6.png}}
		
		\tabularnewline
		
		\fbox{\includegraphics[width=0.23\linewidth]{5_qualitative_results_argoverse_1/val-205-general-view.png}} & 
		\fbox{\includegraphics[width=0.23\linewidth]{5_qualitative_results_argoverse_1/val-205-plausible_hdmap.png}} &
		\fbox{\includegraphics[width=0.23\linewidth]{5_qualitative_results_argoverse_1/val-205-unimodal.png}} & 
		\fbox{\includegraphics[width=0.23\linewidth]{5_qualitative_results_argoverse_1/val-205-multimodal_k_6.png}}
		
		\tabularnewline
		
		\fbox{\includegraphics[width=0.23\linewidth]{5_qualitative_results_argoverse_1/val-152-general-view.png}} & 
		\fbox{\includegraphics[width=0.23\linewidth]{5_qualitative_results_argoverse_1/val-152-plausible_hdmap.png}} &
		\fbox{\includegraphics[width=0.23\linewidth]{5_qualitative_results_argoverse_1/val-152-unimodal.png}} & 
		\fbox{\includegraphics[width=0.23\linewidth]{5_qualitative_results_argoverse_1/val-152-multimodal_k_6.png}}
		
		\tabularnewline
		
		\fbox{\includegraphics[width=0.23\linewidth]{5_qualitative_results_argoverse_1/val-209-general-view.png}} & 
		\fbox{\includegraphics[width=0.23\linewidth]{5_qualitative_results_argoverse_1/val-209-plausible_hdmap.png}} &
		\fbox{\includegraphics[width=0.23\linewidth]{5_qualitative_results_argoverse_1/val-209-unimodal.png}} & 
		\fbox{\includegraphics[width=0.23\linewidth]{5_qualitative_results_argoverse_1/val-209-multimodal_k_6.png}}
		
		\tabularnewline
		
		General view & Plausible HDMap & Unimodal (k=1) prediction & Multimodal (k=6) predictions \tabularnewline
	\end{tabular}
	\caption[Qualitative Results on challenging scenarios in the Argoverse 1 validation set using our best model]{Qualitative Results on challenging scenarios in the Argoverse 1 validation set using our best model.	We represent: our vehicle (\textbf{\textcolor{blue}{ego}}), the \textbf{\textcolor{YellowOrange}{target agent}}, and \textbf{\textcolor{gray75}{other agents}}. We can also see the \textbf{\textcolor{red}{ground-truth}} trajectory of the target agent, our \textbf{\textcolor{ForestGreen}{multimodal predictions}} (with the corresponding confidences) and \textbf{plausible centerlines}. Circles represent last observations and diamonds last future positions. As we can see the plausible HDMap serves as a good guidance to our model, which can predict reasonable trajectories in presence of multiple agents and challenging scenarios. We show, from left to right, a general view of the traffic scenario (including social and map information), our calculated plausible HDMap, unimodal prediction (best mode in terms of confidence) and multimodal prediction (\textit{k} = 6), including confidences (the higher, the most probable)}
	\label{fig:results}
\end{figure}



\subsubsection{Implementation details}

We use the Argoverse 2~\cite{wilson2023argoverse} dataset and benchmark described in Section~\ref{sec:rel_work}. %The Argoverse 2 dataset is a high-quality multi-agent motion prediction dataset. 
For each real driving scenario we have the corresponding local HD map, past trajectories of the agents, metadata about the agents (\eg the ype of agent: cyclist, pedestrian, car), and topological information about the scene. Each scenario is 11 seconds long. We consider five seconds of the past trajectory (also known as motion history), and we predict the next six seconds. 

%\noindent\textbf{Metrics.}~~~\revise{We follow the widely used evaluation metrics~\cite{zeng2021lanercnn,gu2021densetntwaymo,ye2021tpcn}.} %We follow the benchmark settings and adopt widely used metrics.
%Specifically, MR is the ratio of predictions where none of the predicted $K$ trajectories is within 2.0 meters of ground truth according to the endpoint's displacement error. 
%Minimum Final Displacement Error (minFDE) is the L2 distance between the endpoint of the best-forecasted trajectory and the ground truth. Minimum Average Displacement Error (minADE) is the average L2 distance between the best-forecasted trajectory and the ground truth. %Argoverse Motion Forecasting leaderboard is ranked by Brier minimum Final Displacement Error (brier-minFDE6), which adds a probability-related penalty to the endpoint's L2 distance error. 

\noindent\textbf{Implementation.}~~~ We train our model on 2 A100 GPUs using a batch size of 128 with the Adam optimizer for 42 epochs. The initial learning rate is 1 x 10-3, decaying to 1 x 10-4 at 32 epochs. The latent dimension (regarding map and social features) in most of our experiments is 128. The number of attention heads in the social encoder and motion refinement is 8. The training setup including loss functions follows GANet~\cite{wang2022ganet} official implementation as our baseline.

\noindent\textbf{Augmentations}  (i) Dropout and swapping random points from the past trajectory, (ii) point location perturbations under a $\mathcal{N}(0, 0.2)$ [m] noise distribution~\cite{ye2021tpcn}.

\subsubsection{Quantitative and Qualitative results}

Tables \ref{table:table_1} and \ref{table:table_2} present the results obtained on the Argoverse 2 Motion Forecasting validation and test sets, respectively. We achieve near state-of-the-art performance in both sets, which is on-pair with the most promising pipelines, while using notably less parameters. As stated throughout this work, we focus on applying efficient methods to help understand future interactions among the different agents, reducing the number of parameters and inference time. 

We can appreciate in Table \ref{table:table_1} the huge influence of the physical context both in terms of accuracy and runtime. GANet \cite{wang2022ganet} shows the best multimodal prediction metrics, with an approximate amount of 6.2M of parameters of 1612 ms given a batch size of 128 traffic scenarios and an average number of 30 agents per scene. As expected, progressively removing the map influence (remove map from decoder, remove goal areas estimation) in the model we decrease the MP performance with a noticeable parameter decrease. 

%\cite{schmidt2022crat} proposes a non-probabilistic model to reduce model complexity and avoid turning the learning process into a multitask problem. It shows the lowest number of parameters and runtime inference, but the MP results are not up-to-pair with the GANet social approach. 

In our case, we study the influence of substituting the modified ActorNet~\cite{wang2022ganet, liang2020learning} social encoder proposed by GANet, which uses RNNs. Our proposal replaces these by a Linear embedding, a Positional Encoding and Encoder Transformer. Moreover, we add the aforementioned agent metadata (object type and track category), and we substitute the Actor to Actor attention of the fusion cycle for a GCN~\cite{schmidt2022crat} operator to enhance agents global interaction. 
%
It can be appreciated how we obtained a similar performance (both with 128 latent dimension in \textit{Ours-m}, and 64 latent in \textit{Ours-s}), reducing the parameters and inference time. 

Finally, our best model, which includes heuristic proposals that serve as a preliminar multi-modal guidance for the model and motion refinement to improve the quality of the final predictions, obtains a performance on pair with \cite{wang2022ganet}, reducing the number of parameters and inference time about 21\% and 41\% respectively. We can appreciate in Table \ref{table:table_2} how our model generalizes well in the test set, with results (both in uni-modal and multi-modal prediction) up-to-pair with other state-of-the-art algorithms.

\begin{table*}[h]
	\centering
	\caption{Comparison of methods in the \textbf{Argoverse 2 Validation} Set. We show the number of parameters for each model, prediction metrics (minADE, minFDE and brier-minFDE) for the multimodal scenario (k=6) and runtime. Runtime was measured on a single GPU A100-SXM4 (using batch 128). Our experiments are indicated using $\dagger$. We use as baseline method GANet~\cite{wang2022ganet}.}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{lcccccc}
			\toprule
			Method & Map & \# Par.~(M) & minADE~(m) $\downarrow$ & minFDE~(m) $\downarrow$ & brier-minFDE~(m) $\downarrow$ & Runtime~(ms) $\downarrow$ \\
			\midrule
			GANet~\cite{wang2022ganet}                & Yes & 6.2 & 0.806 & 1.402 & 2.02 & 1612 \\
			GANet w/o Map Decoder~\cite{wang2022ganet} & Yes & 5.7 & 0.84 & 1.55 & 2.18 & 1353 \\
			GANet w/o Goal Areas~\cite{wang2022ganet} & Yes & 4.5 & 0.87 & 1.66 & 2.29 & 1134 \\
			GANet w/o map~\cite{wang2022ganet}        & No & 1.79 & 1.034 & 2.212 & 2.825 & 838 \\
			%social only
			$\dagger$ CRAT-Pred~\cite{schmidt2022crat} & No & 0.53 & 1.31 & 2.78 & 3.65 & 223 \\
			\midrule
			$\dagger$ Ours-base: GANet~\cite{wang2022ganet} ~~~ActorNet $\rightarrow$ Attention Transformer & Yes & 5.0 & 0.83 & 1.45 & 2.07 & 923 \\
			$\dagger$ Ours-m: Ours-base + A2A $\rightarrow$ C-GCN + Metadata  & Yes & 4.74 & 0.82 & 1.43 & 2.05 & 892 \\
			% Ours-s = dim=64 --> smaller version
			$\dagger$ Ours-s: Ours-base + A2A $\rightarrow$ C-GCN + Metadata (64 latent size) & Yes & 1.2 & 0.88 & 1.53 & 2.15 & 893 \\
			% Ours final
			$\dagger$ \textbf{Ours:} Ours-m + Proposals + Motion Refinement & Yes & 4.92 & 0.81 & 1.42 & 2.04 & 946 \\
			\bottomrule
		\end{tabular}
	}
	\label{table:table_1}
\end{table*}

We provide advanced qualitative samples in Figure~\ref{fig:results}, where we show the HD Map of real traffic scenes, heuristic trajectory proposals in the form of centerlines, and the multimodal predictions from our model including their respective confidences (the higher, the most probable). 

As we discussed in~\ref{sec:ours}, we designed our model to ensure realistic predictions. We can appreciate that all the predictions are plausible and constrained to the scene geometry \eg lane distribution and centerlines. We believe our heuristic proposals help to regularize the model and produce realistic predictions that would ensure traffic safety. For simplicity, we only illustrate the heuristic proposals for the focal agent and ego-vehicle. We believe our software for qualitative analysis of MP models on the well-known Argoverse 2.0~\cite{wilson2023argoverse} is fundamental and a core contribution.

\begin{table*}[h]
	\small
	\caption{Results on \textbf{Argoverse 2 Test} dataset. The "-" denotes that this result was not reported in their paper. Some numbers are borrowed from~\cite{wang2022ganet}. For all the metrics, the lower, the better.}
	\label{test}
	\centering
	\resizebox{\linewidth}{!}{
	\begin{tabular}{cccccccc}
		\toprule
		Method     &\makecell[c]{b-minFDE\\(K=6)} &\makecell[c]{MR\\(K=6)} &\makecell[c]{minFDE\\(K=6)} &\makecell[c]{minADE\\(K=6)} &\makecell[c]{minFDE\\(K=1)} &\makecell[c]{minADE\\(K=1)} &\makecell[c]{MR\\(K=1)} \\
		\midrule
		
		DirEC &3.29  &0.52 &2.83 &1.26 &	6.82 &	2.67 &0.73    \\
		
		drivingfree&3.03 &	0.49 &2.58 &1.17 &6.26 & 2.47 &0.72     \\
		
		LGU	&2.77 & 0.37 & 2.15 &1.05 &6.91 & 2.77 & 0.73 \\
		
		Autowise.AI(GNA) &2.45	&0.29 &1.82	&0.91 &6.27	& 2.47  &	0.71\\
		
		Timeformer~\cite{gilles2021thomas}   &2.16	&0.20 &1.51 & 0.88 &4.71	&1.95 &0.64\\
		
		QCNet    &2.14	&0.24 &1.58 &0.76	&4.79 & 1.89 &0.63 \\
		
		\textit{OPPred w/o Ensemble}~\cite{zhang2022banet} 	&2.03	&0.180 & 1.389		&0.733	&4.70	&1.84  &0.615\\
		
		\textit{TENET w/o Ensemble}~\cite{wang2022tenet}&2.01&- &-	&-	&-	&-&-\\
		
		Polkach(VILaneIter)  &2.00	&0.19 & 1.39	&\textbf{0.71}	&4.74	&1.82	&0.61 \\
		
		GANet &\textbf{1.969}	& \textbf{0.171} &\textbf{1.352}	&0.728 &\textbf{4.475} &\textbf{1.775} &\textbf{0.597}	 \\
		
		\midrule
		Ours & 1.98 & 0.185 & 1.37 & 0.73 & 4.53 & 1.79 & 0.608 \\
		\bottomrule
	\end{tabular}}
	\label{table:table_2}
\end{table*}

% of 0.0001. 

% Where $\alpha=1.0$ , $\beta=0.1$ and $\gamma=0.65$ initially, and can be manually adjusted during training (especially $\gamma$).

\begin{figure}[h]
	\centering
	\setlength{\tabcolsep}{2.0pt}
	%\renewcommand{\arraystretch}{1.2}%
	\begin{tabular}{ccc}
		
		%\fbox{\includegraphics[width=0.32\linewidth]{figures/qualitative_results/02b57064-6fe3-41a4-8ad5-24e51abbdac4_input_data.pdf}} & 
		\includegraphics[width=0.32\linewidth]{5_qualitative_results_argoverse_2/02b57064-6fe3-41a4-8ad5-24e51abbdac4_input_data.pdf} & 
		\includegraphics[width=0.32\linewidth]{5_qualitative_results_argoverse_2/val_candidates_6_02b57064-6fe3-41a4-8ad5-24e51abbdac4.pdf} &
		\includegraphics[width=0.32\linewidth]{5_qualitative_results_argoverse_2/02b57064-6fe3-41a4-8ad5-24e51abbdac4_mm_prediction.pdf}
		\tabularnewline
		\tabularnewline
		\includegraphics[width=0.32\linewidth]{5_qualitative_results_argoverse_2/0499c82d-458b-464d-a603-e355e5da4ec7_input_data.pdf} & 
		\includegraphics[width=0.32\linewidth]{5_qualitative_results_argoverse_2/val_candidates_6_0499c82d-458b-464d-a603-e355e5da4ec7.pdf} &
		\includegraphics[width=0.32\linewidth]{5_qualitative_results_argoverse_2/0499c82d-458b-464d-a603-e355e5da4ec7_mm_prediction.pdf}
		\tabularnewline
		\tabularnewline
		\includegraphics[width=0.32\linewidth]{5_qualitative_results_argoverse_2/06f1ac5c-712f-4b37-ad17-6afaae41b753_input_data.pdf} & 
		\includegraphics[width=0.32\linewidth]{5_qualitative_results_argoverse_2/val_candidates_6_06f1ac5c-712f-4b37-ad17-6afaae41b753.pdf} &
		\includegraphics[width=0.32\linewidth]{5_qualitative_results_argoverse_2/06f1ac5c-712f-4b37-ad17-6afaae41b753_mm_prediction.pdf}
		%
		\tabularnewline
		\tabularnewline
		HD Map input & Heuristic proposals & Multimodal (k=6) predictions \tabularnewline
	\end{tabular}
	\caption[Qualitative Results on challenging scenarios in Argoverse 2 using our best model]{Qualitative Results on challenging scenarios in Argoverse 2 using our best model. We represent: our vehicle (\textbf{\textcolor{blue}{ego}}), the \textbf{\textcolor{YellowOrange}{focal agent}}, the \textbf{\color{blue-violet}{relevant agents}} in the scene, and \textbf{\textcolor{gray}{other agents}}. We can also see the \textbf{\textcolor{red}{ground-truth}} trajectory of the target agent, our \textbf{\textcolor{ForestGreen}{multimodal predictions}} (with the corresponding \textbf{confidences}). We also highlight the most important topology of the road, such as \color{blue-violet}{pedestrian crossing} and boundaries mark type. We show, from left to right, a general view of the traffic scenario (including and map information), the heuristic proposals for each agent (we only include the \textbf{\textcolor{blue}{ego}} and \textbf{\textcolor{YellowOrange}{focal agent}} for simplicity) and the multimodal prediction (\textit{k} = 6) for the \textbf{\textcolor{YellowOrange}{focal agent}}, including the corresponding confidences (the higher, the most probable)}
	\label{fig:5_argoverse_2_qualitative_results}
\end{figure}

% Additional bio: dong2017hardmine

\section{Decision-Making}
\label{sec:5_decision_making}

% https://xindiwu.github.io/data/motion.pdf

The increasing popularity of autonomous vehicles (AVs) has brought with it significant challenges in ensuring safe and effective decision-making, particularly in complex urban driving scenarios \cite{Yurtsever2019}. Reinforcement learning (RL) techniques have emerged as a promising solution to address these challenges \cite{Ravi2020}. They enable AVs to learn from their interactions with the driving environment, without relying on pre-defined rules. However, RL-based approaches still face a number of limitations that can hinder their development, including issues related to state representation.

A key challenge in RL-based AVs is the development of effective state representations that can account for the complexity of urban driving scenarios. Unlike in simpler environments, state representations for urban driving must be able to incorporate a wide range of information, including dynamic features of traffic flows and interactions among different agents. Finding ways to effectively encode this information and develop accurate state representations is essential to enabling RL-based AVs to generalize to various scenarios and make effective driving decisions. Distilling predictive information from scene representations can aid in the development of effective decision-making policies for AVs. By better understanding the potential consequences of different driving actions, RL-based AVs can make more informed decisions that lead to safer and more efficient driving behaviour.

In this section we propose an approach to enhance the efficiency and generalization of RL-based AVs in urban driving scenarios. Specifically, we introduce the use of a Motion Prediction (MP) module to obtain the future positions of the ego-vehicle and the surrounding vehicles (adversaries) in the scenario. These predictions are the input to an RL-based decision-making module that executes high-level actions. Our approach is developed using the Proximal Policy Optimization (PPO) algorithm  \cite{Schulman2017}.  We carry out an evaluation in the unsignalized T-intersection scenario shown in Fig. \ref{fig:5_dm_t-intersection_scenario} with and without the proposed state representation and provide a comparison with some baseline methods.

\begin{figure}[h]
	\centering
	%\includegraphics[width=0.45\textwidth]{images/teaser.png}
	\includegraphics[width=0.45\textwidth]{5_dm_t-intersection_scenario.pdf}
	\caption{Simulation environment. A visualization of the ego-vehicle driving in the T-intersection scenario. The past positions of the adversaries (\textbf{\color{YellowOrange}{yellow}}) and the predicted trajectories (\textbf{\textcolor{blue}{blue}}) are represented in the scenario.}
	\label{fig:5_dm_t-intersection_scenario}
\end{figure}

In recent years, RL has emerged as a promising approach for developing decision-making policies for AVs \cite{Mnih2015}, outperforming ruled-based approaches that usually cannot solve complex situations \cite{Zhu2019}. However, a large number of interactions with the environment are required to obtain the desired policy. This is why other approaches such as imitation learning \cite{SilverHuangEtAl16nature} and inverse RL \cite{Ross2010}, based on human experts' behaviours are also used in the literature. 

This work is focused on a critical issue for RL-based AVs, which is the state representation problem. Traditional state representations often focus on low-dimensional features such as distance to obstacles, lane positions, and vehicle velocities \cite{Rodrigo2023}. However, these representations may not be sufficient to capture the complex interactions among different agents and road structures in urban driving scenarios. To address the state representation problem, some methods have been proposed that use higher-dimensional or learned representations, such as convolutional neural networks \cite{Johan2018} and recurrent neural networks \cite{Tram2018}; other methods have been proposed to use more detailed representations, such as  Bird-Eye-View images \cite{zhang2021endtoend}, image augmentation \cite{kostrikov2021image} or occupancy grids \cite{moghadam2019hierarchical}. These methods have shown promising results in improving the generalization and robustness of the decision-making approaches. Recently, transformer-based approaches have gained increasing attention for their ability to capture long-term dependencies and interactions among different entities in sequential data. In the context of AVs, transformers have been used to reduce the computational load in end-to-end approaches \cite{Li4} and anticipate future states with prediction-aware planning \cite{valiente2022predictionaware}.

\begin{figure}[h]
	\centering        
	\includegraphics[width=0.95\textwidth]{5_dm_prediction_framework.pdf}
	\caption{An overview of the Augmented Reinforcement Learning with Efficient Social-based Motion Prediction for Autonomous Decision-Making. The observations (both position and ID, so, trackers) of the vehicles in the scenario are obtained from the simulator. The MP module estimates the future positions of these vehicles, taking into account the most plausible score of a multimodal prediction. The decision-making module selects high-level actions based on this information. These actions are executed by the simulator, which provides a new state to the framework. }
	\label{fig:5_dm_prediction_framework}
\end{figure}

The objective of this study is to illustrate the efficacy of employing a low-dimensional state representation in conjunction with an MP method. We aim to prove that the proposed framework can lead to good performance in urban scenarios. More specifically, we present the following contributions:

\begin{itemize}
	\item The augmentation of RL techniques with MP to improve state representation. By predicting vehicle trajectories, we can better capture the complex interactions between different agents and road structures in urban driving scenarios. 
	
	\item Higher explainability than end-to-end methods. Intermediate states are accessible in our approach. This can help to understand the decisions made.
	
	\item We provide a comparison with baseline methods in a standard scenario. We demonstrate that our approach leads to some improvements in performance, particularly in scenarios with high velocities.
\end{itemize}
	
The RL framework proposed in this work, which executes high-level decisions to solve urban driving scenarios, is represented in Fig. \ref{fig:5_dm_prediction_framework}. The past observations of the position of adversaries are obtained from the environment. This information is provided to the MP module, which estimates future positions. The PPO algorithm takes these predictions and generates the decision-making output.

We propose two different learning processes: supervised learning for the motion prediction module and a reinforcement learning approach for the decision-making module. These two modules are trained separately, which allows access to the information of the predictions that feed the decision-making module.

\begin{figure}[h]
	\centering
	\setlength{\tabcolsep}{2.0pt}
	\includegraphics[width=\linewidth]{5_social_prediction_baseline.pdf}
	\caption{Overview of our Efficient Social-based Motion Prediction. The main inputs are the relative displacements and centers (last observations) of the agents in the ego-vehicle frame. The relative displacements and centers are encoded through a sequence of Residual, Convolutional Blocks, LSTM and Attention module. Finally, a multimodal decoder based on residual blocks is used to predict \textit{K} final future trajectories (modes) and their confidence scores.}
	
	% On the other hand, the agents centers are encoded through a linear layer to initialize the hidden and cell vector of the LSTM layer, which processes the previously latent motion history
	\label{fig:5_social_prediction_baseline}
\end{figure}

Predicting the future behaviour of traffic agents \cite{wilson2023argoverse} around the ego-vehicle is one of the key unsolved challenges in reaching full self-driving autonomy. In that sense, an Autonomous Driving Stack (ADS) can be hierarchically broken down into the following tasks: (i) perception, responsible for identifying what is around the vehicle, then track and predict what will happen next, (ii) planning and decision-making, deciding what the AD stack is going to do in the near future and (iii) control, that sends the corresponding low-level commands (brake, throttle and steering angle) to the vehicle. 

This prediction is required to be multi-modal, which means given the past motion of a particular vehicle and its surrounding scene, there may exist more than one possible future behaviour (also known as modes). Therefore, MP models need to cover the different choices a driver could make (i.e. going straight or turning, accelerations or slowing down) as a possible trajectory in the immediate future or as a probability distribution of the agent's future location. In other words, when an ADS attempts to make a specific action (e.g. left turn), it must consider the future motion of the other vehicles, since the own future actions (also known as decision-making or behaviour planning) depends on the all possible maneuvers of the other agents of the scene for safe driving. %In our case, though we train the MP model in a multi-modal way, we provide the best mode (trajectory with the highest confidence).

Since our proposed pipeline (Fig. \ref{fig:5_dm_prediction_framework}) is multi-stage to provide a more interpretable framework, we follow the principles of the Argoverse 2 Motion Forecasting dataset \cite{wilson2023argoverse} to train our prediction model. In our case, we build an efficient model solely based on past trajectories (motion history, $obs_{len} = 50$) and agents interactions, taking into account the corresponding traffic rules, not requiring fully-annotated HD map information, to predict $obs_{len} = 60$ future steps. % Given the standard frequency of 10 Hz, this makes 5s and 6s of observation and prediction respectively. 

The SMARTS \cite{SMARTS} framework provides only the positions of the agents in the timestamp \textit{t}. Nevertheless, in order to predict the future $pred_{len}$ trajectories of the agents, we require their corresponding $obs_{len}$ trackers over a certain set of observations. Most vehicle prediction datasets \cite{wilson2023argoverse} aim to predict the future behaviour of a target agent assuming the surrounding agents have been detected and tracked (so, monitored over time) and the map information is also provided. In that sense, since SMARTS provide the agents in the same order for consecutive timestamps (that is, the agent 5, unless it disappears from the scene, will be the agent 5 again in the next frame), we are able to compute a FIFO (\textit{First Input First Output}) for each agent, not requiring data association \cite{kuhn1955hungarian} to perform this task. 

On top of that, as proposed by multiple methods \cite{liang2020learning, gomez2023improving}, we consider only the vehicles that are observable at \textit{t=0}, handling those agents that are not observed over the full sequence spectrum (observation length = \textit{$obs_{len}$} + prediction length = \textit{$pred_{len}$}) by concatenating a binary flag $b_i^t$ that indicates if the agent is padded or not. In particular, we filter the static elements and track fragments scored by Argoverse 2 to get only the most relevant traffic agents, reducing the number of agents to be considered in complex traffic scenarios. Furthermore, to make the model translation and rotation invariant, the coordinate system in our model is BEV-centered of a given target agent at $t = 0$, and we use the orientation from the target location given in the same timestamp as the positive $x$-axis. Note that this representation will benefit the model to have a common representation to enhance the generalization of the model and prevent overfitting. Once the scene has been translated and rotated, instead of using absolute 2D-BEV (\textit{xy} plane), the input for the agent \textit{i} is a series of relative displacements:

\begin{equation}
	\Delta \boldsymbol{\nu}^{t}_i = \boldsymbol{\nu}^{t}_i - \boldsymbol{\nu}^{t-1}_i
\end{equation}

Where $\boldsymbol{\nu}^{t}_i$ represents the state vector (in this case, \textit{xy} position of the agent \textit{i} at timestamp \textit{t}).

Since our model focus on an efficient encoding of the social information, we base our model on the ActorNet backbone proposed by \cite{liang2020learning}, as observed in Fig. \ref{fig:5_social_prediction_baseline}. While both CNNs and RNNs can be used for temporal data, ActorNet uses an 1D CNN to process the trajectory input for its effectiveness in extracting multi-scale features and efficiency in parallel computing. The output is a temporal feature map, whose element at $t=0$ is used as the actor feature. The network has $3$ groups/scales of 1D convolutions. Each group consists of $2$ residual blocks, with the stride of the first block as $2$. Then, a Feature Pyramid Network (FPN) \cite{lin2017feature} is used to fuse the multi-scale features, and apply another residual block to obtain the output tensor. For all layers, the convolution kernel size is $3$ and the number of output channels is $128$. Layer normalization and the Rectified Linear Unit (ReLU) are used after each convolution. 

On top of that, in a similar way to \cite{wang2022ganet}, the agents centers (observations at $t=0$) are encoded through a linear layer to initialize the hidden and cell vector of the LSTM layer, which processes the previously latent motion history. After that, a social attention block is used to compute the most representative agents interaction.

Taking the final actor features after motion history and agents interaction, a multi-modal prediction header outputs the final motion forecasting. For each agent, it predicts $K$ possible future trajectories and their confidence scores. The header has two branches, a regression branch to predict the trajectory of each mode and a classification branch to predict the confidence score of each mode.

For the $m$-th actor, a residual block and a linear layer in the regression branch to regress the $K$ sequences of BEV coordinates is obtained:

\begin{equation}
	O_{m, \text{reg}} = \{ (\mathbf{p}_{m,1}^k, \mathbf{p}_{m,2}^k, ..., \mathbf{p}_{m,T}^k) \}_{k \in [0, K-1]}
\end{equation}

where $O_{m, \text{reg}}$ is the whole set of regressions and $\mathbf{p}_{m,i}^k$ is the predicted $m$-th actor's BEV coordinates of the $k$-th mode at the $i$-th time step.

On the other hand, for the classification branch, a MLP to $\mathbf{p}_{m,T}^k - \mathbf{p}_{m,0}$ to get $K$ distance embeddings is applied. Finally, each distance embedding is concatenated with the actor feature, applying a residual block and a linear layer to output $K$ confidence scores, $O_{m, \text{cls}} = (c_{m,0}, c_{m,1}, ..., c_{m,K-1})$.

In this particular work, we take the most plausible future trajectory for each agent (both the adversaries and the ego-vehicle) in the following timestamps: \textit{t=0}, \textit{t=10}, \textit{t=20} and \textit{t=30}, which correspond to the current position and the predicted position of the corresponding agent 1, 2 and 3 seconds in the future respectively. Even though we train our prediction model following the principles of Argoverse 2 (5s and 6s of observation and prediction respectively), given the velocities and traffic density of the experiments run in the SMARTS simulator , we believe that predicting 3s in the future is enough for this purpose to evaluate the high-level actions of decision-making layer preventing overfitting. In future works, we will design more difficult scenearios, up-to-pair with the Argoverse 2 dataset (specially in terms of intersections or lane change behaviours at high speed) where multimodal predictions with higher prediction horizons will be required.

A Markov Decision Process (MDP) is a discrete-time stochastic control process that provides a mathematical framework for modelling decision-making environments. An MDP is a tuple $(S,A,P,R)$ in which $S$ is a set of states named state space, $A$ is a set of actions named action space, $P$ is the probability function and $R$ is a reward function. An algorithm with a given state $s \in S$ takes an action $a \in A$ transitioning to $s'$ with a probability $P(s,a,s')$, and getting a reward $R(s,a,s')$ as shown in Fig. \ref{fig:5_dm_prediction_framework}. This algorithm iterates through this loop to learn a desired behaviour. 

The goal in an MDP is to find a good policy for the decision-making system. The objective is to find the optimal policy $\pi*(s)$, that maximizes the cumulative function of the future reward.

We represent the driving scenario as an MDP to develop our decision-making module. We consider the output of the MP module as an input to this module. The state space, action space, and reward functions are defined in this section.

The state is defined by the predicted trajectories of the ego-vehicle and the five closest vehicles in the scenario.
\begin{equation}
	s_t = (K^{ego}_t, K^1_t, ..., K^{5}_t)
	\label{eq:state}
\end{equation}

where $K^{i}_t = (x^i_{t_0}, y^i_{t_0}, x^i_{t_1}, y^i_{t_1}, x^i_{t_2}, y^i_{t_2}, x^i_{t_3}, y^i_{t_3})$ contains the future estimations of the positions of the vehicles across a future horizon of three seconds. A representation of a state vector is shown in Fig \ref{fig:5_dm_state}, where the vehicles' predicted positions are represented.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.40\textwidth]{5_dm_state.pdf}
	\caption{The ego-vehicle (\color{brickred}{red}) and the adversaries (\color{cadetblue}{blue}) predicted positions in the next three seconds are represented.}	
	\label{fig:5_dm_state}
\end{figure}

We propose a discrete action space formed by two actions. A low-level controller implemented by the simulator is in charge of performing smooth driving based on these actions. These actions are focused on the ego-vehicle velocity. The first action aims to reach a desired predefined velocity and the second action reduces the velocity until the vehicle stops. The action space is defined as:

\begin{equation}
	a=(Drive, Stop)    
	\label{eq:action}
\end{equation}

\subsubsection{Reward Function}
The reward function is defined in terms of success or failure. A negative reward is given when there is a collision and a positive reward is given when the vehicle reaches the success point, situated at the end of the scenario.

\begin{equation}
	r = k_v * v_{ego} + \left\lbrace\begin{array}{lcc}
		1 & if & sucess \\ 
		-1 & if & collision \\
	\end{array}\right.
	\label{eq:Reward}
\end{equation}

As shown in Equation \ref{eq:Reward}, we add one more factor to the reward function to encourage the ego-vehicle to move. We propose a cumulative reward based on its longitudinal velocity. We use a constant small enough to ensure that the reward per episode is bounded between -1 and 1.

Our approach for the RL implementation (Fig. \ref{fig:5_dm_RL_network}) builds upon our previous research \cite{Gutierrez2022}, where we demonstrated that incorporating a feature extractor module to a PPO algorithm yields improved metrics and faster convergence. 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.45\textwidth]{5_dm_RL_network.pdf}
	\caption{The neural network architecture consists of two fully connected layers followed by the concatenation of both adversaries and ego vehicle features. The resulting concatenated features are then passed through an actor-critic structure, which comprises two layers, each containing 128 neurons.}	
	\label{fig:5_dm_RL_network}
\end{figure}

In this implementation, we introduce separate feature extractors for adversaries and the ego-vehicle, which are then concatenated into the input for the PPO algorithm. This algorithm consists of two models: the Actor, responsible for selecting an action based on the policy, and the Critic, which estimates the value function.

To validate the performance of our approach, an intersection scenario is implemented in SMARTS, which is a SUMO \cite{Sumo} based simulation platform for research on autonomous driving.

\begin{figure}[h]
	\centering        
	\includegraphics[width=0.95\textwidth]{5_dm_prediction_results.pdf}
	\caption{Simulation overview of our system behaviour. The red car follows the green path, and each image represents a different frame of the simulation. We also show the predicted positions below each image and the actions taken by the decision-making module.}
	\label{fig:5_dm_prediction_results}
\end{figure}

The scenario is an urban unsignalized T-intersection. The objective is to execute a left turn maneuver in the absence of traffic signal protection, allowing the continuous flow of traffic. Fig. \ref{fig:5_dm_t-intersection_scenario} illustrates the drivable area (highlighted in green) where the ego-vehicle can navigate to reach the target location. Simulations are reset under three conditions: 1) the ego-vehicle successfully reaches the target, 2) the episodic step surpasses the maximum time steps limit, and 3) the ego-vehicle collides or deviates from the drivable route.

We define different scenario configurations to test the performance of the proposed framework. First, the regular T-intersection scenario which is defined in SMARTS, where a random number of vehicles between [5-10] are spawned every minute, and the maximum velocity of these vehicles is 14 km/h. Then, we propose different configurations increasing the maximum velocity of the adversaries to 30, 60, and 90 km/h. 

In decision-making, the success rate serves as a direct measure of the effectiveness of the RL agent in accomplishing the designated task. Besides, the average time of the episode is a common metric used in the literature. These metrics are defined as:

\begin{itemize}
	\item $success~[\%] = n_{success}/n_{success}$
	\item $t_{e} [s] = \sum{t_{n}}/n_{episodes}$
\end{itemize}

where the number of episodes $n_{e}$ is 100 and simulation time is measured in seconds. 

To evaluate the performance of our approach we first present a comparison with the existing methods for decision-making in the literature and then an ablation study is conducted. 

This first study compares the proposed approach with other existing methods for decision-making. The baseline methods used for comparison are Data-regularized Q-learning (DrQ) \cite{kostrikov2021image}, Soft Actor-Critic (SAC) \cite{haarnoja2019soft}, and PPO. These methods have different features and serve as reference points for evaluating the proposed approach. The results presented in Table \ref{table:comp} demonstrate a higher success rate of our proposal.

\begin{table}[h!]
	\centering
	\caption{A comparison of the proposed framework against the existing baselines in the T-intersection scenario. The success rate S[\%] and the average episode time $t_{e}$ are presented.}
	\label{table:comp}
	\setlength{\extrarowheight}{2pt}
	\begin{tabular}{cccccc} 
		\ChangeRT{1pt}
		Metric & Ours & PPO & SAC & DrQ\\
		\hline 
		S[\%] & \textbf{80} & 70 & 68 & 78 \\ 
		$t_{e}$(s) & 22.3 & 36.4 & 19.2 & 18.2 \\
		\hline
		\ChangeRT{1pt}
	\end{tabular}
\end{table}

Two ablative studies are carried out to see how the use of motion prediction in the state representation can improve the performance of the framework. The first approach is to use just the position of the vehicles as the input to the decision-making module and the second approach is to use the locations over the past five seconds. We test the three approaches under the previously introduced configurations with different adversaries' velocities, from 15km/h to 90km/h. To correctly evaluate the performance of the decision-making system we propose different metrics that aim to provide a better comprehension of the behaviour. We believe that the success rate is still a good indicator, but we slightly modify the average time, only considering the successful episodes to calculate this metric. In addition, we include a new relevant metric: the average ego-vehicle velocity when a collision takes place $v_{c}$.  

The results presented in Table \ref{table:5_dm_ablation_study_smarts} show that the use of the predicted positions in the state vector avoids more collisions as the velocities increase. Besides, the average collision velocity and the average time to complete the scenario are lower for the proposed approach. 

\begin{table}[h]
	\centering
	\caption{An ablation study comparing three state representations with the different scenario configurations: Current positions, Past positions, and Future positions. The success rate S[\%], the episode time $t_{e}$ in these successful episodes, and the average velocity of collision $v_{c}$ are presented.}
	\label{table:5_dm_ablation_study_smarts}
	\setlength{\extrarowheight}{2pt}
	\begin{tabular}{cccccc} 
		\ChangeRT{1pt}
		& Metric & 15 km/h & 30 km/h & 60 km/h & 90 km/h  \\
		\hline 
		\multirow{3}{*}{Future}
		&S [\%] & 80 & 78 & 78 & 77 \\ 
		&$t_{e}$ (s) & 22.3 & 23.3 & 23.4 & 23.3 \\
		&$v_{c}$ (km/h) & 4.9 & 5.1 & 5.6 & 5.6 \\
		\hline
		\multirow{3}{*}{Current}
		&S [\%] & 77 & 73 & 70 & 70 \\ 
		&$t_{e}$ (s) & 25.1 & 23.4 & 23.4 & 23.3 \\
		&$v_{c}$ (km/h) & 5.1 & 5.5 & 6.1 & 6.2 \\
		\hline
		\multirow{3}{*}{Past}
		&S [\%] & 78 & 75 & 72 & 71 \\ 
		&$t_{e}$ (s) & 24.2 & 23.3 & 23.2 & 23.1 \\
		&$v_{c}$ (km/h) & 4.9 & 5.2 & 5.9 & 6.0 \\
		\hline
		\ChangeRT{1pt}
	\end{tabular}
\end{table}

Finally, an overview of the behaviour of our system is shown in Fig. \ref{fig:5_dm_prediction_results}. The ego-vehicle in red follows the trajectory defined in green. Each image represents a different frame of the simulation and the respective predictions of the positions are displayed below. Besides, the action executed by the decision-making module for each frame is shown.

The proposed method incorporates an Efficient Social-based Motion Prediction module, which predicts the future positions of vehicles within the scenario. These predictions improve a Reinforcement Learning-based Decision Making module. The results of the study demonstrate that our approach achieves significant performance improvements, particularly in scenarios involving high velocities.

This research opens up several potential directions for future work. The proposed approach can be evaluated in a broader range of urban driving scenarios to assess its robustness and scalability, up-to-pair with the difficulty of the Argoverse 2 dataset scenarios (specially in terms of intersections or lane change behaviours at high speed) where multimodal predictions with higher prediction horizons will be required. Furthermore, the Reinforcement Learning-based Decision Making module can be enhanced by exploring advanced algorithms.

\section{Holistic Simulation in CARLA}
\label{sec:5_holistic_simulation}

The main contribution of our AD PerDevKit is the creation of a ground-truth generation tool for the surrounding obstacles of the ego-vehicle using CARLA and ROS. For its implementation, as explained in section \ref{carla_autonomous_driving_simulator}, the CARLA-ROS brige is used so that the simultaneous execution of CARLA and this tool is not necessary, since the execution of both programs can be very demanding due to the simulator requirements. This way it is possible to record a rosbag (a file with all the ROS messages) with the GT information, so that only an area around the ego-vehicle is analyzed and not the whole obstacles in the CARLA runtime.

The messages created by CARLA contain the information of the different objects of the environment in relation to the map on which it is being used. However, to be used independently, the obstacles must be referenced to the ego-vehicle. Therefore, it is necessary to perform the different transformations to go from a coordinate system based on the map to a coordinate system based on the ego-vehicle.

\subsection{Method for obtaining 2D bounding boxes}
While the CARLA-ROS bridge gives access to the 3D bounding boxes of the objects in the environment, it does not provide the projected 2D bounding boxes of these 3D objects in the camera. We perform a series of transformations to go from the coordinates of the world to the pixels containing that object within each image generated by the camera.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.48\textwidth]{5_ad_perdevkit_change_of_coordinates_from_world_to_camera.png}
	\caption{Geometry transformation from world to camera.}
	\label{fig:5_ad_perdevkit_change_of_coordinates_from_world_to_camera}
\end{figure}

A conversion from the world coordinates to the image ones is necessary to transform the eight vertices of a 3D bounding box into the vertices of a 2D bounding box. For this purpose, it is necessary to apply some geometry transformations as shown in Fig. \ref{fig:5_ad_perdevkit_change_of_coordinates_from_world_to_camera}.

Using homogeneous coordinates, the correspondence between a 3D point in the World Coordinate System (WCS) and its projected pixel in the Image Coordinate System at origin (ICS$_O$) is calculated applying the following equation:

\begin{center}
	$
	\begin{bmatrix} wu \\ wv \\ w \end{bmatrix}
	=
	M_{3x4}
	\begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}
	$
\end{center}
where $M_{(3x4)}=M_{int}M_{ext}$ is the camera projection matrix, formed by the product of intrinsic matrix and extrinsic matrix, defined as follow:
\begin{center}
	$
	M_{int}
	=
	\begin{bmatrix}
		f/dx & 0 & u_0 \\
		0 & f/dy & v_0 \\
		0 & 0 & 1 \\
	\end{bmatrix}
	$
\end{center}
\begin{center}
	$
	M_{ext}
	=
	\begin{bmatrix}
		R_{3x3} & T_{3x1} \\
		0 & 1 \\
	\end{bmatrix}
	$
\end{center}

being $f$ focal distance, $(dx,dy)$ physical size of the pixel, $(u_0, v_0)$ centre of the image in pixels, $T_{3x1}$ translation matrix and $R_{3x3}$ rotation matrix from the world (WCS) to the camera (CCS). 

The steps to perform this conversion are the following:

\begin{enumerate}
    \item Transformation from World Coordinate System to Camera Coordinate System (WCS/CCS):
	
	During the geometric transformation of the world coordinates to the image, homogeneous coordinates are used, such a coordinate system adds a fourth component to the three-dimensional coordinates. This coordinate system has a direct equivalence to the Cartesian coordinate system as shown below.

    \begin{center}
	    $(x, y, z) \rightarrow (x', y', z', w)$\\
        $(x, y, z) = (x'/w, y'/w, z'/w)$
    \end{center}

   Firstly the translation matrix from the world to the camera is obtained.

   \begin{center}
	   $
	   T_{3x1}
	   =
	   \begin{bmatrix} T_x \\ T_y \\ T_z \end{bmatrix}
	   $
	\end{center}

    Afterwards, the rotation matrix is built over the 3 rotation matrices of the different dimensions.

    \begin{center}
	    $ R = R_x(\alpha) R_y(\beta) T_z(\gamma) $
	\end{center}

	Based on the translation matrix and the rotation matrix, a matrix of 4x4 dimensions is built, which together with the matrix of the points to be transformed, gives a matrix with the points in the camera coordinate system.

\begin{center}
	   $
	    P_c
	    =
	    \begin{bmatrix} R_{3x3} & T_{3x1} \\ 0 & 1 \end{bmatrix}
		   \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}
		 =
		   \begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix}
		  $
\end{center}
	    \item Transformation from Camera Coordinate System to Image Coordinate System in the center of the image (CCS/ICS$_C$):

	    During the transformation to the image coordinate system, it is necessary to work with the distance between the optical center and the image plane, also called focal length. Thereby a matrix is constructed, which together with the points in the camera coordinate system is obtained in the image coordinate system in the center of the image.
	
	    \begin{center}
		    $
		    \begin{bmatrix} wx \\ wy \\ w \end{bmatrix}
		    =
		    \begin{bmatrix}
			    f & 0 & 0 & 0 \\
			    0 & f & 0 & 0 \\
			    0 & 0 & 1 & 0 \\
			    \end{bmatrix}
		    \begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix}
		    $
		    \end{center}
	    \item Transformation from Image Coordinate System in the center of the image to Image Coordinate System at origin (ICS$_C$/ICS$_O$) in pixels:
	    \par
	    \vspace{1mm}
	    As a last step, the points are transformed to image pixels using the size of the camera sensor as well as the pixel resolution of the image.
	
	    \begin{center}
		    $
		    \begin{bmatrix} wu \\ wv \\ w \end{bmatrix}
		    =
		    \begin{bmatrix}
			    1/dx & 0 & u_0 \\
			    0 & 1/dy & v_0 \\
			    0 & 0 & 1 \\
			    \end{bmatrix}
		    \begin{bmatrix} wx \\ wy \\ w \end{bmatrix}
		    $
		\end{center}
\end{enumerate}
	
After all this, the vertices from the different 3D bounding boxes of the objects in the environment are brought to the pixels of the images taken, and after this, they are transformed to 2D bounding boxes with the vertices furthest from each object.
	
\subsection{Calculation of object visibility}
	One main issue for the GT calculation is that CARLA always render all the objects, even when they are not visible by the camera, LiDAR or Radar, which is a problem for any training process.
	There are many studies about ray tracing that solve this issue. These techniques \cite{raytracing1} \cite{raytracing2} are computationally very expensive so it is very difficult to implement them in real-time.\par
	We propose a method for the calculation of visibility in CARLA and ROS using directly the point cloud calculated by CARLA. A vehicle will be considered as visible, as long as a point of the LiDAR point cloud is found inside an object, in the same way as it is done in the nuScenes dataset \cite{caesar2020nuscenes}. The steps to be performed are the following:
	\begin{enumerate}
		\item Remove objects furthest than the maximum LiDAR distance.
		\item Deletion of the points of the point cloud with the height higher or lower than the objects in the surroundings.
		\item Elimination of points outside the area in which the objects are located, taking into account all possible rotations.
		\item Selection of the visible objects having at least one point in the point cloud taking into account the rotation of the different objects.
	\end{enumerate}
	
	\par
	In order to obtain a much simplified and reduced GT to work with, the tool has been designed with sensor synchronization as the basis. In this way, GT does not have to be calculated every time data is obtained from a sensor, or in certain timestamps in which data is available. Because of this, it is necessary to activate the CARLA synchronization option for the correct functioning of the tool.\par
	
\begin{comment}
	IDEAS:
	
	1. Con el ADS que hay estable, dejarlo otra vez pulido y optimizado en poco tiempo (creo que YOLO está duplicada, PointPillars no corría bien, etc., y eso se puede poner como GT en el R-VIZ, diciendo específicamente que es GT). 
	
	Eso, con el AD-PerDevKit, estimo que fuesen dos días de trabajo.
	
	2. Ya se hicieron pruebas para meter el by-pass de tracking (hecho en SMARTS/SUMO) en CARLA, y funciona bien, aunque hay que retocar.
	
	3. Mi idea es correr el ADS en las rutas del Leaderboard, y obtener scores, sin nada más que lo estable ahora mismo vs meter predicción + dm con RL sobre todo en intersecciones (las decisiones de la red de rodrigo fuera de intersecciones están capadas)
	
	4. Hacer comparativas múltiples:
	
	ADS sin predicción
	ADS sin predicción, con DM en intersecciones (sólo basado en posición)
	ADS con predicción y DM en intersecciones (basado en predicción)
	
	Y la teoría dice que cuanta más capacidad predictiva metas al sistema, mejores deberían ser las métricas holísticas.
	
	Si todo fuese bien, y hasta la fecha de la lectura, mi idea es ayudar al equipo a meter detección, fusión y tracking en simulación y real, aunque no sean integraciones mid-level, y quitar el by-pass, pero lo anterior es más prioritario.
\end{comment}

\section{Summary}
\label{sec:5_summary}

Even though our methods do not obtain the best regression metrics, we achieve up-to-pair results (Table~\ref{table:effcomp}) against other SOTA approaches whilst our number of FLOPs is several orders of magnitude smaller than other approaches \cite{gu2021densetntwaymo} \cite{liang2020learning}, obtaining a good trade-off (specially the map baseline) between model complexity and accuracy (minADE, k=6), making it suitable for real-time operation in the field of AD. In our case, considering the top-25 regression metrics we achieve near SOTA results (just 15 cm, which represents 18.5 \%, worse in terms of minADE k=6 regarding our final approach) while achieving an impressive reduction of parameters and FLOPs. As observed in Table \ref{table:effcomp}, if we compare our final model, which includes social information, agents interaction and preliminary road information, and the methods with the closest minADE k=6 [m] (LaneGCN \cite{liang2020learning}, HOME \cite{gilles2021home} and GOHOME \cite{gilles2021gohome}), we obtain a reduction of 96 \%, 99 \% and 48 \% respectively in terms of FLOPs. It can be observed how including preliminary road information assuming non-holonomic \cite{triggs1993motion} and anisotropic \cite{ross1989planning} constraints respectively (that is, we mostly focus on the front driveable area) instead of processing the whole map, as well as computing social interactions via graph convolutional networks, boost our model for further integration edge-computing devices with a minimum accuracy loss acceptable for real-world Autonomous Driving applications.

Even though our method do not obtain the best regression metrics, we achieve comparable results (Table~\ref{table:effcomp}) against other SOTA approaches whilst our number of FLOPs is several orders of magnitude smaller than other approaches \cite{gu2021densetntwaymo} \cite{liang2020learning}, obtaining a good trade-off between model complexity and error (minADE, k=6).

Moreover, as it is well known in machine learning, the number of parameters is not always proportional to the inference speed. In that sense, our transformer approach also has certain benefits in comparison to LSTM/RNN temporal encoding, since these are non-parallelizable, therefore, despite having more parameters, transformers are faster~\cite{vaswani2017attention}.